{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WxvNKEowpem"
      },
      "source": [
        "# **The Algonauts Project 2025 Challenge - Development Kit Tutorial**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1m-N7pq4nEG_fXmEOSXIqZ4ONszkR_Wfi)\n",
        "\n",
        "## The Algonauts Project\n",
        "\n",
        "The [Algonauts Project](https://algonautsproject.com/), first launched in 2019, is on a mission to bring biological and machine intelligence researchers together on a common platform to exchange ideas and pioneer the intelligence frontier. Inspired by the astronauts' exploration of space, \"algonauts\" explore biological and artificial intelligence with state-of-the-art algorithmic tools, thus advancing both fields.\n",
        "\n",
        "## The 2025 challenge\n",
        "\n",
        "Encoding models of neural responses are increasingly used as predictive and explanatory tools in computational neuroscience ([Kay et al., 2008](https://doi.org/10.1038/nature06713); [Kell et al., 2018](https://doi.org/10.1016/j.neuron.2018.03.044); [Kriegeskorte and Douglas, 2019](https://doi.org/10.1016/j.conb.2019.04.002); [Naselaris et al., 2011](https://doi.org/10.1016/j.neuroimage.2010.07.073); [Tuckute et al., 2023](https://doi.org/10.1371/journal.pbio.3002366); [Van Gerven, 2017](https://doi.org/10.1016/j.jmp.2016.06.009); [Wu et al., 2006](https://doi.org/10.1146/annurev.neuro.29.051605.113024); [Yamins and DiCarlo, 2016](https://doi.org/10.1038/nn.4244)). They consist of algorithms, typically based on deep learning architectures, that take stimuli as input, and output the corresponding neural activations, effectively modeling how the brain responds to (i.e., encodes) these stimuli. Thus, the goal of the 2025 challenge is to provide a platform for biological and artificial intelligence scientists to cooperate and compete in developing cutting-edge functional magnetic resonance imaging (fMRI) encoding models. Specifically, these models should predict fMRI response to multimodal naturalistic movies, and generalize outside their training distribution.\n",
        "\n",
        "The challenge is based on data from the Courtois Project on Neuronal Modelling ([CNeuroMod](https://www.cneuromod.ca/)), which has acquired the dataset that, to date, most intensively samples single-subject fMRI responses to a variety of naturalistic tasks, including movie watching. For more details on the challenge you can visit the [website](https://algonautsproject.com/), read the [paper](https://doi.org/10.48550/arXiv.2501.00504), or watch [this video](https://youtu.be/KvLDpsIO2eg).\n",
        "\n",
        "## Tutorial overview\n",
        "\n",
        "This Development Kit Tutorial will help you to get started with the Algonauts 2025 challenge. It is divided into four sections, **each of which can be run independently**, where you will:\n",
        "1. Familiarize yourself with the challenge data.\n",
        "2. Learn to extract the stimulus features used to train and validate an fMRI encoding model.\n",
        "3. Train and validate an fMRI encoding model.\n",
        "4. Prepare and format fMRI predictions from an encoding model for challenge submission.\n",
        "\n",
        "There is an additional initial configuration section (`0 | Configuration`) that needs to be run prior to running the other sections.\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> **Even if you're not interested in the Algonauts Project 2025 challenge, you might still find this tutorial useful as an introduction to encoding models of fMRI responses to multimodal movie stimuli.**\n",
        "\n",
        "## Additional tutorial information\n",
        "\n",
        "For a walkthrough of the current Development Kit Tutorial, please see [this video](https://youtu.be/S_RxMiLUZ_w?si=6VdWwwuneR_Oe7--).\n",
        "\n",
        "You can run this tutorial either on [Colab](https://colab.research.google.com/drive/1fop0zvaLBLBagvJRC-HDqGDSgQElNWZB?usp=sharing), or locally by downloading it as a Jupyter Notebook file by selecting (`File --> Download --> Download .ipynb`) from the menu.\n",
        "\n",
        "On Colab, the running time of each tutorial section is only of a few minutes, with the exception of feature extraction in `Section 2`, which takes approximately 25 minutes using a GPU, and 1 hour using CPU.\n",
        "\n",
        "This tutorial was created by [Alessandro Gifford](https://www.alegifford.com/) and [Domenic Bersch](https://toastydom.github.io/). If you experience problems with the code, please get in touch with the Algonauts Team (algonauts.mit@gmail.com).\n",
        "\n",
        "## 2025 challenge resources\n",
        "\n",
        "- [Website](https://algonautsproject.com/)\n",
        "- [Paper](https://doi.org/10.48550/arXiv.2501.00504)\n",
        "- [Data](https://forms.gle/kmgYdxR92H4nUBfH7)\n",
        "- [Challenge Development Kit Tutorial](https://colab.research.google.com/drive/1fop0zvaLBLBagvJRC-HDqGDSgQElNWZB?usp=sharing)\n",
        "- [Codabench Challenge Submission Page](https://www.codabench.org/competitions/4313/)\n",
        "- [Challenge Overview Video](https://youtu.be/KvLDpsIO2eg)\n",
        "- [Development Kit Tutorial Walkthrough Video](https://youtu.be/S_RxMiLUZ_w)\n",
        "- [Codabench Submission Walkthrough Video](https://youtu.be/w7zpnoKtusE)\n",
        "- [CNeuroMod](https://www.cneuromod.ca/)\n",
        "\n",
        "## Citations\n",
        "\n",
        "If you use any of the resources provided for the Algonauts Project 2025 challenge, please cite the following papers:\n",
        "\n",
        "> * Gifford AT, Bersch D, St-Laurent M, Pinsard B, Boyle J, Bellec L, Oliva A, Roig G, Cichy RM. 2025. The Algonauts Project 2025 Challenge: How the Human Brain Makes Sense of Multimodal Movies. _arXiv preprint_, arXiv:2501.00504. DOI: [https://doi.org/10.48550/arXiv.2501.00504](https://doi.org/10.48550/arXiv.2501.00504)\n",
        "\n",
        "> * Boyle J, Pinsard B, Borghesani V, Paugam F, DuPre E, Bellec P. 2023. The Courtois NeuroMod project: quality assessment of the initial data release (2020). _2023 Conference on Cognitive Computational Neuroscience_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2AxKJGX90fd"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQMX-C38AEl"
      },
      "source": [
        "# 0 | Configuration\n",
        "\n",
        "Note that in sections `0.1 Install and import the necessary Python libraries` and `0.2 Access the tutorial data`, you will need to run different code cells depending on whether you are running the tutorial on Colab or as a Jupyter Notebook. An `if statement` takes care of this. Just select `'colab'` or `'jupyter_notebook'` for the `platform` variable in the cell below.\n",
        "\n",
        "Furthermore, you can select whether to run the code only on CPU (`'cpu'`), or also using a GPU (`'cuda'`) by setting the `device` variable. If you wish to run each tutorial section independently, we recommend using a GPU to run tutorial `Section 2` (stimuli feature extraction), and CPU to run all the other sections (since their compute time won't change if using GPU). If you wish to run all tutorial sections one after the other, we recommend using a GPU.\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> If you are running the tutorial on Colab and want to use GPU, make sure to first change the runtime type from CPU to GPU (`Runtime --> Change runtime type`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsGwdl1nVaSr",
        "outputId": "1a260c97-644c-4d97-97af-d1ec84008ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on \"colab\" using \"cuda\" device!\n"
          ]
        }
      ],
      "source": [
        "# Select platform\n",
        "platform = 'colab' #@param ['colab', 'jupyter_notebook']\n",
        "\n",
        "# Select device for computation\n",
        "device = 'cuda' # @param ['cpu', 'cuda']\n",
        "\n",
        "print(f'Running on \"{platform}\" using \"{device}\" device!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQmcjeEEWDap"
      },
      "source": [
        "## 0.1 | Install and import the necessary Python libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_GJV4PVs2ba"
      },
      "source": [
        "### 0.1.1 Install libraries for Google Colab\n",
        "\n",
        "If you are running the tutorial on Google Colab, you first need to install a few libraries with `pip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvv8Hasd8DJV",
        "outputId": "5fb3383f-ddab-4dc5-c7fc-7eae471a8d0b"
      },
      "outputs": [],
      "source": [
        "# if platform == 'colab':\n",
        "#   !pip install ipywidgets\n",
        "#   !pip install moviepy\n",
        "#   !pip install nilearn\n",
        "#   !pip install git+https://github.com/facebookresearch/pytorchvideo\n",
        "#   !pip install pysoundfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8aRowcQxM8E"
      },
      "source": [
        "### 0.1.2 | Install libraries for Jupyter Notebook\n",
        "\n",
        "If you are running the tutorial in a Jupyter Notebook, you will need to first install the following libraries:\n",
        "\n",
        "```shell\n",
        "h5py==3.1.0\n",
        "librosa==0.10.2.post1\n",
        "matplotlib==3.9.4\n",
        "moviepy==1.0.3\n",
        "nilearn==0.9.2\n",
        "numpy==1.26.4\n",
        "pandas==1.5.1\n",
        "pytorchvideo==0.1.5\n",
        "scikit-learn==1.1.1\n",
        "scipy==1.12.0\n",
        "torch==1.13.0\n",
        "tqdm==4.64.1\n",
        "torchvision==0.14.0\n",
        "transformers==4.33.3\n",
        "```\n",
        "\n",
        "We recommend copying these libraries and versions to a text file called `requirements.txt`, and then installing them within a virtual environment (e.g., an [Anaconda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) environment) using:\n",
        "\n",
        "```shell\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHGeAeDKWZr1"
      },
      "source": [
        "### 0.1.3 | Import libraries\n",
        "\n",
        "Once you have installed all necessary libraries, import them with the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ne4GTiBPtMMr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import torch\n",
        "import librosa\n",
        "import ast\n",
        "import string\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.linear_model import RidgeCV, Ridge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import pearsonr\n",
        "import cv2\n",
        "import nibabel as nib\n",
        "from nilearn import plotting\n",
        "from nilearn.maskers import NiftiLabelsMasker\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import VBox, Dropdown, Button\n",
        "from IPython.display import Video, display, clear_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'S' from 'sympy' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, Lambda, CenterCrop\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_feature_extractor\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2045\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2044\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2046\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2075\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2073\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2074\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2075\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2073\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2072\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2073\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2074\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2075\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple, Union\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraceback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfx_traceback\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_aot_autograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fun\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree_map\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtesting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogging_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_logs, LoggingTensorMode\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionalTensor\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeta_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_sparse_any\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     definitely_true,\n\u001b[32m     22\u001b[39m     sym_eq,\n\u001b[32m     23\u001b[39m     SymIntEqByExpr,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreductions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     is_traceable_wrapper_subclass,\n\u001b[32m     28\u001b[39m     transform_subclass,\n\u001b[32m     29\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     Application,\n\u001b[32m     69\u001b[39m     CeilToInt,\n\u001b[32m     70\u001b[39m     CleanDiv,\n\u001b[32m     71\u001b[39m     FloorDiv,\n\u001b[32m     72\u001b[39m     FloorToInt,\n\u001b[32m     73\u001b[39m     IsNonOverlappingAndDenseIndicator,\n\u001b[32m     74\u001b[39m     Max,\n\u001b[32m     75\u001b[39m     Mod,\n\u001b[32m     76\u001b[39m     PythonMod,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CppPrinter, PythonPrinter\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Time-Series-Library/.venv/lib/python3.11/site-packages/torch/utils/_sympy/functions.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeVarTuple, Unpack\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sympify\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Expr\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'S' from 'sympy' (unknown location)"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from torchvision.transforms import Compose, Lambda, CenterCrop\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from pytorchvideo.transforms import Normalize, UniformTemporalSubsample, ShortSideScale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.4\n",
            "~/Time-Series-Library/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!which python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYPiqH9Xg3sM"
      },
      "source": [
        "## 0.2 | Access the tutorial data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy4c_S6b7uM_"
      },
      "source": [
        "### 0.2.1 | Access the data on Google Colab\n",
        "\n",
        "If you are running the tutorial on Google Colab, you need to connect the challenge tutorial data, which is stored on a Google Drive public folder called `algonauts_2025_challenge_tutorial_data`, to this Colab notebook. For this, follow four easy steps:\n",
        "\n",
        "1. Fill out [this form](https://docs.google.com/forms/d/e/1FAIpQLScWw5_uBPzFWD-FTXcRZRYBASOJutKwYevMZ8fOAY2RYYZstw/viewform?usp=sf_link) to obtain the tutorial data folder link.\n",
        "2. Select the folder on Google Drive, and choose `Organise --> Add shortcut`. This will create a shortcut of the folder to a desired path in your Google Drive without copying the actual data or taking space (see the screenshot below for a visualization of this step).\n",
        "3. Edit the `root_data_dir` variable in the code cell below with the path to the `algonauts_2025_challenge_tutorial_data` shortcut folder in your Google Drive. This path will change based on where in your Google Drive you create the shortcut of the data foleder.\n",
        "4. Mount your Google Drive to this Colab notebook using `drive.mount()`, by running the code cell below.\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> The challenge tutorial data stored in the `algonauts_2025_challenge_tutorial_data` Google Drive folder is only a subset of the entire Algonauts 2025 chalenge data. After filling the form above, you will also be given the link of the GitHub repository containing the full challenge data.\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> The CNeuroMod data used in the Algonauts 2025 challenge has been openly shared under a [Creative Commons CC0 license](https://creativecommons.org/public-domain/cc0/) by a subset of CNeuroMod participants through the Canadian Open Neuroscience Platform (CONP), funded by Brain Canada and based at McGill University, Canada. Participants provided informed consent both to participate in the CNeuroMod study conducted at CIUSSS du Centre-Sud-de-l’île-de-Montréal and, separately, to share their data through the CONP. The CC0 license enables the Algonauts 2025 team to freely create and distribute derivative works, without restrictions, and the Algonauts 2025 challenge data is likewise distributed under a CC0 license."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WyD0gBWiHUp"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=13_x3ahpu0COg2zEUTrTQ37twWUupg6mP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wryaitcJfnBF",
        "outputId": "27fa84b3-ddf7-421c-be26-5aa30c136b16"
      },
      "outputs": [],
      "source": [
        "# Mount your Google Drive to this Colab notebook\n",
        "if platform == 'colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    root_data_dir = '/content/drive/MyDrive/algonauts_2025_challenge_tutorial_data' #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkZNWzz4WTWL"
      },
      "source": [
        "### 0.2.2 | Access the tutorial data on a Jupyter Notebook\n",
        "\n",
        "If you are running the tutorial on a Jupyter Notebook, you first need to download the tutorial data from a public Google Drive folder called `algonauts_2025_challenge_tutorial_data` (fill out [this form](https://docs.google.com/forms/d/e/1FAIpQLScWw5_uBPzFWD-FTXcRZRYBASOJutKwYevMZ8fOAY2RYYZstw/viewform?usp=sf_link) to obtain the tutorial data folder link).\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> The challenge tutorial data stored in the `algonauts_2025_challenge_tutorial_data` Google Drive folder is only a subset of the entire Algonauts 2025 chalenge data. After filling the form above, you will also be given the link of the GitHub repository containing the full challenge data.\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> The CNeuroMod data used in the Algonauts 2025 challenge has been openly shared under a [Creative Commons CC0 license](https://creativecommons.org/public-domain/cc0/) by a subset of CNeuroMod participants through the Canadian Open Neuroscience Platform (CONP), funded by Brain Canada and based at McGill University, Canada. Participants provided informed consent both to participate in the CNeuroMod study conducted at CIUSSS du Centre-Sud-de-l’île-de-Montréal and, separately, to share their data through the CONP. The CC0 license enables the Algonauts 2025 team to freely create and distribute derivative works, without restrictions, and the Algonauts 2025 challenge data is likewise distributed under a CC0 license.\n",
        "\n",
        "Once you have downloaded the tutorial data, edit the `root_data_dir` variable in the cell below with the path to the `algonauts_2025_challenge_tutorial_data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr2gucXi3WQc"
      },
      "outputs": [],
      "source": [
        "if platform == 'jupyter_notebook':\n",
        "    root_data_dir = '../algonauts_2025_challenge_tutorial_data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdk4Ug099AHN"
      },
      "source": [
        "### 0.2.3 | Overview of the downloaded tutorial data\n",
        "\n",
        "The `algonauts_2025_challenge_tutorial_data` folder is organized into three subfolders:\n",
        "- **`algonauts_2025.competitors`:** Multimodal movie stimuli (i.e., `.mkv` files of audiovisual movies, and `.tsv` files containing timestamped movie transcripts) and corresponding fMRI responses from the CNeuroMod dataset. You will familiarize yourself with these data in `Section 1`. In `Section 2`, you will extract **visual**, **audio** and **language** stimulus features from the multimodal movie stimuli, and in `Section 3` you will use them with their corresponding fMRI responses to train and validate brain encoding models.\n",
        "- **`stimulus_features`:** Raw and PCA-downsampled **visual**, **audio**, and **language** stimulus features, extracted from the challenge movie stimuli. You will learn how to extract these features in `Section 2`, you will use them in `Section 3` to train and validate encoding models, and in `Section 4` to prepare the challenge submission.\n",
        "- **`trained_encoding_models`:** Encoding models trained using **visual**, **audio**, **language**, or **all** stimulus features. You will use these models in `Section 4` to learn how to prepare challenge submissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqZOz2a7diW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imEkF3Iov-Jf"
      },
      "source": [
        "# 1 | Familiarize yourself with the challenge data\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1V9boHQzQpJxgUJHUm5laOZ-f6WO28X9P)\n",
        "\n",
        "The challenge data comes from the [CNeuroMod](https://www.cneuromod.ca/) dataset, and consists of multimodal movie stimuli and corresponding whole-brain time series fMRI responses of four subjects. Challenge participants will train and evaluate their encoding models using a subset of CNeuroMod's data which includes almost 80 hours of multimodal movie stimuli and corresponding fMRI responses. The stimuli consist of movie visual frames, audio samples, and time-stamped language transcripts. The neural data consist of whole-brain fMRI responses for four CNeuroMod subjects (sub-01, sub-02, sub-03 and sub-05), normalized to the Montreal Neurological Institute (MNI) spatial template ([Brett et al., 2002](https://doi.org/10.1038/nrn756)), and processed as time series whose signal is assigned to 1,000 functionally defined brain parcels ([Schaefer et al., 2018](https://doi.org/10.1093/cercor/bhx179)).\n",
        "\n",
        "The 2025 challenge includes two main phases, followed by a post-challenge phase. Each phase involves a different data subset:\n",
        "\n",
        "1. **Model building phase (6 months, January 6 2025 - July 6 2025):** During this first phase, challenge participants will train and test encoding models using movie stimuli and fMRI responses from the same distribution. For *model training*, we provide 55 hours of movie stimuli and corresponding fMRI responses for each of the four subjects for all episodes of seasons 1 to 6 of the sitcom *Friends*. We also provide 10 hours of movie stimuli and corresponding fMRI responses from the *Movie10* dataset for which the same four subjects watched the following four movies: *The Bourne Supremacy*, *Hidden Figures*, *Life* (a BBC nature documentary), and *The Wolf of Wall Street*. Each movie was presented to each subject once, except for *Life* and *Hidden Figures* which were presented twice. Challenge participants can train their encoding models using these data. For *model testing*, we provide 10 hours of movie stimuli for all episodes of seasons 7 of Friends, and withhold the corresponding fMRI responses for each subject. Challenge participants can test their encoding models against the withheld fMRI responses by submitting predicted fMRI responses for Friends season 7 to [Codabench](https://www.codabench.org/competitions/4313/). After each submission, the scoring program will correlate (Pearson’s *r*) the predicted fMRI responses for each parcel and subject with the recorded (withheld) fMRI responses across all Friends season 7 episodes, resulting in one correlation score for each parcel and subject. These correlation scores are averaged first across parcels and then across subjects, to obtain a single correlation score quantifying the performance of each submission.\n",
        "\n",
        "2. **Model selection phase (1 week, July 6 2025 - July 13 2025):** During this second phase, the winning models will be selected based on the accuracy of their predicted fMRI responses for withheld OOD movie stimuli. We will provide 2 hours of out-of-distribution (OOD) movie stimuli, and withhold the corresponding fMRI responses for each of the four subjects. The nature of the OOD movie stimuli will not be revealed until the beginning of the model selection phase. To participate in the winners selection process, challenge participants can submit their encoding model’s predicted fMRI responses for the OOD movie stimuli to [Codabench](https://www.codabench.org/competitions/4313/). After each submission, the scoring program will correlate the predicted fMRI responses for each parcel and subject with the recorded (withheld) fMRI responses, independently for each of the OOD movie stimuli, resulting in one correlation score for each parcel, OOD movie and subject. These correlation scores are averaged first across parcels, then across OOD movies, and finally across subjects, thus obtaining a single correlation score quantifying the performance of each submission.\n",
        "\n",
        "3. **Post-challenge phase (indefinite, from July 13 2025):** Once the challenge is over, we will open an indefinite post-challenge phase which will serve as a public benchmark. This benchmark will consist of two separate leaderboards that will rank encoding models based on their fMRI predictions for in-distribution (Friends season 7) or out-of-distribution (OOD movies) multimodal movie stimuli, respectively.\n",
        "\n",
        "In this tutorial, you will familiarize yourself with the data from the **Model building phase**. You will learn how to use those data to train and validate encoding models, and to prepare the model predictions for Friends season 7 for challenge submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ENzAcMS9NRs"
      },
      "source": [
        "\n",
        "## 1.1 | Multimodal movie stimuli\n",
        "\n",
        "The multimodal (**audio**, **visual** and **language**) stimuli of the Algonauts 2025 challenge consist of `.mkv` files of audiovisual movies, and of `.tsv` files that contain corresponding timestamped movie transcripts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDBWbIF3Z9Nx"
      },
      "source": [
        "### 1.1.1 | .mkv files (audiovisual movie stimuli)\n",
        "\n",
        "The `.mkv` files consist of movies that combine the visual and audio modalities, for seasons 1 to 7 of Friends and for Movie10.\n",
        "\n",
        "#### Friends (seasons 1-7)\n",
        "\n",
        "The `.mkv` files for seasons 1 to 7 of Friends are found at `../algonauts_2025.competitors/stimuli/movies/friends/s<season>/`, and have the naming convention `friends_s-<season>e<episode><episode_split>.mkv`, where:\n",
        "- **`season`:** Number indicating the Friends season.\n",
        "- **`episode`:** Number indicating the Friends episode.\n",
        "- **`episode_split`:** Full episodes were split into shorter (~12 min) segments watched by participants inside the MRI in order to reduce the duration of fMRI data acquisition runs. Letters indicate the split of each episode. Most Friends episodes are split into two parts (i.e., splits `a` and `b`), but a handful of longer episodes are split into four parts (i.e., splits `a`, `b`, `c` and `d`).\n",
        "\n",
        "#### Movie10\n",
        "\n",
        "The `.mkv` files for Movie10 are found at `../algonauts_2025.competitors/stimuli/movies/movie10/<movie>/`, and have the naming convention `<movie><movie_split>.mkv`, where:\n",
        "- **`movie`:** String indicating the movie name.\n",
        "- **`movie_split`:** Number indicating the movie split. Each movie was split into several segments to limit the duration of consecutive fMRI data acquisition runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq1EyZx_pA4O"
      },
      "source": [
        "As an example, here you will load and visualize the file `friends_s01e01a.mkv`, that is, the first half (split `a`) of the first episode from the first season of Friends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUO-SXM5bqW2"
      },
      "outputs": [],
      "source": [
        "def load_mkv_file(movie_path):\n",
        "    \"\"\"\n",
        "    Load video and audio data from the given .mkv movie file, and additionally\n",
        "    prints related information.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the .mkv file\n",
        "    cap = cv2.VideoCapture(movie_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open movie.\")\n",
        "        return\n",
        "\n",
        "    # Get video information\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video_duration = video_total_frames / video_fps\n",
        "    video_duration_minutes = video_duration / 60\n",
        "\n",
        "    # Print video information\n",
        "    print(\">>> Video Information <<<\")\n",
        "    print(f\"Video FPS: {video_fps}\")\n",
        "    print(f\"Video Resolution: {video_width}x{video_height}\")\n",
        "    print(f\"Total Frames: {video_total_frames}\")\n",
        "    print(f\"Video Duration: {video_duration:.2f} seconds or {video_duration_minutes:.2f} minutes\")\n",
        "\n",
        "    # Release the video object\n",
        "    cap.release()\n",
        "\n",
        "    # Audio information\n",
        "    clip = VideoFileClip(movie_path)\n",
        "    audio = clip.audio\n",
        "    audio_duration = audio.duration\n",
        "    audio_fps = audio.fps\n",
        "    print(\"\\n>>> Audio Information <<<\")\n",
        "    print(f\"Audio Duration: {audio_duration:.2f} seconds\")\n",
        "    print(f\"Audio FPS (Sample Rate): {audio_fps} Hz\")\n",
        "\n",
        "    # Extract and display the first 20 seconds of the video\n",
        "    output_video_path = 'first_20_seconds.mp4'\n",
        "    video_segment = clip.subclip(0, min(20, video_duration))\n",
        "    print(\"\\nCreating clip of the first 20 seconds of the video...\")\n",
        "    video_segment.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n",
        "\n",
        "    # Display the video in the notebook\n",
        "    display(Video(output_video_path, embed=True, width=640, height=480))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b6ia1wicjiq",
        "outputId": "3a55ebf2-6a0d-4cfd-9f2a-46cbc069b3d9"
      },
      "outputs": [],
      "source": [
        "# Load the .mkv file\n",
        "movie_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e01a.mkv\"\n",
        "load_mkv_file(movie_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nnx4It4b0A3"
      },
      "source": [
        "### 1.1.2 | .tsv files (timestamped movie transcripts)\n",
        "\n",
        "The `.tsv` files contain the timestamped movie transcripts, that is, transcripts of spoken content (dialogue) in the movie stimuli, for seasons 1 to 7 of Friends and for Movie10.\n",
        "\n",
        "#### Friends (seasons 1-7)\n",
        "\n",
        "The `.tsv` files for seasons 1 to 7 of Friends are found at `../algonauts_neuromod.competitors/stimuli/transcripts/friends/s<season>/`, and have the naming convention `friends_s-<season>e<episode><episode_split>.tsv`, where:\n",
        "- **`season`:** Number indicating the Friends season.\n",
        "- **`episode`:** Number indicating the Friends episode.\n",
        "- **`episode_split`:** Letter indicating the split of the episode. Most Friends episodes are split into two parts (i.e., splits `a` and `b`), but a handful of longer episodes are split into four parts (i.e., splits `a`, `b`, `c` and `d`).\n",
        "\n",
        "#### Movie10\n",
        "\n",
        "The `.tsv` files for Movie10 are found at `../algonauts_neuromod.competitors/stimuli/transcripts/movie10/<movie>/`, and have the naming convention `movie10_<movie><movie_split>.tsv`, where:\n",
        "- **`movie`:** String indicating the movie name.\n",
        "- **`movie_split`:** Number indicating the movie split.\n",
        "\n",
        "#### .tsv file content\n",
        "\n",
        "The `.tsv` files splits transcribed movie dialogue into chunks of 1.49 seconds, where each **row** of the `.tsv` file corresponds to one such chunk. This segmentation was performed to facilitate alignment with the fMRI data, since fMRI volumes were acquired with a repetition time (TR) of 1.49 seconds (that is, one fMRI sample was acquired every 1.49 seconds). If no words were spoken during a specific chunk, the corresponding `.tsv` file row will be empty.\n",
        "\n",
        "The **columns** of the `.tsv` files consist of\n",
        "different attributes of the language transcripts:\n",
        "- **`text_per_tr`:** Sentence consisting of words that were spoken during the chunk of interest (i.e., words with word offset within the chunk of interest, even if their onset was in the previous chunk).\n",
        "- **`words_per_tr`:** List of individual words that were spoken during the chunk of interest.\n",
        "- **`onsets_per_tr`:** Starting time (in seconds) of each word spoken during the chunk, relative to movie onset.\n",
        "- **`durations_per_tr`:** Duration (in seconds) of each word spoken during the chunk.\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> Since the transcribed movie dialogue is not split based on word onset/offset (but rather into chunks of 1.49 seconds), the onset and offset of some words might fall in different (consecutive) chunks. In case a word is split into two consecutive chunks, it will be assigned to the chunk of word offset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWSSBzyg9DJx"
      },
      "source": [
        "As an example, here you will load the file `friends_s01e01a.tsv`, that is, the transcript of the first half (split `a`) of the first episode from the first season of Friends, and print the data from the first 20 chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K2GaEjkEOti"
      },
      "outputs": [],
      "source": [
        "def load_tsv_file(transcript_path):\n",
        "    \"\"\"\n",
        "    Load and visualize language transcript data from the given .TSV file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    transcript_path : str\n",
        "        Path to the .tsv transcript file.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the .tsv into a pandas DataFrame\n",
        "    transcript_df = pd.read_csv(transcript_path, sep='\\t')\n",
        "\n",
        "    # Select the first 20 rows (chunks)\n",
        "    sample_transcript_data = transcript_df.iloc[:20]\n",
        "\n",
        "    # Display the first 20 rows (chunks)\n",
        "    # The first 11 rows are empty since no words were spoken during the\n",
        "    # beginning of the episode.\n",
        "    print(\"Transcript data (Rows 0 to 20):\")\n",
        "    display(sample_transcript_data)\n",
        "\n",
        "    # Print other transcript info\n",
        "    print(f\"\\nTranscript has {transcript_df.shape[0]} rows (chunks of 1.49 seconds) and {transcript_df.shape[1]} columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "4spucZvJd7pZ",
        "outputId": "ab91706a-eb6a-4407-d1eb-5870fc68ef77"
      },
      "outputs": [],
      "source": [
        "# Load the .tsv file\n",
        "transcript_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e01a.tsv\"\n",
        "load_tsv_file(transcript_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnCU9BdLaDQV"
      },
      "source": [
        "### 1.2.3 | Align the .mkv movies with the .tsv transcripts\n",
        "\n",
        "Here, you will align the `.mkv` to the `.tsv` files. Since the `.tsv` files are divided into chunks of 1.49 seconds (in line with the fMRI sampling rate, or TR), to align the `.mkv` and `.tsv` file content, you will first need to divide the `.mkv` movies into chunks of 1.49 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrkkoVmjNlwr"
      },
      "outputs": [],
      "source": [
        "def load_transcript(transcript_path):\n",
        "    \"\"\"\n",
        "    Loads a transcript file and returns it as a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    transcript_path : str\n",
        "        Path to the .tsv transcript file.\n",
        "\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(transcript_path, sep='\\t')\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_movie_info(movie_path):\n",
        "    \"\"\"\n",
        "    Extracts the frame rate (FPS) and total duration of a movie.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(movie_path)\n",
        "    fps, frame_count = cap.get(cv2.CAP_PROP_FPS), cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    cap.release()\n",
        "\n",
        "    return fps, frame_count / fps\n",
        "\n",
        "\n",
        "def split_movie_into_chunks(movie_path, chunk_duration=1.49):\n",
        "    \"\"\"\n",
        "    Divides a video into fixed-duration chunks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "    chunk_duration : float, optional\n",
        "        Duration of each chunk in seconds (default is 1.49).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _, video_duration = get_movie_info(movie_path)\n",
        "    chunks = []\n",
        "    start_time = 0.0\n",
        "\n",
        "    # Create chunks for the specified time\n",
        "    while start_time < video_duration:\n",
        "        end_time = min(start_time + chunk_duration, video_duration)\n",
        "        chunks.append((start_time, end_time))\n",
        "        start_time += chunk_duration\n",
        "    return chunks\n",
        "\n",
        "def extract_movie_segment_with_sound(movie_path, start_time, end_time,\n",
        "    output_path='output_segment.mp4'):\n",
        "    \"\"\"\n",
        "    Extracts a specific segment of a video with sound and saves it.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "    start_time : float\n",
        "        Start time of the segment in seconds.\n",
        "    end_time : float\n",
        "        End time of the segment in seconds.\n",
        "    output_path : str, optional\n",
        "        Path to save the output segment (default is 'output_segment.mp4').\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Create movie segment\n",
        "    movie_segment = VideoFileClip(movie_path).subclip(start_time, end_time)\n",
        "    print(f\"\\nWriting movie file from {start_time}s until {end_time}s\")\n",
        "\n",
        "    # Write video file\n",
        "    movie_segment.write_videofile(output_path, codec=\"libx264\",\n",
        "        audio_codec=\"aac\", verbose=False, logger=None)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def display_transcript_and_movie(chunk_index, transcript_df, chunks,\n",
        "    movie_path):\n",
        "    \"\"\"\n",
        "    Displays transcript, movie, onset, and duration for a selected chunk.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chunk_index : int\n",
        "        Index of the selected chunk.\n",
        "    transcript_df : DataFrame\n",
        "        DataFrame containing transcript data.\n",
        "    chunks : list\n",
        "        List of (start_time, end_time) tuples for video chunks.\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "\n",
        "    \"\"\"\n",
        "    # Retrieve the start and end times for the selected chunk\n",
        "    start_time, end_time = chunks[chunk_index]\n",
        "\n",
        "    # Get the corresponding transcript row if it exists in the DataFrame\n",
        "    transcript_chunk = transcript_df.iloc[chunk_index] if chunk_index < len(transcript_df) else None\n",
        "\n",
        "    # Display the stimulus chunk number\n",
        "    print(f\"\\nChunk number: {chunk_index + 1}\")\n",
        "\n",
        "    # Display transcript details if available; otherwise, indicate no dialogue\n",
        "    if transcript_chunk is not None and pd.notna(transcript_chunk['text_per_tr']):\n",
        "        print(f\"\\nText: {transcript_chunk['text_per_tr']}\")\n",
        "        print(f\"Words: {transcript_chunk['words_per_tr']}\")\n",
        "        print(f\"Onsets: {transcript_chunk.get('onsets_per_tr', 'N/A')}\")\n",
        "        print(f\"Durations: {transcript_chunk.get('durations_per_tr', 'N/A')}\")\n",
        "    else:\n",
        "        print(\"<No dialogue in this scene>\")\n",
        "\n",
        "    # Extract and display the video segment\n",
        "    output_movie_path = extract_movie_segment_with_sound(movie_path, start_time,\n",
        "        end_time)\n",
        "    display(Video(output_movie_path, embed=True, width=640, height=480))\n",
        "\n",
        "\n",
        "def create_dropdown_by_text(transcript_df):\n",
        "    \"\"\"\n",
        "    Creates a dropdown widget for selecting chunks by their text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    transcript_df : DataFrame\n",
        "        DataFrame containing transcript data.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    options = []\n",
        "\n",
        "    # Iterate over each row in the transcript DataFrame\n",
        "    for i, row in transcript_df.iterrows():\n",
        "        if pd.notna(row['text_per_tr']):  # Check if the transcript text is not NaN\n",
        "            options.append((row['text_per_tr'], i))\n",
        "        else:\n",
        "            options.append((\"<No dialogue in this scene>\", i))\n",
        "    return widgets.Dropdown(options=options, description='Select scene:')\n",
        "\n",
        "\n",
        "def interface_display_transcript_and_movie(movie_path, transcript_path):\n",
        "    \"\"\"\n",
        "    Interactive interface to align movie and transcript chunks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "    transcript_path : str\n",
        "        Path to the transcript file (.tsv).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the transcript data from the provided path\n",
        "    transcript_df = load_transcript(transcript_path)\n",
        "\n",
        "    # Split the video file into chunks of 1.49 seconds\n",
        "    chunks = split_movie_into_chunks(movie_path)\n",
        "\n",
        "    # Create a dropdown widget with transcript text as options\n",
        "    dropdown = create_dropdown_by_text(transcript_df)\n",
        "\n",
        "    # Create an output widget to display video and transcript details\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Display the dropdown and output widgets\n",
        "    display(dropdown, output)\n",
        "\n",
        "    # Define the function to handle dropdown value changes\n",
        "    def on_chunk_select(change):\n",
        "        with output:\n",
        "            output.clear_output()  # Clears previous content\n",
        "            chunk_index = dropdown.value\n",
        "            display_transcript_and_movie(chunk_index, transcript_df, chunks,\n",
        "                movie_path)\n",
        "\n",
        "    dropdown.observe(on_chunk_select, names='value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC07NvAfH7aI"
      },
      "outputs": [],
      "source": [
        "# Align the .mkv movies and .tsv language transcripts\n",
        "interface_display_transcript_and_movie(movie_path, transcript_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAelLja5bQqR"
      },
      "source": [
        "## 1.2 | fMRI data\n",
        "\n",
        "The Algonauts 2025 Challenge uses fMRI data from four [CNeuromod](https://www.cneuromod.ca/) subjects (`sub-01`, `sub-02`, `sub-03` and `sub-05`). For each subject, the data include fMRI responses to each movie, brain atlases, and the number of samples (in chunks of 1.49s) for the withheld fMRI responses to the test movie stimuli.\n",
        "\n",
        "### fMRI responses\n",
        "\n",
        "The fMRI responses are found at `../algonauts_2025.competitors/fmri/sub-0X/func/`. They consist of whole-brain fMRI responses to\n",
        "seasons 1 to 6 of Friends and to Movie10 for four subjects. These fMRI responses are normalized to the Montreal Neurological Institute (MNI) spatial template ([Brett et al., 2002](https://doi.org/10.1038/nrn756)), processed as time series whose signal is assigned to 1,000 functionally defined brain parcels ([Schaefer et al., 2018](https://doi.org/10.1093/cercor/bhx179)), and saved in `.h5` files (one file for Friends season 1-6 and one for Movie10 for each subject).\n",
        "\n",
        "Each `.h5` file consists of multiple fMRI datasets, corresponding to different movie segments, where every dataset consists of a 2D array of shape `(N samples, 1,000 parcels)`. Since the fMRI responses were collected with a repetition time (TR) of 1.49 seconds, one fMRI sample was recorded every 1.49 seconds worth of movie watching.\n",
        "\n",
        "#### Friends (seasons 1-6)\n",
        "\n",
        "For fMRI responses to Friends, the datasets within the `.h5` files have the following naming convention `ses-<recording_session>_task-s<season>e<episode><episode_split>`, where:\n",
        "- **`recording_session`:** Number indicating the fMRI recording session (data were acquired over multiple scanning sessions spread across multiple days).\n",
        "- **`season`:** Number indicating the Friends season.\n",
        "- **`episode`:** Number indicating the Friends episode.\n",
        "- **`episode_split`:** Letter indicating the split of the episode. Most Friends episodes are split into two parts (i.e., splits `a` and `b`), but a handful of longer episodes are split into four parts (i.e., splits `a`, `b`, `c` and `d`).\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> most, but not all Friends episodes were presented according to the series order (for example, fMRI responses for episode 1 of season 1 were collected during the second or third recording session).\n",
        "\n",
        "<font color='red'><b>MISSING fMRI DATA:</b></font>\n",
        "* Subject 2 is missing `s05e20a`.\n",
        "* Subject 5 is missing `s04e01a`, `s04e01b` and `s04e13b`.\n",
        "\n",
        "#### Movie10\n",
        "\n",
        "For fMRI responses to Movie10, fMRI datasets were saved within `.h5` files (one per subject) according to the naming convention `ses-<recording_session>_task-<movie><movie_split>_run-<run_number>`, where:\n",
        "- **`recording_session`:** Number indicating the fMRI recording session (movie splits were acquired over multiple sessions spread over many days).\n",
        "- **`movie`:** String indicating the movie name.\n",
        "- **`movie_split`:** Number indicating the movie split.\n",
        "- **`run_number`:** Number (`1` or `2`) indicating whether the movie was presented for the first or second time. Only applies to datasets for movies *Life* and *Hidden figures*, since these two movies were presented twice to each subject (whereas all other movies were only presented once).\n",
        "\n",
        "### MRI atlases\n",
        "\n",
        "The MRI atlases are found at `../algonauts_2025.competitors/fmri/sub-0X/atlas/`. They were used to assign whole-brain fMRI signal to 1,000 functionally defined brain parcels from the Schaefer brain atlas ([Schaefer et al., 2018](https://doi.org/10.1093/cercor/bhx179)). Subject-specific atlases can be used to project a subject's 1,000 parcel-wise data (e.g., fMRI responses, Pearson's $r$ scores) back into a 3D brain volume space for plotting.\n",
        "\n",
        "Each atlas file is a 3D array of shape of `(97 voxels, 115 voxels, 97 voxels)`, where each brain voxel is assigned a parcel ID between 1 and 1,000; zeros represent areas outside the brain.\n",
        "\n",
        "### fMRI sample number for the test movie stimuli\n",
        "\n",
        "These files are found at `../algonauts_2025.competitors/fmri/sub-0X/target_samples_number/`, and indicate the number of fMRI response samples in each timeseries from Friends season 7 for each subject. These fMRI timeseries are withheld to test model accuracy throughout the Model Building phase, but the number of samples per timeseries is disclosed for each subject. You will use these files in tutorial `Section 4` to learn how to prepare challenge submissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zgwNNrtrti_"
      },
      "source": [
        "Here, you will load the fMRI timeseries of one of the four subjects for either Friends or Movie10, and visualize their content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZGfpBRvpRZ7"
      },
      "outputs": [],
      "source": [
        "# Function to list available subjects based on folder names\n",
        "def list_subjects(fmri_dir):\n",
        "    return sorted([d for d in os.listdir(fmri_dir) if d.startswith('sub-')])\n",
        "\n",
        "# Function to explore HDF5 file structure and organize datasets by season/movie\n",
        "def explore_h5_file(file_path, selected_dataset):\n",
        "    season_movie_dict = {}\n",
        "    with h5py.File(file_path, 'r') as h5_file:\n",
        "        for name, obj in h5_file.items():\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                if selected_dataset == 'Friends':\n",
        "                    season_movie = name.split('_')[1].split('-')[1][:3]  # Extract season (e.g., 's01')\n",
        "                elif selected_dataset == 'Movie10':\n",
        "                    season_movie = name.split('_')[1].split('-')[1][:-2]  # Extract movie (e.g., 'bourne')\n",
        "                season_movie_dict.setdefault(season_movie, []).append(f\"{name} (Shape: {obj.shape})\")\n",
        "    return season_movie_dict\n",
        "\n",
        "# Function to display datasets in a DataFrame\n",
        "def display_datasets_in_table(season_dict):\n",
        "    max_len = max(len(v) for v in season_dict.values())\n",
        "    df = pd.DataFrame({k: v + [''] * (max_len - len(v)) for k, v in sorted(season_dict.items())})\n",
        "    display(df)\n",
        "\n",
        "# Create subject and dataset selector widget\n",
        "def create_subject_selector(fmri_dir):\n",
        "    subjects = list_subjects(fmri_dir)\n",
        "    dataset_options = ['Friends', 'Movie10']\n",
        "\n",
        "    subject_dropdown = Dropdown(options=subjects, description='Select Subject:')\n",
        "    dataset_dropdown = Dropdown(options=dataset_options, description='Select Dataset:')\n",
        "    button = Button(description=\"Explore File\", button_style='primary')\n",
        "\n",
        "    def on_button_click(b):\n",
        "        clear_output(wait=True)\n",
        "        display(VBox([subject_dropdown, dataset_dropdown, button]))\n",
        "\n",
        "        selected_subject = subject_dropdown.value\n",
        "        selected_dataset = dataset_dropdown.value\n",
        "\n",
        "        if selected_dataset == 'Friends':\n",
        "            h5_file_path = os.path.join(\n",
        "                fmri_dir, selected_subject, 'func',\n",
        "                f\"{selected_subject}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
        "            )\n",
        "        elif selected_dataset == 'Movie10':\n",
        "            h5_file_path = os.path.join(\n",
        "                fmri_dir, selected_subject, 'func',\n",
        "                f\"{selected_subject}_task-movie10_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_bold.h5\"\n",
        "            )\n",
        "\n",
        "        if os.path.exists(h5_file_path):\n",
        "            season_movie_data = explore_h5_file(h5_file_path, selected_dataset)\n",
        "            display_datasets_in_table(season_movie_data)\n",
        "        else:\n",
        "            print(\"Error: HDF5 file not found.\")\n",
        "\n",
        "    button.on_click(on_button_click)\n",
        "    display(VBox([subject_dropdown, dataset_dropdown, button]))\n",
        "\n",
        "# Base directory for fMRI data\n",
        "fmri_dir = root_data_dir + \"/algonauts_2025.competitors/fmri/\"\n",
        "\n",
        "# Run the subject selector widget\n",
        "create_subject_selector(fmri_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io0OLlBtlHd0"
      },
      "source": [
        "## 1.3 | Align the fMRI responses with the movie stimuli\n",
        "\n",
        "Here, you will align fMRI responses from subject 1 (`sub-01`) for episode 1 of season 1 of Friends, with the corresponding movie stimuli (i.e., the `.mkv` and `.tsv` files), by associating a stimulus chunk of interest with its corresponding fMRI sample.\n",
        "\n",
        "Since fMRI samples were acquired with a repetition time (TR) of 1.49 seconds (that is, one fMRI sample was acquired every 1.49 seconds), and since the language transcripts are also divided into chunks of 1.49 seconds, the two are straightforward to align: fMRI sample 10 will correspond to transcript chunk 10, fMRI sample 25 will correspond to transcript chunk 25, and so on, although you will need to account for delays in the brain's BOLD response (see hrf_delay parameter below).\n",
        "\n",
        "The movie stimuli, on the other hand, first need to be manually divided into chunks of 1.49 seconds. After this is done, fMRI sample 10 will correspond to movie chunk 10, fMRI sample 25 will correspond to movie chunk 25, and so on.\n",
        "\n",
        "<font color='red'><b>IMPORTANT:</b></font> fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal that reflects changes in blood oxygenation levels in response to activity in the brain. Blood flow increases to a given brain region in response to its activity. This vascular response, which follows the hemodynamic response function (HRF), takes time. Typically, the HRF peaks around 5–6 seconds after a neural event: this delay reflects the time needed for blood oxygenation changes to propagate and for the fMRI signal to capture them. Therefore, with the `hrf_delay` parameter you can introduce a delay between stimulus chunks and fMRI samples for a better correspondence between input stimuli and the brain response. For example, with a `hrf_delay` of 3, if the stimulus chunk of interest is 17, the corresponding fMRI sample will be 20.\n",
        "\n",
        "<font color='red'><b>IMPORTANT:</b></font> Even though the number of stimulus chunks does not always match perfectly with the fMRI sample number (e.g., the fMRI responses can have more samples than chunks available for the corresponding stimuli), **<font color='red'><b>the fMRI time series onset is ALWAYS SYNCHRONIZED with movie onset (i.e., the first fMRI sample is always synchronized with the first stimulus chunk).</b></font>**\n",
        "\n",
        "With the code below, you can select a sample of interest, print the words from the corresponding transcript chunk, play the corresponding movie chunk, and plot the corresponding fMRI response sample in subject `sub-01` (you will use the subject's brain atlases to transform the 1,000 fMRI parcels to a 3D brain volume space for plotting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzPWk4OsncNB"
      },
      "outputs": [],
      "source": [
        "def plot_fmri_on_brain(chunk_index, fmri_file_path, atlas_path, dataset_name,\n",
        "    hrf_delay):\n",
        "    \"\"\"\n",
        "    Map fMRI responses to brain parcels and plot it on a glass brain.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chunk_index : pandas.Series\n",
        "        The selected chunk from the transcript, used to determine the fMRI\n",
        "        sample.\n",
        "    fmri_file_path : str\n",
        "        Path to the HDF5 file containing fMRI data.\n",
        "    atlas_path : str\n",
        "        Path to the atlas NIfTI file.\n",
        "    dataset_name : str\n",
        "        Name of the dataset inside the HDF5 file.\n",
        "    hrf_delay : int\n",
        "        fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
        "        that reflects changes in blood oxygenation levels in response to\n",
        "        activity in the brain. Blood flow increases to a given brain region in\n",
        "        response to its activity. This vascular response, which follows the\n",
        "        hemodynamic response function (HRF), takes time. Typically, the HRF\n",
        "        peaks around 5–6 seconds after a neural event: this delay reflects the\n",
        "        time needed for blood oxygenation changes to propagate and for the fMRI\n",
        "        signal to capture them. Therefore, this parameter introduces a delay\n",
        "        between stimulus chunks and fMRI samples for a better correspondence\n",
        "        between input stimuli and the brain response. For example, with a\n",
        "        hrf_delay of 3, if the stimulus chunk of interest is 17, the\n",
        "        corresponding fMRI sample will be 20.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nLoading fMRI file: {fmri_file_path}\")\n",
        "\n",
        "    # Load the atlas image\n",
        "    atlas_img = nib.load(atlas_path)\n",
        "    atlas_data = atlas_img.get_fdata()\n",
        "\n",
        "    # Open the fMRI reeponses file, and extract the specific dataset\n",
        "    with h5py.File(fmri_file_path, 'r') as f:\n",
        "        print(f\"Opening fMRI dataset: {dataset_name}\")\n",
        "        fmri_data = f[dataset_name][()]\n",
        "        print(f\"fMRI dataset shape: {fmri_data.shape}\")\n",
        "\n",
        "    # Extract the corresponding sample from the fMRI responses based on the\n",
        "    # selected transcript chunk, and on the hrf_delay\n",
        "    if (chunk_index + hrf_delay) > len(fmri_data):\n",
        "        selected_sample = len(fmri_data)\n",
        "    else:\n",
        "        selected_sample = chunk_index + hrf_delay\n",
        "    fmri_sample_data = fmri_data[selected_sample]\n",
        "    print(f\"Extracting fMRI sample {selected_sample+1}.\")\n",
        "\n",
        "    # Map fMRI sample values to the brain parcels in the atlas\n",
        "    output_data = np.zeros_like(atlas_data)\n",
        "    for parcel_index in range(1000):\n",
        "        output_data[atlas_data == (parcel_index + 1)] = \\\n",
        "            fmri_sample_data[parcel_index]\n",
        "\n",
        "    # Create the output NIfTI image\n",
        "    output_img = nib.Nifti1Image(output_data, affine=atlas_img.affine)\n",
        "\n",
        "    # Plot the glass brain with the mapped fMRI data\n",
        "    display = plotting.plot_glass_brain(\n",
        "        output_img,\n",
        "        display_mode='lyrz',\n",
        "        cmap='inferno',\n",
        "        colorbar=True,\n",
        "        plot_abs=False)\n",
        "    colorbar = display._cbar\n",
        "    colorbar.set_label(\"fMRI activity\", rotation=90, labelpad=12, fontsize=12)\n",
        "    plotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8LcRg8njnL"
      },
      "source": [
        "Now, we integrate the movie and transcript display from above (`Section 1.2.3`) with the fMRI selection and plotting functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2wVQHQ-HH55"
      },
      "outputs": [],
      "source": [
        "# Main interactive interface with brain visualization\n",
        "def interface_display_transcript_movie_brain(movie_path, transcript_path,\n",
        "    fmri_file_path, atlas_path, dataset_name, hrf_delay):\n",
        "    \"\"\"\n",
        "    Interactive interface to display movie and transcripts chunks along with\n",
        "    the fMRI response from the corresponding sample.\n",
        "\n",
        "    This code uses functions from Section 1.2.3.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_path : str\n",
        "        Path to the .mkv movie file.\n",
        "    transcript_path : str\n",
        "        Path to the .tsv transcript file.\n",
        "    fmri_file_path : str\n",
        "        Path to the fMRI data file.\n",
        "    atlas_path : str\n",
        "        Path to the brain atlas file.\n",
        "    dataset_name : str\n",
        "        Name of the dataset to display fMRI data from.\n",
        "    hrf_delay : int\n",
        "        fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
        "        that reflects changes in blood oxygenation levels in response to\n",
        "        activity in the brain. Blood flow increases to a given brain region in\n",
        "        response its activity. This vascular response, which follows the\n",
        "        hemodynamic response function (HRF), takes time. Typically, the HRF\n",
        "        peaks around 5–6 seconds after a neural event: this delay reflects the\n",
        "        time needed for blood oxygenation changes to propagate and for the fMRI\n",
        "        signal to capture them. Therefore, this parameter introduces a delay\n",
        "        between stimulus chunks and fMRI samples. For example, with a hrf_delay\n",
        "        of 3, if the stimulus chunk of interest is 17, the corresponding fMRI\n",
        "        sample will be 20.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the .tsv transcript data from the provided path\n",
        "    transcript_df = load_transcript(transcript_path)  # from 1.2.3\n",
        "\n",
        "    # Split the .mkv movie file into chunks of 1.49 seconds\n",
        "    chunks = split_movie_into_chunks(movie_path)  # from 1.2.3\n",
        "\n",
        "    # Create a dropdown widget with transcript text as options\n",
        "    dropdown = create_dropdown_by_text(transcript_df)  # from 1.2.3\n",
        "\n",
        "    # Create an output widget to display video, transcript, and brain\n",
        "    # visualization\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Define the function to handle dropdown value changes\n",
        "    def on_chunk_select(change):\n",
        "        with output:\n",
        "            output.clear_output()  # Clear the previous output\n",
        "            chunk_index = dropdown.value\n",
        "\n",
        "            # Display video chunk and transcript\n",
        "            display_transcript_and_movie(chunk_index, transcript_df, chunks,\n",
        "                movie_path)  # from 1.2.3\n",
        "\n",
        "            # Visualize brain fMRI data\n",
        "            plot_fmri_on_brain(chunk_index, fmri_file_path, atlas_path,\n",
        "                dataset_name, hrf_delay)\n",
        "\n",
        "    dropdown.observe(on_chunk_select, names='value')\n",
        "    display(dropdown, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEmxL9Qropz9"
      },
      "source": [
        "Finally, we execute this cell to visualize matching stimuli and fMRI responses from a 1.49 seconds segment of your choice from season 1, episode 1 of Friends.\n",
        "\n",
        "With the `hrf_delay` parameter you can introduce a delay between stimulus samples and fMRI samples for a better correspondence between the input stimuli and the brain responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmDm06exOL91"
      },
      "outputs": [],
      "source": [
        "# HRF delay parameter\n",
        "hrf_delay = 3  #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "# Define file paths and dataset name\n",
        "movie_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e01a.mkv\"\n",
        "transcript_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e01a.tsv\"\n",
        "fmri_file_path = root_data_dir + \"/algonauts_2025.competitors/fmri/sub-01/func/sub-01_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
        "atlas_path = root_data_dir + \"/algonauts_2025.competitors/fmri/sub-01/atlas/sub-01_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-dseg_parcellation.nii.gz\"\n",
        "dataset_name = \"ses-003_task-s01e01a\"\n",
        "\n",
        "# Get the selected transcript row/chunk from the interface\n",
        "interface_display_transcript_movie_brain(movie_path, transcript_path,\n",
        "    fmri_file_path, atlas_path, dataset_name, hrf_delay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qpNJSczOpuP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seL5oTQ-Ot_K"
      },
      "source": [
        "# 2 | Stimulus feature extraction\n",
        "\n",
        "In this section, you will learn how to extract **visual**, **audio** and **language** features from the multimodal movie stimuli (focusing on the first half of the first episode from the first season of Friends as an example), and reduce their dimensionality using principal component analysis (PCA). The stimulus features will be used in `Section 3` to train encoding models to predict fMRI responses.\n",
        "\n",
        "<font color='red'><b>Note:</b></font> You don't need to save the extracted stimulus features to run the PCA analysis in `Section 2.2`, as there we will provide you with pre-extracted stimulus features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUwanjGGbV9k"
      },
      "source": [
        "## 2.1 | Feature extraction\n",
        "\n",
        "<font color='red'><b>Note:</b></font> Each modality (visual, audio, language) requires different feature extraction steps. You can run the feature extraction of each modality independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZPusSVOgXkK"
      },
      "source": [
        "### 2.1.1 | Visual feature extraction\n",
        "\n",
        "Here, you will extract visual features from the `.mkv` movie files using [`slow_r50`](https://pytorch.org/hub/facebookresearch_pytorchvideo_resnet/), a convolutional neural network pre-trained on action recognition on [Kinetics 400](https://arxiv.org/abs/1705.06950). The visual features consist of `slow_r50` activations for movie frame pixels given as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEaIfIe_r_uu"
      },
      "source": [
        "Prior to inputing them to the model, `slow_r50` requires visual frames to undergo a few preprocessing steps:\n",
        "- **`UniformTemporalSubsample`:** Reduces the number of frames by selecting every 8th frame.\n",
        "- **`Lambda`:** Scales pixel values to a range between 0 and 1.\n",
        "- **`Normalize`:** Centers and scales each RGB video channel using the provided mean and standard deviation.\n",
        "- **`ShortSideScale`:** Determines the shorter spatial dim of the video (i.e. width or height) and scales it to the specified size. To maintain aspect ratio, the longer side is then scaled proportionally.\n",
        "- **`CenterCrop`:** Crops the given video at the center of the frame.\n",
        "\n",
        "<font color='red'><b>Note:</b></font> These preprocessing steps are specific to the `slow_r50` model, and might have to change if using other models for feature extraction.\n",
        "\n",
        "You will begin by defining the preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etjdwLDJXg6s"
      },
      "outputs": [],
      "source": [
        "def define_frames_transform():\n",
        "    \"\"\"Defines the preprocessing pipeline for the video frames. Note that this\n",
        "    transform is specific to the slow_r50 model.\"\"\"\n",
        "    transform = Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(8),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            Normalize([0.45, 0.45, 0.45], [0.225, 0.225, 0.225]),\n",
        "            ShortSideScale(size=256),\n",
        "            CenterCrop(256)\n",
        "        ]\n",
        "  )\n",
        "    return transform\n",
        "\n",
        "transform = define_frames_transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am0oNTv1sb0s"
      },
      "source": [
        "Next, you will load the `slow_r50` model from [PyTorchVideo](https://pytorchvideo.org/), and select `blocks.5.pool` as the model layer from which the visual features will be extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuUzOnEVQaP5"
      },
      "outputs": [],
      "source": [
        "def get_vision_model(device):\n",
        "    \"\"\"\n",
        "    Load a pre-trained slow_r50 video model and set up the feature extractor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    device : torch.device\n",
        "        The device on which the model will run (i.e., 'cpu' or 'cuda').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    feature_extractor : torch.nn.Module\n",
        "        The feature extractor model.\n",
        "    model_layer : str\n",
        "        The layer from which visual features will be extracted.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the model\n",
        "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50',\n",
        "        pretrained=True)\n",
        "\n",
        "    # Select 'blocks.5.pool' as the feature extractor layer\n",
        "    model_layer = 'blocks.5.pool'\n",
        "    feature_extractor = create_feature_extractor(model,\n",
        "        return_nodes=[model_layer])\n",
        "    feature_extractor.to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    return feature_extractor, model_layer\n",
        "\n",
        "feature_extractor, model_layer = get_vision_model(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipZGCp5CspIk"
      },
      "source": [
        "Now, you will extract the visual features following these three steps:\n",
        "\n",
        "1. **Divide the movie stimuli into chunks:** To facilitate alignment with the fMRI responses, you will first divide the visual frames from the `.mkv` files into chunks of 1.49 seconds.\n",
        "2. **Preproces each chunk:** Next, you will preprocess each visual frame chunk independently.\n",
        "3. **Extract Features:** Finally, you will feed the preprocessed chunks into the `slow_r50` model, which will output in an independent sample of visual features for each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAefJQqKudvL"
      },
      "outputs": [],
      "source": [
        "def extract_visual_features(episode_path, tr, feature_extractor, model_layer,\n",
        "    transform, device, save_dir_temp, save_dir_features):\n",
        "    \"\"\"\n",
        "    Extract visual features from a movie using a pre-trained video model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode_path : str\n",
        "        Path to the movie file for which the visual features are extracted.\n",
        "    tr : float\n",
        "        Duration of each chunk, in seconds (aligned with the fMRI repetition\n",
        "        time, or TR).\n",
        "    feature_extractor : torch.nn.Module\n",
        "        Pre-trained feature extractor model.\n",
        "    model_layer : str\n",
        "        The model layer from which the visual features are extracted.\n",
        "    transform : torchvision.transforms.Compose\n",
        "        Transformation pipeline for processing video frames.\n",
        "    device : torch.device\n",
        "        Device for computation ('cpu' or 'cuda').\n",
        "    save_dir_temp : str\n",
        "        Directory where the chunked movie clips are temporarily stored for\n",
        "        feature extraction.\n",
        "    save_dir_features : str\n",
        "        Directory where the extracted visual features are saved.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    visual_features : float\n",
        "        Array containing the extracted visual features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the onset time of each movie chunk\n",
        "    clip = VideoFileClip(episode_path)\n",
        "    start_times = [x for x in np.arange(0, clip.duration, tr)][:-1]\n",
        "    # Create the directory where the movie chunks are temporarily saved\n",
        "    temp_dir = os.path.join(save_dir_temp, 'temp')\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Empty features list\n",
        "    visual_features = []\n",
        "\n",
        "    # Loop over chunks\n",
        "    with tqdm(total=len(start_times), desc=\"Extracting visual features\") as pbar:\n",
        "        for start in start_times:\n",
        "\n",
        "            # Divide the movie in chunks of length TR, and save the resulting\n",
        "            # clips as '.mp4' files\n",
        "            clip_chunk = clip.subclip(start, start+tr)\n",
        "            chunk_path = os.path.join(temp_dir, 'visual_chunk.mp4')\n",
        "            clip_chunk.write_videofile(chunk_path, verbose=False, audio=False,\n",
        "                logger=None)\n",
        "            # Load the frames from the chunked movie clip\n",
        "            video_clip = VideoFileClip(chunk_path)\n",
        "            chunk_frames = [frame for frame in video_clip.iter_frames()]\n",
        "\n",
        "            # Format the frames to shape:\n",
        "            # (batch_size, channels, num_frames, height, width)\n",
        "            frames_array = np.transpose(np.array(chunk_frames), (3, 0, 1, 2))\n",
        "            # Convert the video frames to tensor\n",
        "            inputs = torch.from_numpy(frames_array).float()\n",
        "            # Preprocess the video frames\n",
        "            inputs = transform(inputs).unsqueeze(0).to(device)\n",
        "\n",
        "            # Extract the visual features\n",
        "            with torch.no_grad():\n",
        "                preds = feature_extractor(inputs)\n",
        "            visual_features.append(np.reshape(preds[model_layer].cpu().numpy(), -1))\n",
        "\n",
        "            # Update the progress bar\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Convert the visual features to float32\n",
        "    visual_features = np.array(visual_features, dtype='float32')\n",
        "\n",
        "    # Save the visual features\n",
        "    #out_file_visual = os.path.join(\n",
        "    #    save_dir_features, f'friends_s01e01a_features_visual.h5')\n",
        "    #with h5py.File(out_file_visual, 'a' if Path(out_file_visual).exists() else 'w') as f:\n",
        "    #    group = f.create_group(\"s01e01a\")\n",
        "    #    group.create_dataset('visual', data=visual_features, dtype=np.float32)\n",
        "    #print(f\"Visual features saved to {out_file_visual}\")\n",
        "\n",
        "    # Output\n",
        "    return visual_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clXtxTUIjsU6"
      },
      "outputs": [],
      "source": [
        "# As an exemple, extract visual features for season 1, episode 1 of Friends\n",
        "episode_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e01a.mkv\"\n",
        "\n",
        "# Duration of each movie chunk, aligned with the fMRI TR of 1.49 seconds\n",
        "tr = 1.49\n",
        "\n",
        "# Saving directories\n",
        "save_dir_temp = \"./visual_features\"\n",
        "save_dir_features = root_data_dir +  \"/stimulus_features/raw/visual/\"\n",
        "\n",
        "# Execute visual feature extraction\n",
        "visual_features = extract_visual_features(episode_path, tr, feature_extractor,\n",
        "    model_layer, transform, device, save_dir_temp, save_dir_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01PntMi9iP4Y"
      },
      "source": [
        "Here, you can inspect the extracted visual features by printing their shape, and visualizing the feature vectors for five movie chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dm-MhbrxBNhm"
      },
      "outputs": [],
      "source": [
        "# Print the features shape\n",
        "print(\"Visual features shape for 'friends_s01e01a.mkv':\")\n",
        "print(visual_features.shape)\n",
        "print('(Movie samples × Visual features length)')\n",
        "\n",
        "# Visualize the features for five movie chunks\n",
        "print(\"\\nVisual feature vectors for 5 movie chunks:\\n\")\n",
        "print(visual_features[20:25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkzpLmghPJy5"
      },
      "source": [
        "### 2.1.2 | Audio feature extraction\n",
        "\n",
        "Here, you will extract audio features from the `.mkv` movie files using [`Mel-frequency cepstral coefficients (MFCCs)`](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html), a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency ([Wikipedia](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrYDaWiYyB4D"
      },
      "source": [
        "You will extract the audio features with the following two steps:\n",
        "\n",
        "1. **Divide the movie stimuli into chunks:** to facilitate alignment with the fMRI responses, divide the audio samples from the `.mkv` files into chunks of 1.49 seconds.\n",
        "3. **Extract Features:** compute the MFCCs on each chunk, resulting in an independent sample of audio features for each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLW8KnndFAwL"
      },
      "outputs": [],
      "source": [
        "def extract_audio_features(episode_path, tr, sr, device, save_dir_temp,\n",
        "    save_dir_features):\n",
        "    \"\"\"\n",
        "    Extract audio features from a movie using Mel-frequency cepstral\n",
        "    coefficients (MFCCs).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode_path : str\n",
        "        Path to the movie file for which the audio features are extracted.\n",
        "    tr : float\n",
        "        Duration of each chunk, in seconds (aligned with the fMRI repetition\n",
        "        time, or TR).\n",
        "    sr : int\n",
        "        Audio sampling rate.\n",
        "    device : str\n",
        "        Device to perform computations ('cpu' or 'gpu').\n",
        "    save_dir_temp : str\n",
        "        Directory where the chunked movie clips are temporarily stored for\n",
        "        feature extraction.\n",
        "    save_dir_features : str\n",
        "        Directory where the extracted audio features are saved.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    audio_features : float\n",
        "        Array containing the extracted audio features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the onset time of each movie chunk\n",
        "    clip = VideoFileClip(episode_path)\n",
        "    start_times = [x for x in np.arange(0, clip.duration, tr)][:-1]\n",
        "    # Create the directory where the movie chunks are temporarily saved\n",
        "    temp_dir = os.path.join(save_dir_temp, 'temp')\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Empty features list\n",
        "    audio_features = []\n",
        "\n",
        "    ### Loop over chunks ###\n",
        "    with tqdm(total=len(start_times), desc=\"Extracting audio features\") as pbar:\n",
        "        for start in start_times:\n",
        "\n",
        "            # Divide the movie in chunks of length TR, and save the resulting\n",
        "            # audio clips as '.wav' files\n",
        "            clip_chunk = clip.subclip(start, start+tr)\n",
        "            chunk_audio_path = os.path.join(temp_dir, 'audio_s01e01a.wav')\n",
        "            clip_chunk.audio.write_audiofile(chunk_audio_path, verbose=False,\n",
        "                logger=None)\n",
        "            # Load the audio samples from the chunked movie clip\n",
        "            y, sr = librosa.load(chunk_audio_path, sr=sr, mono=True)\n",
        "\n",
        "            # Extract the audio features (MFCC)\n",
        "            mfcc_features = np.mean(librosa.feature.mfcc(y=y, sr=sr), axis=1)\n",
        "            audio_features.append(mfcc_features)\n",
        "            # Update the progress bar\n",
        "            pbar.update(1)\n",
        "\n",
        "    ### Convert the visual features to float32 ###\n",
        "    audio_features = np.array(audio_features, dtype='float32')\n",
        "\n",
        "    # Save the audio features\n",
        "    #out_file_audio = os.path.join(\n",
        "    #    save_dir_features, f'friends_s01e01a_features_audio.h5')\n",
        "    #with h5py.File(out_file_audio, 'a' if Path(out_file_audio).exists() else 'w') as f:\n",
        "    #    group = f.create_group(\"s01e01a\")\n",
        "    #    group.create_dataset('audio', data=audio_features, dtype=np.float32)\n",
        "    #print(f\"Audio features saved to {out_file_audio}\")\n",
        "\n",
        "    ### Output ###\n",
        "    return audio_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XnGBInMFDil"
      },
      "outputs": [],
      "source": [
        "# As an example, extract audio features using season 1, episode 1 of Friends\n",
        "episode_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e01a.mkv\"\n",
        "\n",
        "# Duration of each movie chunk, aligned with the fMRI TR of 1.49 seconds\n",
        "tr = 1.49\n",
        "\n",
        "# Audio sampling rate\n",
        "sr = 22050\n",
        "\n",
        "# Saving directories\n",
        "save_dir_temp = \"./audio_features\"\n",
        "save_dir_features = root_data_dir +  \"/stimulus_features/raw/audio/\"\n",
        "\n",
        "# Execute audio feature extraction\n",
        "audio_features = extract_audio_features(episode_path, tr, sr, device,\n",
        "    save_dir_temp, save_dir_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY-eRDSvyRrU"
      },
      "source": [
        "Finally, here you can inspect the extracted audio features by printing their shape and visualizing the feature vectors for five movie chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8yAvcHbytHL"
      },
      "outputs": [],
      "source": [
        "# Print the features shape\n",
        "print(\"Audio features shape for 'friends_s01e01a.mkv':\")\n",
        "print(audio_features.shape)\n",
        "print('(Movie samples × Audio features length)')\n",
        "\n",
        "# Visualize the features for five movie chunks\n",
        "print(\"\\nAudio feature vectors for 5 movie chunks:\\n\")\n",
        "print(audio_features[20:25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1ClkTyPPLjm"
      },
      "source": [
        "### 2.1.3 | Language feature extraction\n",
        "\n",
        "Here, you will extract language features from the `.tsv` movie transcripts using the large language model (LLM) [`bert-base-uncased`](https://huggingface.co/google-bert/bert-base-uncased), a BERT transformer pre-trained on a large corpus of English data in a self-supervised fashion. The language features consist of `bert-base-uncased` activations for language transcripts given as input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBlHAZxm0fGM"
      },
      "source": [
        "You will start by loading the pre-trained `bert-base-uncased` model and its corresponding tokenizer, which is used to correclty split the text into words or subwords.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFfTs3oc0hbC"
      },
      "outputs": [],
      "source": [
        "def get_language_model(device):\n",
        "    \"\"\"\n",
        "    Load a pre-trained bert-base-uncased language model and its corresponding\n",
        "    tokenizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    device : torch.device\n",
        "        Device on which the model will run (e.g., 'cpu' or 'cuda').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : object\n",
        "        Pre-trained language model.\n",
        "    tokenizer : object\n",
        "        Tokenizer corresponding to the language model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Load the model ###\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    model.eval().to(device)\n",
        "\n",
        "    ### Load the tokenizer ###\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "        do_lower_case=True)\n",
        "\n",
        "    ### Output ###\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = get_language_model(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm4mKKf2nt8z"
      },
      "source": [
        "Now, you will extract the language features with the following three steps:\n",
        "\n",
        "1. **Load the transcripts:** Load the `.tsv` files containing the movies transcripts. The `.tsv` files organises movie dialogue into chunks of 1.49 seconds of movie, with each row from the `.tsv` corresponding to one such chunk.\n",
        "\n",
        "2. **Tokenize the transcripts:** Use the tokenizer to correclty split the text into words or subwords.\n",
        "\n",
        "3. **Extract Features:** Feed the tokens to the `bert-base-uncased` model to obtain language features for each chunk. <font color='red'><b>Note I:</b></font> Since linguistic meaning typically builds up over temporal scales larger than the chunk length of 1.49 seconds, the model input for each chunk consists of the tokens from the chunk of interest plus *N* tokens from previous chunks. <font color='red'><b>Note II:</b></font> `bert-base-uncased` gives two types of output: `last_hidden_state`, which contains one set of features for each token; and `pooler_output`, which containings one general set of features across all input tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhgTw6Ab5VRq"
      },
      "outputs": [],
      "source": [
        "def extract_language_features(episode_path, model, tokenizer, num_used_tokens,\n",
        "    kept_tokens_last_hidden_state, device, save_dir_features):\n",
        "    \"\"\"\n",
        "    Extract language features from a movie using a pre-trained language model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode_path : str\n",
        "        Path to the movie transcripts for which the language features are\n",
        "        extracted.\n",
        "    model : object\n",
        "        Pre-trained language model.\n",
        "    tokenizer : object\n",
        "        Tokenizer corresponding to the language model.\n",
        "    num_used_tokens : int\n",
        "        Total number of tokens that are fed to the language model for each\n",
        "        chunk, including the tokens from the chunk of interest plus N tokens\n",
        "        from previous chunks (the maximum allowed by the model is 510).\n",
        "    kept_tokens_last_hidden_state : int\n",
        "        Number of features retained for the last_hidden_state, where each\n",
        "        feature corresponds to a token, starting from the most recent token.\n",
        "    device : str\n",
        "        Device to perform computations ('cpu' or 'gpu').\n",
        "    save_dir_features : str\n",
        "        Directory where the extracted language features are saved.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pooler_output : list\n",
        "        List containing the pooler_output features for each chunk.\n",
        "    last_hidden_state : list\n",
        "        List containing the last_hidden_state features for each chunk\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Load the transcript ###\n",
        "    df = pd.read_csv(episode_path, sep='\\t')\n",
        "    df.insert(loc=0, column=\"is_na\", value=df[\"text_per_tr\"].isna())\n",
        "\n",
        "    ### Initialize the tokens and features lists ###\n",
        "    tokens, np_tokens, pooler_output, last_hidden_state = [], [], [], []\n",
        "\n",
        "    ### Loop over text chunks ###\n",
        "    for i in tqdm(range(df.shape[0]), desc=\"Extracting language features\"):\n",
        "\n",
        "        ### Tokenize raw text ###\n",
        "        if not df.iloc[i][\"is_na\"]: # Only tokenize if words were spoken during a chunk (i.e., if the chunk is not empty)\n",
        "            # Tokenize raw text with puntuation (for pooler_output features)\n",
        "            tr_text = df.iloc[i][\"text_per_tr\"]\n",
        "            tokens.extend(tokenizer.tokenize(tr_text))\n",
        "            # Tokenize without punctuation (for last_hidden_state features)\n",
        "            tr_np_tokens = tokenizer.tokenize(\n",
        "                tr_text.translate(str.maketrans('', '', string.punctuation)))\n",
        "            np_tokens.extend(tr_np_tokens)\n",
        "\n",
        "        ### Extract the pooler_output features ###\n",
        "        if len(tokens) > 0: # Only extract features if there are tokens available\n",
        "            # Select the number of tokens used from the current and past chunks,\n",
        "            # and convert them into IDs\n",
        "            used_tokens = tokenizer.convert_tokens_to_ids(\n",
        "                tokens[-(num_used_tokens):])\n",
        "            # IDs 101 and 102 are special tokens that indicate the beginning and\n",
        "            # end of an input sequence, respectively.\n",
        "            input_ids = [101] + used_tokens + [102]\n",
        "            tensor_tokens = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "            # Extract and store the pooler_output features\n",
        "            with torch.no_grad():\n",
        "                outputs = model(tensor_tokens)\n",
        "                pooler_output.append(outputs['pooler_output'][0].cpu().numpy())\n",
        "        else: # Store NaN values if no tokes are available\n",
        "            pooler_output.append(np.full(768, np.nan, dtype='float32'))\n",
        "\n",
        "        ### Extract the last_hidden_state features ###\n",
        "        if len(np_tokens) > 0: # Only extract features if there are tokens available\n",
        "            np_feat = np.full((kept_tokens_last_hidden_state, 768), np.nan, dtype='float32')\n",
        "            # Select the number of tokens used from the current and past chunks,\n",
        "            # and convert them into IDs\n",
        "            used_tokens = tokenizer.convert_tokens_to_ids(\n",
        "                np_tokens[-(num_used_tokens):])\n",
        "            # IDs 101 and 102 are special tokens that indicate the beginning and\n",
        "            # end of an input sequence, respectively.\n",
        "            np_input_ids = [101] + used_tokens + [102]\n",
        "            np_tensor_tokens = torch.tensor(np_input_ids).unsqueeze(0).to(device)\n",
        "            # Extract and store the last_hidden_state features\n",
        "            with torch.no_grad():\n",
        "                np_outputs = model(np_tensor_tokens)\n",
        "                np_outputs = np_outputs['last_hidden_state'][0][1:-1].cpu().numpy()\n",
        "            tk_idx = min(kept_tokens_last_hidden_state, len(np_tokens))\n",
        "            np_feat[-tk_idx:, :] = np_outputs[-tk_idx:]\n",
        "            last_hidden_state.append(np_feat)\n",
        "        else: # Store NaN values if no tokens are available\n",
        "            last_hidden_state.append(np.full(\n",
        "                (kept_tokens_last_hidden_state, 768), np.nan, dtype='float32'))\n",
        "\n",
        "    ### Convert the language features to float32 ###\n",
        "    pooler_output = np.array(pooler_output, dtype='float32')\n",
        "    last_hidden_state = np.array(last_hidden_state, dtype='float32')\n",
        "\n",
        "    ### Save the language features ###\n",
        "    #out_file_language = os.path.join(\n",
        "    #    save_dir_features, f'friends_s01e01a_features_language.h5')\n",
        "    #with h5py.File(out_file_language, 'a' if Path(out_file_language).exists() else 'w') as f:\n",
        "    #    group = f.create_group(\"s01e01a\")\n",
        "    #    group.create_dataset('language_pooler_output', data=pooler_output,\n",
        "    #        dtype=np.float32)\n",
        "    #    group.create_dataset('language_last_hidden_state',\n",
        "    #        data=last_hidden_state, dtype=np.float32)\n",
        "    #print(f\"Language features saved to {out_file_language}\")\n",
        "\n",
        "    ### Output ###\n",
        "    return pooler_output, last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8cCDlmzOimK"
      },
      "outputs": [],
      "source": [
        "# As an exemple, extract language features using season 1, episode 1 of Friends\n",
        "episode_path = root_data_dir + \"/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e01a.tsv\"\n",
        "\n",
        "# Saving directory\n",
        "save_dir_features = root_data_dir +  \"/stimulus_features/raw/language/\"\n",
        "\n",
        "# Other parameters\n",
        "num_used_tokens = 510\n",
        "kept_tokens_last_hidden_state = 10\n",
        "\n",
        "# Execute language feature extraction\n",
        "pooler_output, last_hidden_state = extract_language_features(episode_path,\n",
        "    model, tokenizer, num_used_tokens, kept_tokens_last_hidden_state, device,\n",
        "    save_dir_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVYdsu_B0_Fw"
      },
      "source": [
        "Here, you can inspect the extracted language features by printing their shape and visualizing the feature vectors for five movie chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrS8Zom3pfSQ"
      },
      "outputs": [],
      "source": [
        "# Print the features shape\n",
        "# pooler_output\n",
        "print(\"pooler_output features shape for 'friends_s01e01a.mkv':\")\n",
        "print(pooler_output.shape)\n",
        "print('(Movie samples × pooler_output features length)')\n",
        "# last_hidden_state\n",
        "print(\"\\nlast_hidden_state features shape for 'friends_s01e01a.mkv':\")\n",
        "print(last_hidden_state.shape)\n",
        "print('(Movie samples × Kept tokens × pooler_output features length)')\n",
        "\n",
        "# Visualize the features for five movie chunks\n",
        "# pooler_output\n",
        "print(\"\\npooler_output features for 5 movie chunks:\\n\")\n",
        "print(pooler_output[20:25])\n",
        "# last_hidden_state\n",
        "print(\"\\nlast_hidden_state features for 5 movie chunks:\\n\")\n",
        "print(last_hidden_state[20:25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHba_lYRrNdo"
      },
      "source": [
        "## 2.2 | Reduce the stimulus features dimensionality using PCA\n",
        "\n",
        "Here, you will reduce the dimensionality of the extracted stimulus features using principal component analysis (PCA). By removing reduntant or uninformative dimensions, the stimulus features will decrease in size while still retaining the most important stimulus information, thus reducing computational cost when used to train fMRI encoding models. You will, as an example, apply PCA on the extracted stimulus features for the first episode of the first season of Friends, which can be found at `../stimulus_features/raw/<modality>/friends_s01e01a_features_<modality>.h5`, where:\n",
        "- **`modality`:** String indicating the stimulus modality of the extracted features. Options are: [`\"visual\"`, `\"audio\"`, `\"language\"`, `\"all\"`]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLkdxOEB-9rp"
      },
      "source": [
        "This function loads the extracted stimulus features. Since PCA requires data to be in a `(Samples × Features)` format, the `pooler_output` and `last_hidden_state` language features are vectorized and appended to each other during loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gK2pJk_-4fz"
      },
      "outputs": [],
      "source": [
        "def load_features(root_data_dir, modality):\n",
        "    \"\"\"\n",
        "    Load the extracted features from the HDF5 file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_data_dir : str\n",
        "        Root data directory.\n",
        "    modality : str\n",
        "        The modality of the features ('visual', 'audio', or 'language').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features : float\n",
        "        Stimulus features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Get the stimulus features file directory ###\n",
        "    data_dir = os.path.join(root_data_dir, 'stimulus_features', 'raw', modality,\n",
        "        'friends_s01e01a_features_'+modality+'.h5')\n",
        "\n",
        "    ### Load the stimulus features ###\n",
        "    with h5py.File(data_dir, 'r') as data:\n",
        "        for episode in data.keys():\n",
        "            if modality != 'language':\n",
        "                features = np.asarray(data[episode][modality])\n",
        "            else:\n",
        "                # Vectorize and append pooler_output and last_hidden_state\n",
        "                # language features\n",
        "                pooler_output = np.asarray(\n",
        "                    data[episode][modality+'_pooler_output'])\n",
        "                last_hidden = np.asarray(np.reshape(\n",
        "                    data[episode][modality+'_last_hidden_state'],\n",
        "                    (len(pooler_output), -1)))\n",
        "                features = np.append(pooler_output, last_hidden, axis=1)\n",
        "    print(f\"{modality} features original shape: {features.shape}\")\n",
        "    print('(Movie samples × Features)')\n",
        "\n",
        "    ### Output ###\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqqz3LEf_ZDX"
      },
      "source": [
        "This function replaces `NaN` values in the stimulus features with zeros, and then *z*-scores the features (to correctly identify the directions that maximize the variance in the data, PCA requires the data to be centered and scaled)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4JdZIDW_I11"
      },
      "outputs": [],
      "source": [
        "def preprocess_features(features):\n",
        "    \"\"\"\n",
        "    Rplaces NaN values in the stimulus features with zeros, and z-score the\n",
        "    features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    features : float\n",
        "        Stimulus features.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    prepr_features : float\n",
        "        Preprocessed stimulus features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Convert NaN values to zeros ###\n",
        "    features = np.nan_to_num(features)\n",
        "\n",
        "    ### Z-score the features ###\n",
        "    scaler = StandardScaler()\n",
        "    prepr_features = scaler.fit_transform(features)\n",
        "\n",
        "    ### Output ###\n",
        "    return prepr_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWaQVmb9_ex6"
      },
      "source": [
        "This function runs a PCA on the stimulus features, reducing their dimensionality to the specified number of principal components (PCs) to retain. Note that the number of PCs cannot exceed the dimensionality of the stimulus features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtjQmjre_jHV"
      },
      "outputs": [],
      "source": [
        "def perform_pca(prepr_features, n_components):\n",
        "    \"\"\"\n",
        "    Perform PCA on the standardized features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prepr_features : float\n",
        "        Preprocessed stimulus features.\n",
        "    n_components : int\n",
        "        Number of components to keep\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features_pca : float\n",
        "        PCA-downsampled stimulus features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Set the number of principal components to keep ###\n",
        "    # If number of PCs is larger than the number of features, set the PC number\n",
        "    # to the number of features\n",
        "    if n_components > prepr_features.shape[1]:\n",
        "        n_components = prepr_features.shape[1]\n",
        "\n",
        "    ### Perform PCA ###n_init=4, max_iter=300\n",
        "    pca = PCA(n_components, random_state=20200220)\n",
        "    features_pca = pca.fit_transform(prepr_features)\n",
        "    print(f\"\\n{modality} features PCA shape: {features_pca.shape}\")\n",
        "    print('(Movie samples × Principal components)')\n",
        "\n",
        "    ### Output ###\n",
        "    return features_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfNwdpk0_2GU"
      },
      "source": [
        "Here, you can choose the stimulus input modality, specify the number of principal components to retain, and run the PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUJb5bo9_yea"
      },
      "outputs": [],
      "source": [
        "# Choose modality and PCs\n",
        "modality = \"visual\" #@param [\"visual\", \"audio\", \"language\"]\n",
        "n_components = 250 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
        "\n",
        "# Load the stimulus features\n",
        "features = load_features(root_data_dir, modality)\n",
        "\n",
        "# Preprocess the stimulus features\n",
        "prepr_features = preprocess_features(features)\n",
        "\n",
        "# Perform PCA\n",
        "features_pca = perform_pca(prepr_features, n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stE7e8HaOtIX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtYLbKz9SDr0"
      },
      "source": [
        "# 3 | fMRI encoding model training and validation\n",
        "\n",
        "In this section, you will learn how to **train** and **validate** an encoding model that takes movie stimuli as input (in the form of stimulus features) and predicts the corresponding fMRI responses elicited when a human subject watches the corresponding movie segment. You will **train** and **validate** the encoding model using PCA-downsampled stimulus features and whole-brain parcellated fMRI responses for seasons 1 to 6 of Friends and Movie10.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1MOoUHm0-EtfAoApcNA3NzWnm9S72HmCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5p9qEtRjUHB"
      },
      "source": [
        "## 3.1 | Choose the encoding parameters\n",
        "\n",
        "Start by selecting some parameters relative to the encoding model training and validation:\n",
        "\n",
        "- **`subject`:** Integer indicating the subject on whose data the encoding model is trained and validated. The four challenge subject numbers are [`1`, `2`, `3`, `5`].\n",
        "- **`modality`:** String indicating the stimulus feature modality used to train and validate the encoding model. Available modality options are [`\"visual\"`, `\"audio\"`, `\"language\"`, `\"all\"`].\n",
        "- **`excluded_samples_start`:** Integer indicating the first *N* fMRI samples that will be excluded and not used for model training. The reason for excluding these samples is that due to the latency of the hemodynamic response the fMRI responses of first few fMRI samples contain less information than later samples. Note that this parameter is different from the `hrf_delay` parameter explained below, controls the delay between stimulus samples and fMRI samples.\n",
        "- **`excluded_samples_end`:** Integer indicating the last *N* fMRI samples that will be excluded and not used for model training. The reason for excluding these samples is that stimulus feature samples can be shorter than the fMRI samples, since in some cases the fMRI run ran longer than the actual movie. However, note that **<font color='red'><b>the fMRI timeseries onset is ALWAYS SYNCHRONIZED with movie onset (i.e., the first fMRI sample is always synchronized with the first stimulus sample).</b></font>**\n",
        "- **`hrf_delay`:** fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal that reflects changes in blood oxygenation levels in response to activity in the brain. Blood flow increases to a given brain region in response to its activity. This vascular response, which follows the hemodynamic response function (HRF), takes time. Typically, the HRF peaks around 5–6 seconds after a neural event: this delay reflects the time needed for blood oxygenation changes to propagate and for the fMRI signal to capture them. Therefore, this parameter introduces a delay between stimulus samples and fMRI samples for a better correspondence between input stimuli and the brain response. For example, with a `hrf_delay` of 3, if the fMRI sample of interest is 20, the corresponding stimulus sample will be 17.\n",
        "- **`stimulus_window`:** Integer indicating how many stimulus feature samples are used to model each fMRI sample, starting from the stimulus sample corresponding to the fMRI sample of interest, minus the `hrf_delay`, and going back in time. For example, with a `stimulus_window` of 5, and a `hrf_delay` of 3, if the fMRI sample of interest is 20, it will be modeled with stimulus samples [13, 14, 15, 16, 17]. Note that this only applies to visual and audio features, since the language features were already extracted using transcript words spanning several movie samples (thus, each fMRI sample will only be modeled using the corresponding language feature sample, minus the `hrf_delay`). Also note that a larger stimulus window will increase compute time, since it increases the amount of stimulus features used to train and validate the fMRI encoding models.\n",
        "- **`movies_train`:** List of strings indicating the movies used to train the encoding model, out of the first six seasons of Friends [`\"friends-s01\"`, `\"friends-s02\"`, `\"friends-s03\"`, `\"friends-s04\"`, `\"friends-s05\"`, `\"friends-s06\"`], and the four movies from Movie10 [`\"movie10-bourne\"`, `\"movie10-figures\"`, `\"movie10-life\"`, `\"movie10-wolf\"`].\n",
        "- **`movies_val`:** List of strings indicating the movies used to validate the encoding model, out of the first six seasons of Friends [`\"friends-s01\"`, `\"friends-s02\"`, `\"friends-s03\"`, `\"friends-s04\"`, `\"friends-s05\"`, `\"friends-s06\"`], and the four movies from Movie10 [`\"movie10-bourne\"`, `\"movie10-figures\"`, `\"movie10-life\"`, `\"movie10-wolf\"`]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUFxTpSNjcNa"
      },
      "outputs": [],
      "source": [
        "subject = 1  #@param [\"1\", \"2\", \"3\", \"5\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "modality = \"all\"  #@param [\"visual\", \"audio\", \"language\", \"all\"]\n",
        "\n",
        "excluded_samples_start = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "\n",
        "excluded_samples_end = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "\n",
        "hrf_delay = 3  #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "stimulus_window = 5  #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "movies_train = [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\", \"friends-s05\", \"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"] # @param {allow-input: true}\n",
        "\n",
        "movies_val = [\"friends-s06\"] # @param {allow-input: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TUIvoOgiTLF"
      },
      "source": [
        "## 3.2 | Load the stimulus features and fMRI responses\n",
        "\n",
        "Here, you will load the stimulus features and fMRI responses for seasons 1 to 6 of Friends plus Movie10, which you will use later to train and validate an encoding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LeQ8cj-uMS7"
      },
      "source": [
        "### 3.2.1 | Load the stimulus features\n",
        "\n",
        "You will begin by loading the pre-computed and PCA-downsampled stimulus features for the chosen stimulus modality, which can be found at `../stimulus_features/pca/friends_movie10/<modality>/features_train.npy`, where:\n",
        "- **`modality`:** String indicating the stimulus modality of the extracted features. Options are: [`\"visual\"`, `\"audio\"`, `\"language\"`, `\"all\"`].\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font> These are NOT the PCA-downsampled stimulus features created in `Section 2.2`. In Section 2.2 you applied PCA on the extracted stimulus features for only the first episode of the first season of Friends (for demostrational purposes). Instead, here you will load the pre-computed and PCA-downsampled stimulus features for seasons 1 to 6 of Friends, and for the Movie10 movies. To create these PCA-downsampled features we applied PCA jointly on all features from all Friends and Movie 10 stimuli (using [this code](https://github.com/courtois-neuromod/algonauts_2025.competitors/tree/main/code/challenge_baseline_model/01_stimulus_feature_extraction)). As a consequence, the resulting PCA-downsampled features will be numerically different from the PCA-downsampled features from Section 2.2, since they were created using features from a single Friends episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUSohtCMuYBa"
      },
      "outputs": [],
      "source": [
        "def load_stimulus_features(root_data_dir, modality):\n",
        "    \"\"\"\n",
        "    Load the stimulus features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_data_dir : str\n",
        "        Root data directory.\n",
        "    modality : str\n",
        "        Used feature modality.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features : dict\n",
        "        Dictionary containing the stimulus features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    ### Load the visual features ###\n",
        "    if modality == 'visual' or modality == 'all':\n",
        "        stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
        "            'friends_movie10', 'visual', 'features_train.npy')\n",
        "        features['visual'] = np.load(stimuli_dir, allow_pickle=True).item()\n",
        "\n",
        "    ### Load the audio features ###\n",
        "    if modality == 'audio' or modality == 'all':\n",
        "        stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
        "            'friends_movie10', 'audio', 'features_train.npy')\n",
        "        features['audio'] = np.load(stimuli_dir, allow_pickle=True).item()\n",
        "\n",
        "    ### Load the language features ###\n",
        "    if modality == 'language' or modality == 'all':\n",
        "        stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
        "            'friends_movie10', 'language', 'features_train.npy')\n",
        "        features['language'] = np.load(stimuli_dir, allow_pickle=True).item()\n",
        "\n",
        "    ### Output ###\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_wMS5It-Zq3"
      },
      "outputs": [],
      "source": [
        "# Load the stimulus features\n",
        "features = load_stimulus_features(root_data_dir, modality)\n",
        "\n",
        "# Print all available movie splits for each stimulus modality\n",
        "for key_modality, value_modality in features.items():\n",
        "    print(f\"\\n{key_modality} features movie splits name and shape:\")\n",
        "    for key_movie, value_movie in value_modality.items():\n",
        "        print(key_movie + \" \" + str(value_movie.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_M6fovuiFgm"
      },
      "source": [
        "### 3.2.2 | Load the fMRI responses\n",
        "\n",
        "Next, you will load the fMRI responses for the subject of your choice, which can be found at `../algonauts_2025.competitors/fmri/sub-0<subject>/func/`, where:\n",
        "- **`subject`:** Integer indicating the subject whose data the encoding model is trained and validated on. Options are: [`1`, `2`, `3`, `5`]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj_p_AxtiJnc"
      },
      "outputs": [],
      "source": [
        "def load_fmri(root_data_dir, subject):\n",
        "    \"\"\"\n",
        "    Load the fMRI responses for the selected subject.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_data_dir : str\n",
        "        Root data directory.\n",
        "    subject : int\n",
        "        Subject used to train and validate the encoding model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fmri : dict\n",
        "        Dictionary containing the  fMRI responses.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    fmri = {}\n",
        "\n",
        "    ### Load the fMRI responses for Friends ###\n",
        "    # Data directory\n",
        "    fmri_file = f'sub-0{subject}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5'\n",
        "    fmri_dir = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
        "        'fmri', f'sub-0{subject}', 'func', fmri_file)\n",
        "    # Load the the fMRI responses\n",
        "    fmri_friends = h5py.File(fmri_dir, 'r')\n",
        "    for key, val in fmri_friends.items():\n",
        "        fmri[str(key[13:])] = val[:].astype(np.float32)\n",
        "    del fmri_friends\n",
        "\n",
        "    ### Load the fMRI responses for Movie10 ###\n",
        "    # Data directory\n",
        "    fmri_file = f'sub-0{subject}_task-movie10_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_bold.h5'\n",
        "    fmri_dir = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
        "        'fmri', f'sub-0{subject}', 'func', fmri_file)\n",
        "    # Load the the fMRI responses\n",
        "    fmri_movie10 = h5py.File(fmri_dir, 'r')\n",
        "    for key, val in fmri_movie10.items():\n",
        "        fmri[key[13:]] = val[:].astype(np.float32)\n",
        "    del fmri_movie10\n",
        "    # Average the fMRI responses across the two repeats for 'figures'\n",
        "    keys_all = fmri.keys()\n",
        "    figures_splits = 12\n",
        "    for s in range(figures_splits):\n",
        "        movie = 'figures' + format(s+1, '02')\n",
        "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
        "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
        "        del fmri[keys_movie[0]]\n",
        "        del fmri[keys_movie[1]]\n",
        "    # Average the fMRI responses across the two repeats for 'life'\n",
        "    keys_all = fmri.keys()\n",
        "    life_splits = 5\n",
        "    for s in range(life_splits):\n",
        "        movie = 'life' + format(s+1, '02')\n",
        "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
        "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
        "        del fmri[keys_movie[0]]\n",
        "        del fmri[keys_movie[1]]\n",
        "\n",
        "    ### Output ###\n",
        "    return fmri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uIrriXciPHR"
      },
      "outputs": [],
      "source": [
        "# Load the fMRI responses\n",
        "fmri = load_fmri(root_data_dir, subject)\n",
        "\n",
        "# Print all available movies\n",
        "print(f\"Subject {subject} fMRI movies splits name and shape:\")\n",
        "for key, value in fmri.items():\n",
        "    print(key + \" \" + str(value.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMUzDMX7clFe"
      },
      "source": [
        "## 3.3 | Encoding model training\n",
        "\n",
        "Now that you loaded the stimulus features and fMRI responses for seasons 1 to 6 of Friends plus Movie10, you will use the datasets specified with the `movies_train` parameter to train an encoding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ieV6w8Vcfk4"
      },
      "source": [
        "### 3.3.1 | Align the stimulus features and fMRI response samples for the train set\n",
        "\n",
        "Here, you will select and align the stimulus features and fMRI response samples for the specified training movies, and prepare these data in the right format to train an encoding model. The goal is to bring the fMRI responses to a `(Train Samples × Parcels)` format, and the stimulus features to a `(Train Samples × Features)` format, with the `Train samples` dimension matching between both arrays. You will use these formatted data later to train an encoding model.\n",
        "\n",
        "While selecting the fMRI responses for the train set, the first and last fMRI samples are excluded based on the `excluded_samples_start` and `excluded_samples_end` variables.\n",
        "\n",
        "First, the stimulus features for the train set are selected based on available fMRI responses for a given subject: if a subject is missing fMRI responses for a given movie split, the corresponding stimulus features for that split will not be loaded. Next, the stimulus feature samples are aligned with the fMRI response samples (using the `excluded_samples_start` and `hrf_delay` variables). Since fMRI responses are influenced by stimulation up to several seconds in the past, for the visual and audio modalities the *N* stimulus feature samples up to the fMRI sample of interest minus the `hrf_delay` are appended and used to model this fMRI sample, where *N* is defined by the `stimulus_window` variable. Since the language features were already extracted using transcript words spanning the duration of several movie samples, each fMRI sample will only be modeled using the corresponding language feature sample minus the `hrf_delay`. Finally, the features of different modalities are concatenated together to model fMRI responses using a multi-modal stimulus feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poapL5RjE3sL"
      },
      "outputs": [],
      "source": [
        "def align_features_and_fmri_samples(features, fmri, excluded_samples_start,\n",
        "    excluded_samples_end, hrf_delay, stimulus_window, movies):\n",
        "    \"\"\"\n",
        "    Align the stimulus feature with the fMRI response samples for the selected\n",
        "    movies, later used to train and validate the encoding models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    features : dict\n",
        "        Dictionary containing the stimulus features.\n",
        "    fmri : dict\n",
        "        Dictionary containing the fMRI responses.\n",
        "    excluded_trs_start : int\n",
        "        Integer indicating the first N fMRI TRs that will be excluded and not\n",
        "        used for model training. The reason for excluding these TRs is that due\n",
        "        to the latency of the hemodynamic response the fMRI responses of first\n",
        "        few fMRI TRs do not yet contain stimulus-related information.\n",
        "    excluded_trs_end : int\n",
        "        Integer indicating the last N fMRI TRs that will be excluded and not\n",
        "        used for model training. The reason for excluding these TRs is that\n",
        "        stimulus feature samples (i.e., the stimulus chunks) can be shorter than\n",
        "        the fMRI samples (i.e., the fMRI TRs), since in some cases the fMRI run\n",
        "        ran longer than the actual movie. However, keep in mind that the fMRI\n",
        "        timeseries onset is ALWAYS SYNCHRONIZED with movie onset (i.e., the\n",
        "        first fMRI TR is always synchronized with the first stimulus chunk).\n",
        "    hrf_delay : int\n",
        "        fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
        "        that reflects changes in blood oxygenation levels in response to\n",
        "        activity in the brain. Blood flow increases to a given brain region in\n",
        "        response to its activity. This vascular response, which follows the\n",
        "        hemodynamic response function (HRF), takes time. Typically, the HRF\n",
        "        peaks around 5–6 seconds after a neural event: this delay reflects the\n",
        "        time needed for blood oxygenation changes to propagate and for the fMRI\n",
        "        signal to capture them. Therefore, this parameter introduces a delay\n",
        "        between stimulus chunks and fMRI samples for a better correspondence\n",
        "        between input stimuli and the brain response. For example, with a\n",
        "        hrf_delay of 3, if the stimulus chunk of interest is 17, the\n",
        "        corresponding fMRI sample will be 20.\n",
        "    stimulus_window : int\n",
        "        Integer indicating how many stimulus features' chunks are used to model\n",
        "        each fMRI TR, starting from the chunk corresponding to the TR of\n",
        "        interest, and going back in time. For example, with a stimulus_window of\n",
        "        5, if the fMRI TR of interest is 20, it will be modeled with stimulus\n",
        "        chunks [16, 17, 18, 19, 20]. Note that this only applies to visual and\n",
        "        audio features, since the language features were already extracted using\n",
        "        transcript words spanning several movie chunks (thus, each fMRI TR will\n",
        "        only be modeled using the corresponding language feature chunk). Also\n",
        "        note that a larger stimulus window will increase compute time, since it\n",
        "        increases the amount of stimulus features used to train and test the\n",
        "        fMRI encoding models.\n",
        "    movies: list\n",
        "        List of strings indicating the movies for which the fMRI responses and\n",
        "        stimulus features are aligned, out of the first six seasons of Friends\n",
        "        [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\",\n",
        "        \"friends-s05\", \"friends-s06\"], and the four movies from Movie10\n",
        "        [\"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"].\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    aligned_features : float\n",
        "        Aligned stimulus features for the selected movies.\n",
        "    aligned_fmri : float\n",
        "        Aligned fMRI responses for the selected movies.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Empty data variables ###\n",
        "    aligned_features = []\n",
        "    aligned_fmri = np.empty((0,1000), dtype=np.float32)\n",
        "\n",
        "    ### Loop across movies ###\n",
        "    for movie in movies:\n",
        "\n",
        "        ### Get the IDs of all movies splits for the selected movie ###\n",
        "        if movie[:7] == 'friends':\n",
        "            id = movie[8:]\n",
        "        elif movie[:7] == 'movie10':\n",
        "            id = movie[8:]\n",
        "        movie_splits = [key for key in fmri if id in key[:len(id)]]\n",
        "\n",
        "        ### Loop over movie splits ###\n",
        "        for split in movie_splits:\n",
        "\n",
        "            ### Extract the fMRI ###\n",
        "            fmri_split = fmri[split]\n",
        "            # Exclude the first and last fMRI samples\n",
        "            fmri_split = fmri_split[excluded_samples_start:-excluded_samples_end]\n",
        "            aligned_fmri = np.append(aligned_fmri, fmri_split, 0)\n",
        "\n",
        "            ### Loop over fMRI samples ###\n",
        "            for s in range(len(fmri_split)):\n",
        "                # Empty variable containing the stimulus features of all\n",
        "                # modalities for each fMRI sample\n",
        "                f_all = np.empty(0)\n",
        "\n",
        "                ### Loop across modalities ###\n",
        "                for mod in features.keys():\n",
        "\n",
        "                    ### Visual and audio features ###\n",
        "                    # If visual or audio modality, model each fMRI sample using\n",
        "                    # the N stimulus feature samples up to the fMRI sample of\n",
        "                    # interest minus the hrf_delay (where N is defined by the\n",
        "                    # 'stimulus_window' variable)\n",
        "                    if mod == 'visual' or mod == 'audio':\n",
        "                        # In case there are not N stimulus feature samples up to\n",
        "                        # the fMRI sample of interest minus the hrf_delay (where\n",
        "                        # N is defined by the 'stimulus_window' variable), model\n",
        "                        # the fMRI sample using the first N stimulus feature\n",
        "                        # samples\n",
        "                        if s < (stimulus_window + hrf_delay):\n",
        "                            idx_start = excluded_samples_start\n",
        "                            idx_end = idx_start + stimulus_window\n",
        "                        else:\n",
        "                            idx_start = s + excluded_samples_start - hrf_delay \\\n",
        "                                - stimulus_window + 1\n",
        "                            idx_end = idx_start + stimulus_window\n",
        "                        # In case there are less visual/audio feature samples\n",
        "                        # than fMRI samples minus the hrf_delay, use the last N\n",
        "                        # visual/audio feature samples available (where N is\n",
        "                        # defined by the 'stimulus_window' variable)\n",
        "                        if idx_end > (len(features[mod][split])):\n",
        "                            idx_end = len(features[mod][split])\n",
        "                            idx_start = idx_end - stimulus_window\n",
        "                        f = features[mod][split][idx_start:idx_end]\n",
        "                        f_all = np.append(f_all, f.flatten())\n",
        "\n",
        "                    ### Language features ###\n",
        "                    # Since language features already consist of embeddings\n",
        "                    # spanning several samples, only model each fMRI sample\n",
        "                    # using the corresponding stimulus feature sample minus the\n",
        "                    # hrf_delay\n",
        "                    elif mod == 'language':\n",
        "                        # In case there are no language features for the fMRI\n",
        "                        # sample of interest minus the hrf_delay, model the fMRI\n",
        "                        # sample using the first language feature sample\n",
        "                        if s < hrf_delay:\n",
        "                            idx = excluded_samples_start\n",
        "                        else:\n",
        "                            idx = s + excluded_samples_start - hrf_delay\n",
        "                        # In case there are fewer language feature samples than\n",
        "                        # fMRI samples minus the hrf_delay, use the last\n",
        "                        # language feature sample available\n",
        "                        if idx >= (len(features[mod][split]) - hrf_delay):\n",
        "                            f = features[mod][split][-1,:]\n",
        "                        else:\n",
        "                            f = features[mod][split][idx]\n",
        "                        f_all = np.append(f_all, f.flatten())\n",
        "\n",
        "                 ### Append the stimulus features of all modalities for this sample ###\n",
        "                aligned_features.append(f_all)\n",
        "\n",
        "    ### Convert the aligned features to a numpy array ###\n",
        "    aligned_features = np.asarray(aligned_features, dtype=np.float32)\n",
        "\n",
        "    ### Output ###\n",
        "    return aligned_features, aligned_fmri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCdd7Q7qHR1R"
      },
      "outputs": [],
      "source": [
        "# Align the stimulus features with the fMRI responses for the training movies\n",
        "features_train, fmri_train = align_features_and_fmri_samples(features, fmri,\n",
        "    excluded_samples_start, excluded_samples_end, hrf_delay, stimulus_window,\n",
        "    movies_train)\n",
        "\n",
        "# Print the shape of the training fMRI responses and stimulus features: note\n",
        "# that the two have the same sample size!\n",
        "print(\"Training fMRI responses shape:\")\n",
        "print(fmri_train.shape)\n",
        "print('(Train samples × Parcels)')\n",
        "print(\"\\nTraining stimulus features shape:\")\n",
        "print(features_train.shape)\n",
        "print('(Train samples × Features)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mgYVLz2lsNS"
      },
      "source": [
        "### 3.3.2 | Train the encoding model\n",
        "\n",
        "In this step, you will train an encoding model to predict fMRI responses to movie watching. The encoding model is based on a linear regression that learns a linear mapping from the stimulus features to the fMRI responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-ZJ9V3AWf-U"
      },
      "outputs": [],
      "source": [
        "def train_encoding(features_train, fmri_train):\n",
        "    \"\"\"\n",
        "    Train a linear-regression-based encoding model to predict fMRI responses\n",
        "    using movie features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    features_train : float\n",
        "        Stimulus features for the training movies.\n",
        "    fmri_train : float\n",
        "        fMRI responses for the training movies.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : object\n",
        "        Trained regression model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Train the linear regression model ###\n",
        "    model = LinearRegression().fit(features_train, fmri_train)\n",
        "\n",
        "    ### Output ###\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-x7zV_C8Dj3"
      },
      "outputs": [],
      "source": [
        "# Train the encoding model\n",
        "model = train_encoding(features_train, fmri_train)\n",
        "\n",
        "# Remove unused variables from memory\n",
        "del features_train, fmri_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsFILyiOvelo"
      },
      "source": [
        "## 3.4 | Encoding model validation\n",
        "\n",
        "Now that you trained an encoding model, you will validate its prediction accuracy using the datasets listed with the `movies_val` parameter. Specifically, you will compare the fMRI response predictions of your encoding model for the validation movies with the corresponding recorded (ground truth) fMRI responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPkQXimX_wRA"
      },
      "source": [
        "### 3.4.1 | Align the stimulus feature and fMRI response samples for the validation set\n",
        "\n",
        "Here, you will align the stimulus feature and fMRI response samples for the selected validation dataset, and prepare these data in the right format for input to the trained encoding model. The goal is to bring the fMRI responses to a `(Validation Samples × Parcels)` format, and the stimulus features to a `(Validation Samples × Features)` format, with the `Validation samples` dimension matching between both arrays. You will use these validation stimulus features to predict fMRI responses to watching the validation movies, and compare these predictions against the actual (ground truth) validation fMRI responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS-bTRnMAH0p"
      },
      "outputs": [],
      "source": [
        "# Align the stimulus features with the fMRI responses for the validation movies\n",
        "features_val, fmri_val = align_features_and_fmri_samples(features, fmri,\n",
        "    excluded_samples_start, excluded_samples_end, hrf_delay, stimulus_window,\n",
        "    movies_val)\n",
        "\n",
        "# Remove unused variables from memory\n",
        "del features, fmri\n",
        "\n",
        "# Print the shape of the test fMRI responses and stimulus features: note\n",
        "# that the two have the same sample size!\n",
        "print(\"Validation fMRI responses shape:\", fmri_val.shape)\n",
        "print('(Validation samples × Parcels)')\n",
        "print(\"\\nValidation stimulus features shape:\", features_val.shape)\n",
        "print('(Validation samples × Features)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_uB8pe3n_x"
      },
      "source": [
        "### 3.4.2 | Predict fMRI responses for the validation stimuli\n",
        "\n",
        "Here, you will use the encoding model trained in `Section 3.3` to predict fMRI responses to the validation set movies, using the validation stimulus features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYbo6by_3sta"
      },
      "outputs": [],
      "source": [
        "# Predict the fMRI responses for the validation movies\n",
        "fmri_val_pred = model.predict(features_val)\n",
        "\n",
        "# Print the shape of the recorded and predicted test fMRI responses: note that\n",
        "# the two have the same shape!\n",
        "print(\"Validation fMRI responses shape:\", fmri_val.shape)\n",
        "print('(Validation samples × Parcels)')\n",
        "print(\"\\nValidation predicted fMRI responses shape:\", fmri_val_pred.shape)\n",
        "print('(Validation samples × Parcels)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-YP8XDUFUum"
      },
      "source": [
        "### 3.4.3 | Compute and plot the encoding accuracy of the predicted fMRI responses for the validation stimuli\n",
        "\n",
        "Finally, you will compute and plot the encoding accuracy of your encoding model for the validation set movies. The predicted and recorded fMRI responses are correlated (Pearson's *r*) independently at each parcel across all validation samples, resulting in one correlation score for each of the 1,000 parcels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkfTCT8iFY-h"
      },
      "outputs": [],
      "source": [
        "def compute_encoding_accuracy(fmri_val, fmri_val_pred, subject, modality):\n",
        "    \"\"\"\n",
        "    Compare the  recorded (ground truth) and predicted fMRI responses, using a\n",
        "    Pearson's correlation. The comparison is perfomed independently for each\n",
        "    fMRI parcel. The correlation results are then plotted on a glass brain.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    fmri_val : float\n",
        "        fMRI responses for the validation movies.\n",
        "    fmri_val_pred : float\n",
        "        Predicted fMRI responses for the validation movies\n",
        "    subject : int\n",
        "        Subject number used to train and validate the encoding model.\n",
        "    modality : str\n",
        "        Feature modality used to train and validate the encoding model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Correlate recorded and predicted fMRI responses ###\n",
        "    encoding_accuracy = np.zeros((fmri_val.shape[1]), dtype=np.float32)\n",
        "    for p in range(len(encoding_accuracy)):\n",
        "        encoding_accuracy[p] = pearsonr(fmri_val[:, p],\n",
        "            fmri_val_pred[:, p])[0]\n",
        "    mean_encoding_accuracy = np.round(np.mean(encoding_accuracy), 3)\n",
        "\n",
        "    ### Map the prediction accuracy onto a 3D brain atlas for plotting ###\n",
        "    atlas_file = f'sub-0{subject}_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-dseg_parcellation.nii.gz'\n",
        "    atlas_path = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
        "        'fmri', f'sub-0{subject}', 'atlas', atlas_file)\n",
        "    atlas_masker = NiftiLabelsMasker(labels_img=atlas_path)\n",
        "    atlas_masker.fit()\n",
        "    encoding_accuracy_nii = atlas_masker.inverse_transform(encoding_accuracy)\n",
        "\n",
        "    ### Plot the encoding accuracy ###\n",
        "    title = f\"Encoding accuracy, sub-0{subject}, modality-{modality}, mean accuracy: \" + str(mean_encoding_accuracy)\n",
        "    display = plotting.plot_glass_brain(\n",
        "        encoding_accuracy_nii,\n",
        "        display_mode=\"lyrz\",\n",
        "        cmap='hot_r',\n",
        "        colorbar=True,\n",
        "        plot_abs=False,\n",
        "        symmetric_cbar=False,\n",
        "        title=title\n",
        "    )\n",
        "    colorbar = display._cbar\n",
        "    colorbar.set_label(\"Pearson's $r$\", rotation=90, labelpad=12, fontsize=12)\n",
        "    plotting.show()\n",
        "\n",
        "compute_encoding_accuracy(fmri_val, fmri_val_pred, subject, modality)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaqLYCxdrM_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GavIijofLpha"
      },
      "source": [
        "# 4 | Prepare the challenge submission\n",
        "\n",
        "In this section, you will learn to prepare and format an encoding models' predictions for challenge submissions, using the pretrained challenge baseline encoding models. These encoding models were trained using stimuli (visual + audio + language) and fMRI responses for Friends seasons 1-6 plus Movie10, using a [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) regression (you can find the code used to train the challenge baseline encoding models on the challenge GitHub repository, which you can access by filling [this form](https://forms.gle/L56ytYDWgCcwWYmM9)). Since the challenge submissions for the **Model building phase** require predicted fMRI responses for all four challenge subjects to Friends season 7 (i.e., the test stimuli of the **Model building phase**), you will load and use the baseline encoding models for each of the four subjects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qjxmUouid-E"
      },
      "source": [
        "## 4.1 | Load the stimulus features for the test stimuli\n",
        "\n",
        "You will begin by loading the pre-computed and PCA-downsampled stimulus features for Friends season 7 (the test stimuli of the **Model building phase**), which can be found at `../stimulus_features/pca/friends_movie10/<modality>/features_test.npy`, where:\n",
        "- **`modality`:** String indicating the stimulus modality of the extracted features. Since the challenge baseline encoding models were trained using all three stimulus modalities together, here you will use the `modality` value `\"all\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0NJ-wpLPylm"
      },
      "outputs": [],
      "source": [
        "def load_stimulus_features_friends_s7(root_data_dir):\n",
        "    \"\"\"\n",
        "    Load the stimulus features of all modalities (visual + audio + language) for\n",
        "    Friends season 7.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_data_dir : str\n",
        "        Root data directory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features_friends_s7 : dict\n",
        "        Dictionary containing the stimulus features for Friends season 7.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    features_friends_s7 = {}\n",
        "\n",
        "    ### Load the visual features ###\n",
        "    stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
        "        'friends_movie10', 'visual', 'features_test.npy')\n",
        "    features_friends_s7['visual'] = np.load(stimuli_dir,\n",
        "        allow_pickle=True).item()\n",
        "\n",
        "    ### Load the audio features ###\n",
        "    stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
        "        'friends_movie10', 'audio', 'features_test.npy')\n",
        "    features_friends_s7['audio'] = np.load(stimuli_dir,\n",
        "        allow_pickle=True).item()\n",
        "\n",
        "    ### Load the language features ###\n",
        "    stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
        "        'friends_movie10', 'language', 'features_test.npy')\n",
        "    features_friends_s7['language'] = np.load(stimuli_dir,\n",
        "        allow_pickle=True).item()\n",
        "\n",
        "    ### Output ###\n",
        "    return features_friends_s7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eitpo2nb5jg9"
      },
      "outputs": [],
      "source": [
        "# Load the stimulus features\n",
        "features_friends_s7 = load_stimulus_features_friends_s7(root_data_dir)\n",
        "\n",
        "# Print all available movies for each stimulus modality\n",
        "for key_modality, value_modality in features_friends_s7.items():\n",
        "    print(f\"\\n{key_modality} features movie names and shape:\")\n",
        "    for key_movie, value_movie in value_modality.items():\n",
        "        print(key_movie + \" \" + str(value_movie.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GgDldVxentG"
      },
      "source": [
        "## 4.2 | Align the stimulus features with the fMRI response samples\n",
        "\n",
        "Now that you loaded the stimulus features for Friends season 7, you need to align these features with the corresponding fMRI response samples, so that the encoding models predict the correct fMRI samples for each episode of Friends season 7. Since the fMRI responses for Friends season 7 are withheld, the exact number of fMRI samples is made available at `../algonauts_2025.competitors/fmri/sub-0X/target_sample_number/sub-0X_friends-s7_fmri_samples.npy`.\n",
        "\n",
        "<font color='red'><b>IMPORTANT:</b></font> The predicted fMRI responses for each subject and episode of Friends season 7 should have the exact sample number as indicated in the `sub-0X_friends-s7_fmri_samples.npy` files, otherwise the challenge scoring program will throw an error. Thus, during model prediction for the challenge submission, **no fMRI sample should be discarded as done during model training and testing in `Section 3`**. The scoring program will take care of this for you, by removing scores for the first five and last five fMRI samples in your submitted predictions.\n",
        "\n",
        "The following function aligns the stimulus features with the fMRI response samples, and returns a two-layers nested dictionary with subject numbers (i.e., `sub-01`, `sub-02`, `sub-03`, `sub-05`) as first-layer keys, and Friends season 7 episode names (e.g., `s07e01a`) as second-layer keys. The values for the second dictionary layer are 2D arrays of stimulus features of shape `(N samples × Features)`, where *N* matches the sample dimensionality of the withheld fMRI responses provided in the `sub-0X_friends-s7_fmri_samples.npy` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQH4f3WDc3rO"
      },
      "outputs": [],
      "source": [
        "def align_features_and_fmri_samples_friends_s7(features_friends_s7,\n",
        "    root_data_dir):\n",
        "    \"\"\"\n",
        "    Align the stimulus feature with the fMRI response samples for Friends season\n",
        "    7 episodes, later used to predict the fMRI responses for challenge\n",
        "    submission.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    features_friends_s7 : dict\n",
        "        Dictionary containing the stimulus features for Friends season 7.\n",
        "    root_data_dir : str\n",
        "        Root data directory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    aligned_features_friends_s7 : dict\n",
        "        Aligned stimulus features for each subject and Friends season 7 episode.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Empty results dictionary ###\n",
        "    aligned_features_friends_s7 = {}\n",
        "\n",
        "    ### HRF delay ###\n",
        "    # fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
        "    # that reflects changes in blood oxygenation levels in response to activity\n",
        "    # in the brain. Blood flow increases to a given brain region in response to\n",
        "    # its activity. This vascular response, which follows the hemodynamic\n",
        "    # response function (HRF), takes time. Typically, the HRF peaks around 5–6\n",
        "    # seconds after a neural event: this delay reflects the time needed for\n",
        "    # blood oxygenation changes to propagate and for the fMRI signal to capture\n",
        "    # them. Therefore, this parameter introduces a delay between stimulus chunks\n",
        "    # and fMRI samples for a better correspondence between input stimuli and the\n",
        "    # brain response. For example, with a hrf_delay of 3, if the stimulus chunk\n",
        "    # of interest is 17, the corresponding fMRI sample will be 20.\n",
        "    hrf_delay = 3\n",
        "\n",
        "    ### Stimulus window ###\n",
        "    # stimulus_window indicates how many stimulus feature samples are used to\n",
        "    # model each fMRI sample, starting from the stimulus sample corresponding to\n",
        "    # the fMRI sample of interest, minus the hrf_delay, and going back in time.\n",
        "    # For example, with a stimulus_window of 5, and a hrf_delay of 3, if the\n",
        "    # fMRI sample of interest is 20, it will be modeled with stimulus samples\n",
        "    # [13, 14, 15, 16, 17]. Note that this only applies to visual and audio\n",
        "    # features, since the language features were already extracted using\n",
        "    # transcript words spanning several movie samples (thus, each fMRI sample\n",
        "    # will only be modeled using the corresponding language feature sample,\n",
        "    # minus the hrf_delay). Also note that a larger stimulus window will\n",
        "    # increase compute time, since it increases the amount of stimulus features\n",
        "    # used to train and validate the fMRI encoding models. Here you will use a\n",
        "    # value of 5, since this is how the challenge baseline encoding models were\n",
        "    # trained.\n",
        "    stimulus_window = 5\n",
        "\n",
        "    ### Loop over subjects ###\n",
        "    subjects = [1, 2, 3, 5]\n",
        "    desc = \"Aligning stimulus and fMRI features of the four subjects\"\n",
        "    for sub in tqdm(subjects, desc=desc):\n",
        "        aligned_features_friends_s7[f'sub-0{sub}'] = {}\n",
        "\n",
        "        ### Load the Friends season 7 fMRI samples ###\n",
        "        samples_dir = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
        "            'fmri', f'sub-0{sub}', 'target_sample_number',\n",
        "            f'sub-0{sub}_friends-s7_fmri_samples.npy')\n",
        "        fmri_samples = np.load(samples_dir, allow_pickle=True).item()\n",
        "\n",
        "        ### Loop over Friends season 7 episodes ###\n",
        "        for epi, samples in fmri_samples.items():\n",
        "            features_epi = []\n",
        "\n",
        "            ### Loop over fMRI samples ###\n",
        "            for s in range(samples):\n",
        "                # Empty variable containing the stimulus features of all\n",
        "                # modalities for each sample\n",
        "                f_all = np.empty(0)\n",
        "\n",
        "                ### Loop across modalities ###\n",
        "                for mod in features_friends_s7.keys():\n",
        "\n",
        "                    ### Visual and audio features ###\n",
        "                    # If visual or audio modality, model each fMRI sample using\n",
        "                    # the N stimulus feature samples up to the fMRI sample of\n",
        "                    # interest minus the hrf_delay (where N is defined by the\n",
        "                    # 'stimulus_window' variable)\n",
        "                    if mod == 'visual' or mod == 'audio':\n",
        "                        # In case there are not N stimulus feature samples up to\n",
        "                        # the fMRI sample of interest minus the hrf_delay (where\n",
        "                        # N is defined by the 'stimulus_window' variable), model\n",
        "                        # the fMRI sample using the first N stimulus feature\n",
        "                        # samples\n",
        "                        if s < (stimulus_window + hrf_delay):\n",
        "                            idx_start = 0\n",
        "                            idx_end = idx_start + stimulus_window\n",
        "                        else:\n",
        "                            idx_start = s - hrf_delay - stimulus_window + 1\n",
        "                            idx_end = idx_start + stimulus_window\n",
        "                        # In case there are less visual/audio feature samples\n",
        "                        # than fMRI samples minus the hrf_delay, use the last N\n",
        "                        # visual/audio feature samples available (where N is\n",
        "                        # defined by the 'stimulus_window' variable)\n",
        "                        if idx_end > len(features_friends_s7[mod][epi]):\n",
        "                            idx_end = len(features_friends_s7[mod][epi])\n",
        "                            idx_start = idx_end - stimulus_window\n",
        "                        f = features_friends_s7[mod][epi][idx_start:idx_end]\n",
        "                        f_all = np.append(f_all, f.flatten())\n",
        "\n",
        "                    ### Language features ###\n",
        "                    # Since language features already consist of embeddings\n",
        "                    # spanning several samples, only model each fMRI sample\n",
        "                    # using the corresponding stimulus feature sample minus the\n",
        "                    # hrf_delay\n",
        "                    elif mod == 'language':\n",
        "                        # In case there are no language features for the fMRI\n",
        "                        # sample of interest minus the hrf_delay, model the fMRI\n",
        "                        # sample using the first language feature sample\n",
        "                        if s < hrf_delay:\n",
        "                            idx = 0\n",
        "                        else:\n",
        "                            idx = s - hrf_delay\n",
        "                        # In case there are fewer language feature samples than\n",
        "                        # fMRI samples minus the hrf_delay, use the last\n",
        "                        # language feature sample available\n",
        "                        if idx >= (len(features_friends_s7[mod][epi]) - hrf_delay):\n",
        "                            f = features_friends_s7[mod][epi][-1,:]\n",
        "                        else:\n",
        "                            f = features_friends_s7[mod][epi][idx]\n",
        "                        f_all = np.append(f_all, f.flatten())\n",
        "\n",
        "                ### Append the stimulus features of all modalities for this sample ###\n",
        "                features_epi.append(f_all)\n",
        "\n",
        "            ### Add the episode stimulus features to the features dictionary ###\n",
        "            aligned_features_friends_s7[f'sub-0{sub}'][epi] = np.asarray(\n",
        "                features_epi, dtype=np.float32)\n",
        "\n",
        "    return aligned_features_friends_s7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa6lEyTkc-gW"
      },
      "outputs": [],
      "source": [
        "# Align the stimulus features with the fMRI responses for Friends season 7\n",
        "aligned_features_friends_s7 = align_features_and_fmri_samples_friends_s7(\n",
        "    features_friends_s7, root_data_dir)\n",
        "\n",
        "# As an example, print the shape of the stimulus features of one episode for\n",
        "# each subject\n",
        "for key, val in aligned_features_friends_s7.items():\n",
        "    episode_name = \"s07e01a\"\n",
        "    example_episode_shape = val[episode_name].shape\n",
        "    print(f\"Subject: {key}\")\n",
        "    print(f\"  Episode: {episode_name} - Features shape: {example_episode_shape}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hkianSNims8"
      },
      "source": [
        "## 4.3 | Load the pretrained challenge baseline encoding models\n",
        "\n",
        "Here, you will load the four pretrained challenge baseline encoding models (one for each of the four challenge subjects). Each challenge baseline encoding model consists of a ridge regression which maps stimulus features onto fMRI responses. The model was trained using [this code](https://github.com/courtois-neuromod/algonauts_2025.competitors/tree/main/code/challenge_baseline_model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzmkNeU6Nf-7"
      },
      "outputs": [],
      "source": [
        "def load_baseline_encoding_models(root_data_dir):\n",
        "    \"\"\"\n",
        "    Load the challenge baseline encoding models for all four challenge subject.\n",
        "    These models were trained to predict fMRI responses to movies using all\n",
        "    stimulus modalities (visual + audio + language)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_data_dir : str\n",
        "        Root data directory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    baseline_models : dict\n",
        "        Pretrained challenge baseline RidgeCV models.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    baseline_models = {}\n",
        "\n",
        "    ### Loop over subjects ###\n",
        "    subjects = [1, 2, 3, 5]\n",
        "    for s in subjects:\n",
        "\n",
        "        ### Load the trained encoding model weights ###\n",
        "        weights_dir = os.path.join(root_data_dir, 'trained_encoding_models',\n",
        "            'trained_encoding_model_sub-0'+str(s)+'_modality-all.npy')\n",
        "        model_weights = np.load(weights_dir, allow_pickle=True).item()\n",
        "\n",
        "        ### Initialize the Ridge regression and load the trained weights ###\n",
        "        model = Ridge()\n",
        "        model.coef_ = model_weights['coef_']\n",
        "        model.intercept_ = model_weights['intercept_']\n",
        "        model.n_features_in_ = model_weights['n_features_in_']\n",
        "\n",
        "        ### Store the pretrained encoding model into a dictionary ###\n",
        "        baseline_models['sub-0'+str(s)] = model\n",
        "        del model\n",
        "\n",
        "    ### Output ###\n",
        "    return baseline_models\n",
        "\n",
        "baseline_models = load_baseline_encoding_models(root_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rplYIa2zjMyG"
      },
      "source": [
        "## 4.4 | Predict the fMRI responses for the test stimuli, and format predictions for challenge submission\n",
        "\n",
        "Here, you will use the stimulus features and the pretrained challenge baseline encoding models to predict the fMRI responses for Friends season 7 (the test stimuli of the **Model building phase**).\n",
        "\n",
        "<font color='red'><b>For submission to [Codabench](https://www.codabench.org/competitions/4313/), the predicted fMRI responses for Friends season 7 have to be organized in the following nested dictionary structure (otherwise the scoring program will throw an error):</b></font>\n",
        "\n",
        "- **First-Layer Keys:** Each subject IDs (i.e., `\"sub-01\"`, `\"sub-02\"`, `\"sub-03\"` `\"sub-05\"`).\n",
        "- **Second-Layer Keys:** Under each subject, the nested second-layer keys consist of Friends season 7 episode names (i.e., from `\"s07e01a\"` to `\"s07e23d\"`). Each key is paired with a value that is a 2D array containing the predicted fMRI responses for a given episode. Each array is of shape `(Samples, 1,000)`, where:\n",
        "  - **Samples** is the number of fMRI samples for each episode, which should correspond to the sample number indicated in the `sub-0X_friends-s7_fmri_samples.npy` files.\n",
        "  - **1,000** represents the number of fMRI parcels covering the entire brain.\n",
        "\n",
        "Here is a visualization of how the nested dictionary structure should look like for a submission to Codabench:\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"sub-01\": {\n",
        "        \"s07e01a\": array([[...], [...], ...], dtype=float32),\n",
        "        ...\n",
        "        \"s07e23d\": array([[...], [...], ...], dtype=float32),\n",
        "    },\n",
        "    \"sub-02\": {\n",
        "        \"s07e01a\": array([[...], [...], ...], dtype=float32),\n",
        "        ...\n",
        "        \"s07e23d\": array([[...], [...], ...], dtype=float32),\n",
        "    },\n",
        "    \"sub-03\": {\n",
        "        \"s07e01a\": array([[...], [...], ...], dtype=float32),\n",
        "        ...\n",
        "        \"s07e23d\": array([[...], [...], ...], dtype=float32),\n",
        "    },\n",
        "    \"sub-05\": {\n",
        "        \"s07e01a\": array([[...], [...], ...], dtype=float32),\n",
        "        ...\n",
        "        \"s07e23d\": array([[...], [...], ...], dtype=float32),\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "<font color='red'><b>NOTE:</b></font><b> To reduce submission size and the running time of the scoring program, we recommend converting these 2D arrays to `float32` format prior to submission.</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHClKiPCLg18"
      },
      "outputs": [],
      "source": [
        "# Empty submission predictions dictionary\n",
        "submission_predictions = {}\n",
        "\n",
        "# Loop through each subject\n",
        "desc = \"Predicting fMRI responses of each subject\"\n",
        "for sub, features in tqdm(aligned_features_friends_s7.items(), desc=desc):\n",
        "\n",
        "    # Initialize the nested dictionary for each subject's predictions\n",
        "    submission_predictions[sub] = {}\n",
        "\n",
        "    # Loop through each Friends season 7 episode\n",
        "    for epi, feat_epi in features.items():\n",
        "\n",
        "        # Predict fMRI responses for the aligned features of this episode, and\n",
        "        # convert the predictions to float32\n",
        "        fmri_pred = baseline_models[sub].predict(feat_epi).astype(np.float32)\n",
        "\n",
        "        # Store formatted predictions in the nested dictionary\n",
        "        submission_predictions[sub][epi] = fmri_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl5MykN0jZTm"
      },
      "outputs": [],
      "source": [
        "# Display the structure and shapes of the predicted fMRI responses dictionary\n",
        "for subject, episodes_dict in submission_predictions.items():\n",
        "    # Print the subject and episode number for Friends season 7\n",
        "    print(f\"Subject: {subject}\")\n",
        "    print(f\"  Number of Episodes: {len(episodes_dict)}\")\n",
        "    # Print the predicted fMRI response shape for each episode\n",
        "    for episode, predictions in episodes_dict.items():\n",
        "        print(f\"    - Episode: {episode}, Predicted fMRI shape: {predictions.shape}\")\n",
        "    print(\"-\" * 40)  # Separator for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFLIxhIAjlC9"
      },
      "source": [
        "Finally, here you will save the predicted fMRI response dictionary as a `.npy` file, and zip this file for submission to [Codabench](https://www.codabench.org/competitions/4313/). <font color='red'><b>NOTE:</b></font> **The scoring program will throw an error if the submission file is not a zipped `.npy` file.**\n",
        "\n",
        "[This video](https://youtu.be/w7zpnoKtusE) goes over the Codabench competition website layout, walks you through an example submission, and explains the submission outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMM2tfOdf659"
      },
      "outputs": [],
      "source": [
        "# Select the saving directory\n",
        "save_dir = '/content/drive/MyDrive/' #@param {type:\"string\"}\n",
        "\n",
        "# Save the predicted fMRI dictionary as a .npy file\n",
        "output_file = save_dir + \"fmri_predictions_friends_s7.npy\"\n",
        "np.save(output_file, submission_predictions)\n",
        "print(f\"Formatted predictions saved to: {output_file}\")\n",
        "\n",
        "# Zip the saved file for submission\n",
        "zip_file = save_dir + \"fmri_predictions_friends_s7.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
        "    zipf.write(output_file, os.path.basename(output_file))\n",
        "print(f\"Submission file successfully zipped as: {zip_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
