{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† HBN and TUEG EEG Processing Pipeline\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\"\"\"\n",
    "HBN and TUEG EEG Data Processing with OmnEEG\n",
    "===========================================\n",
    "\n",
    "This notebook processes HBN and TUEG EEG datasets using OmnEEG to create \n",
    "standardized HDF5 files for tokenizable timeseries processing.\n",
    "\n",
    "Author: Created for EEG tokenization pipeline\n",
    "Date: 2025-01-14\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add OmnEEG to path\n",
    "sys.path.append('./OmnEEG')\n",
    "from omneeg.io import EEG\n",
    "\n",
    "print(\"üß† HBN and TUEG EEG Processing Pipeline\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory: /home/mahta/scratch/omneeg_hdf5/\n",
      "‚öôÔ∏è  Configuration loaded\n",
      "   ‚Ä¢ Sampling rate: 128 Hz\n",
      "   ‚Ä¢ Epoch duration: 1 sec\n",
      "   ‚Ä¢ Number of epochs: 100\n",
      "   ‚Ä¢ Topomap resolution: 64x64\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# =============\n",
    "\n",
    "# Data paths\n",
    "HBN_DATA_PATH = \"/home/mahta/projects/ctb-gdumas85/data/HBN/EEG/\"\n",
    "SCRATCH_OUTPUT_PATH = \"/home/mahta/scratch/omneeg_hdf5/\"  # Adjust scratch path as needed\n",
    "TUEG_DATA_PATH = \"/path/to/tueg/data/\"  # Update with actual TUEG path\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(SCRATCH_OUTPUT_PATH, exist_ok=True)\n",
    "print(f\"üìÅ Output directory: {SCRATCH_OUTPUT_PATH}\")\n",
    "\n",
    "# OmnEEG configuration\n",
    "OMNEEG_CONFIG = {\n",
    "    'data': SCRATCH_OUTPUT_PATH,\n",
    "    'sfreq': 128,           # Sampling frequency in Hz\n",
    "    'duration': 1,          # Epoch duration in seconds\n",
    "    'epochs': 100,          # Number of epochs to extract (increased for more data)\n",
    "    'resolution': 64,       # Resolution for topographic maps (increased for better detail)\n",
    "    'overwrite': True       # Overwrite existing HDF5 files\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration loaded\")\n",
    "print(f\"   ‚Ä¢ Sampling rate: {OMNEEG_CONFIG['sfreq']} Hz\")\n",
    "print(f\"   ‚Ä¢ Epoch duration: {OMNEEG_CONFIG['duration']} sec\")\n",
    "print(f\"   ‚Ä¢ Number of epochs: {OMNEEG_CONFIG['epochs']}\")\n",
    "print(f\"   ‚Ä¢ Topomap resolution: {OMNEEG_CONFIG['resolution']}x{OMNEEG_CONFIG['resolution']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created OmnEEG config: ./OmnEEG/config.yaml\n",
      "‚úÖ Created HBN cohort config: ./OmnEEG/data/hbn.yaml\n",
      "‚úÖ Created TUEG cohort config: ./OmnEEG/data/tueg.yaml\n"
     ]
    }
   ],
   "source": [
    "# Setup OmnEEG Configuration Files\n",
    "# ===============================\n",
    "\n",
    "def create_omneeg_config():\n",
    "    \"\"\"Create the main config.yaml file for OmnEEG\"\"\"\n",
    "    config_path = \"./OmnEEG/config.yaml\"\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(OMNEEG_CONFIG, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Created OmnEEG config: {config_path}\")\n",
    "\n",
    "def create_cohort_configs():\n",
    "    \"\"\"Create cohort configuration files for HBN and TUEG\"\"\"\n",
    "    \n",
    "    # HBN cohort configuration\n",
    "    hbn_config = {\n",
    "        'regexp': f'{HBN_DATA_PATH}**/*.tar.gz',  # HBN files are in tar.gz format\n",
    "        'create_epochs': True,\n",
    "        'rename_channels': False,  # We'll handle this based on actual channel names\n",
    "        'set_montage': 'standard_1020'  # Standard 10-20 montage\n",
    "    }\n",
    "    \n",
    "    # TUEG cohort configuration  \n",
    "    tueg_config = {\n",
    "        'regexp': f'{TUEG_DATA_PATH}**/*.edf',  # Assuming TUEG uses EDF format\n",
    "        'create_epochs': True,\n",
    "        'rename_channels': False,\n",
    "        'set_montage': 'standard_1020'\n",
    "    }\n",
    "    \n",
    "    # Write cohort configs\n",
    "    hbn_path = \"./OmnEEG/data/hbn.yaml\"\n",
    "    tueg_path = \"./OmnEEG/data/tueg.yaml\"\n",
    "    \n",
    "    with open(hbn_path, 'w') as f:\n",
    "        yaml.dump(hbn_config, f, default_flow_style=False)\n",
    "    \n",
    "    with open(tueg_path, 'w') as f:\n",
    "        yaml.dump(tueg_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Created HBN cohort config: {hbn_path}\")\n",
    "    print(f\"‚úÖ Created TUEG cohort config: {tueg_path}\")\n",
    "    \n",
    "    return hbn_config, tueg_config\n",
    "\n",
    "# Create configuration files\n",
    "create_omneeg_config()\n",
    "hbn_config, tueg_config = create_cohort_configs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Exploring HBN data structure...\n",
      "üìä Found 3567 HBN tar.gz files\n",
      "\n",
      "üìã Sample HBN files:\n",
      "   1. NDARHZ255RA6.tar.gz (2834.9 MB)\n",
      "   2. NDARMC325JCN.tar.gz (2952.9 MB)\n",
      "   3. NDARZN148PMN.tar.gz (1414.9 MB)\n",
      "   4. NDARZR412TBP.tar.gz (2592.3 MB)\n",
      "   5. NDARME656MTN.tar.gz (2375.5 MB)\n",
      "   ... and 3562 more files\n"
     ]
    }
   ],
   "source": [
    "# Explore HBN Data Structure\n",
    "# ==========================\n",
    "\n",
    "def explore_hbn_data():\n",
    "    \"\"\"Explore the HBN data structure to understand file organization\"\"\"\n",
    "    print(\"üîç Exploring HBN data structure...\")\n",
    "    \n",
    "    # List HBN files\n",
    "    hbn_files = glob.glob(os.path.join(HBN_DATA_PATH, \"*.tar.gz\"))\n",
    "    print(f\"üìä Found {len(hbn_files)} HBN tar.gz files\")\n",
    "    \n",
    "    if hbn_files:\n",
    "        # Show first few files\n",
    "        print(\"\\nüìã Sample HBN files:\")\n",
    "        for i, file in enumerate(hbn_files[:5]):\n",
    "            basename = os.path.basename(file)\n",
    "            size_mb = os.path.getsize(file) / (1024*1024)\n",
    "            print(f\"   {i+1}. {basename} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        if len(hbn_files) > 5:\n",
    "            print(f\"   ... and {len(hbn_files) - 5} more files\")\n",
    "    \n",
    "    return hbn_files\n",
    "\n",
    "def extract_and_process_hbn_sample(tar_file, extract_dir):\n",
    "    \"\"\"Extract a sample HBN tar.gz file to examine its contents\"\"\"\n",
    "    import tarfile\n",
    "    \n",
    "    print(f\"\\nüîß Extracting sample file: {os.path.basename(tar_file)}\")\n",
    "    \n",
    "    # Create extraction directory\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract tar.gz file\n",
    "    with tarfile.open(tar_file, 'r:gz') as tar:\n",
    "        tar.extractall(extract_dir)\n",
    "    \n",
    "    # Find EEG files in extracted directory\n",
    "    eeg_files = []\n",
    "    for ext in ['*.edf', '*.fif', '*.mff']:\n",
    "        eeg_files.extend(glob.glob(os.path.join(extract_dir, '**', ext), recursive=True))\n",
    "    \n",
    "    print(f\"üìÅ Extracted to: {extract_dir}\")\n",
    "    print(f\"üß† Found {len(eeg_files)} EEG files:\")\n",
    "    \n",
    "    for eeg_file in eeg_files[:3]:  # Show first 3\n",
    "        rel_path = os.path.relpath(eeg_file, extract_dir)\n",
    "        print(f\"   ‚Ä¢ {rel_path}\")\n",
    "    \n",
    "    return eeg_files\n",
    "\n",
    "# Explore HBN data\n",
    "hbn_files = explore_hbn_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting HBN data processing pipeline...\n",
      "   Processing 3567 HBN files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HBN files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Processing subject: NDARHZ255RA6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HBN files:   0%|          | 0/1 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_files\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Create and run the extraction pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m processed_hbn_files = \u001b[43mcreate_hbn_extraction_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mcreate_hbn_extraction_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Extract tar.gz file\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tarfile.open(tar_file, \u001b[33m'\u001b[39m\u001b[33mr:gz\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_extract_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Find EEG files\u001b[39;00m\n\u001b[32m     34\u001b[39m eeg_files = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/tarfile.py:2257\u001b[39m, in \u001b[36mTarFile.extractall\u001b[39m\u001b[34m(self, path, members, numeric_owner, filter)\u001b[39m\n\u001b[32m   2252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isdir():\n\u001b[32m   2253\u001b[39m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[32m   2254\u001b[39m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[32m   2255\u001b[39m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[32m   2256\u001b[39m         directories.append(tarinfo)\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[32m   2261\u001b[39m directories.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m a: a.name, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/tarfile.py:2320\u001b[39m, in \u001b[36mTarFile._extract_one\u001b[39m\u001b[34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[39m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28mself\u001b[39m._check(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2319\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2321\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2322\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2324\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_fatal_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/tarfile.py:2403\u001b[39m, in \u001b[36mTarFile._extract_member\u001b[39m\u001b[34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[39m\n\u001b[32m   2400\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbg(\u001b[32m1\u001b[39m, tarinfo.name)\n\u001b[32m   2402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isreg():\n\u001b[32m-> \u001b[39m\u001b[32m2403\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tarinfo.isdir():\n\u001b[32m   2405\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedir(tarinfo, targetpath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/tarfile.py:2456\u001b[39m, in \u001b[36mTarFile.makefile\u001b[39m\u001b[34m(self, tarinfo, targetpath)\u001b[39m\n\u001b[32m   2454\u001b[39m     target.truncate()\n\u001b[32m   2455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2456\u001b[39m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mReadError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/tarfile.py:252\u001b[39m, in \u001b[36mcopyfileobj\u001b[39m\u001b[34m(src, dst, length, exception, bufsize)\u001b[39m\n\u001b[32m    250\u001b[39m blocks, remainder = \u001b[38;5;28mdivmod\u001b[39m(length, bufsize)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(blocks):\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     buf = \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) < bufsize:\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[33m\"\u001b[39m\u001b[33munexpected end of data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/gzip.py:301\u001b[39m, in \u001b[36mGzipFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merrno\u001b[39;00m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno.EBADF, \u001b[33m\"\u001b[39m\u001b[33mread() on write-only GzipFile object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer.read(size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/_compression.py:68\u001b[39m, in \u001b[36mDecompressReader.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view.cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/python3.11/gzip.py:507\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[32m    505\u001b[39m buf = \u001b[38;5;28mself\u001b[39m._fp.read(io.DEFAULT_BUFFER_SIZE)\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m uncompress = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail != \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28mself\u001b[39m._fp.prepend(\u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process HBN Data with OmnEEG\n",
    "# ============================\n",
    "\n",
    "def create_hbn_extraction_pipeline():\n",
    "    \"\"\"Create a pipeline to extract and process HBN data\"\"\"\n",
    "    import tarfile\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Create temporary extraction directory\n",
    "    temp_extract_dir = os.path.join(SCRATCH_OUTPUT_PATH, \"temp_extract\")\n",
    "    os.makedirs(temp_extract_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"üöÄ Starting HBN data processing pipeline...\")\n",
    "    print(f\"   Processing {len(hbn_files)} HBN files\")\n",
    "    \n",
    "    processed_files = []\n",
    "    \n",
    "    # let's just do one participant for now\n",
    "    one_participant = hbn_files[0]\n",
    "    \n",
    "    for i, tar_file in enumerate(tqdm(hbn_files[:1], desc=\"Processing HBN files\")):  # Process first 3 for testing\n",
    "        try:\n",
    "            # Extract tar file\n",
    "            subject_id = os.path.basename(tar_file).replace('.tar.gz', '')\n",
    "            subject_extract_dir = os.path.join(temp_extract_dir, subject_id)\n",
    "            \n",
    "            print(f\"\\nüì¶ Processing subject: {subject_id}\")\n",
    "            \n",
    "            # Extract tar.gz file\n",
    "            with tarfile.open(tar_file, 'r:gz') as tar:\n",
    "                tar.extractall(subject_extract_dir)\n",
    "            \n",
    "            # Find EEG files\n",
    "            eeg_files = []\n",
    "            for ext in ['*.edf', '*.fif', '*.mff']:\n",
    "                eeg_files.extend(glob.glob(os.path.join(subject_extract_dir, '**', ext), recursive=True))\n",
    "            \n",
    "            print(f\"   Found {len(eeg_files)} EEG files for {subject_id}\")\n",
    "            \n",
    "            # Update HBN config for this specific subject\n",
    "            if eeg_files:\n",
    "                subject_config = {\n",
    "                    'regexp': os.path.join(subject_extract_dir, '**/*.edf'),  # Assuming EDF format\n",
    "                    'create_epochs': True,\n",
    "                    'rename_channels': False,\n",
    "                    'set_montage': 'standard_1020'\n",
    "                }\n",
    "                \n",
    "                # Write subject-specific config\n",
    "                subject_config_path = f\"./OmnEEG/data/hbn_{subject_id}.yaml\"\n",
    "                with open(subject_config_path, 'w') as f:\n",
    "                    yaml.dump(subject_config, f, default_flow_style=False)\n",
    "                \n",
    "                processed_files.append({\n",
    "                    'subject_id': subject_id,\n",
    "                    'config_path': subject_config_path,\n",
    "                    'eeg_files': eeg_files,\n",
    "                    'extract_dir': subject_extract_dir\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {tar_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "# Create and run the extraction pipeline\n",
    "processed_hbn_files = create_hbn_extraction_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HDF5 Files using OmnEEG\n",
    "# =================================\n",
    "\n",
    "def process_with_omneeg(processed_files):\n",
    "    \"\"\"Process extracted EEG files with OmnEEG to create HDF5 files\"\"\"\n",
    "    \n",
    "    print(\"üß† Converting EEG data to HDF5 using OmnEEG...\")\n",
    "    \n",
    "    # Change to OmnEEG directory to use relative paths\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir('./OmnEEG')\n",
    "    \n",
    "    try:\n",
    "        hdf5_outputs = []\n",
    "        \n",
    "        for file_info in processed_files:\n",
    "            subject_id = file_info['subject_id']\n",
    "            cohort_name = f\"hbn_{subject_id}\"\n",
    "            \n",
    "            print(f\"\\nüîÑ Processing subject {subject_id} with OmnEEG...\")\n",
    "            \n",
    "            try:\n",
    "                # Create EEG dataset for this subject\n",
    "                dataset = EEG(cohort=cohort_name)\n",
    "                \n",
    "                print(f\"   üìä Dataset created for {len(dataset)} files\")\n",
    "                \n",
    "                # Process all files in the dataset\n",
    "                for i in range(len(dataset)):\n",
    "                    print(f\"   Processing file {i+1}/{len(dataset)}...\")\n",
    "                    \n",
    "                    # This will automatically create HDF5 files\n",
    "                    data = dataset[i]\n",
    "                    \n",
    "                    print(f\"   ‚úÖ Generated tensor shape: {data.shape}\")\n",
    "                    # Shape should be: (epochs, height, width, timepoints)\n",
    "                \n",
    "                hdf5_outputs.append({\n",
    "                    'subject_id': subject_id,\n",
    "                    'dataset': dataset,\n",
    "                    'tensor_shape': data.shape if 'data' in locals() else None\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error processing {subject_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return hdf5_outputs\n",
    "        \n",
    "    finally:\n",
    "        # Return to original directory\n",
    "        os.chdir(original_cwd)\n",
    "\n",
    "# Process files with OmnEEG\n",
    "if processed_hbn_files:\n",
    "    hdf5_results = process_with_omneeg(processed_hbn_files)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No processed files available. Skipping OmnEEG processing.\")\n",
    "    hdf5_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results and Summary\n",
    "# =============================\n",
    "\n",
    "def visualize_hdf5_outputs(hdf5_results):\n",
    "    \"\"\"Visualize the generated HDF5 data\"\"\"\n",
    "    \n",
    "    if not hdf5_results:\n",
    "        print(\"‚ö†Ô∏è  No HDF5 results to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìà Visualizing OmnEEG outputs...\")\n",
    "    \n",
    "    # Create visualization for first subject\n",
    "    first_result = hdf5_results[0]\n",
    "    subject_id = first_result['subject_id']\n",
    "    dataset = first_result['dataset']\n",
    "    \n",
    "    # Get sample data\n",
    "    sample_data = dataset[0]  # Shape: (epochs, height, width, timepoints)\n",
    "    \n",
    "    print(f\"üß† Sample data from subject {subject_id}:\")\n",
    "    print(f\"   Shape: {sample_data.shape}\")\n",
    "    print(f\"   Data type: {sample_data.dtype}\")\n",
    "    print(f\"   Value range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\n",
    "    \n",
    "    # Plot topographic maps\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f'EEG Topographic Maps - Subject {subject_id}', fontsize=16)\n",
    "    \n",
    "    # Plot 6 different timepoints from first epoch\n",
    "    epoch_idx = 0\n",
    "    timepoints = np.linspace(0, sample_data.shape[3]-1, 6, dtype=int)\n",
    "    \n",
    "    for i, (ax, t) in enumerate(zip(axes.flat, timepoints)):\n",
    "        topomap = sample_data[epoch_idx, :, :, t]\n",
    "        \n",
    "        im = ax.imshow(topomap, cmap='RdBu_r', \n",
    "                      vmin=-np.abs(topomap).max(), \n",
    "                      vmax=np.abs(topomap).max())\n",
    "        ax.set_title(f'Time: {t}/{sample_data.shape[3]-1}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax, shrink=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_summary_report(hdf5_results):\n",
    "    \"\"\"Generate a summary report of the processing\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã PROCESSING SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"üóÇÔ∏è  Output Directory: {SCRATCH_OUTPUT_PATH}\")\n",
    "    print(f\"üìä Total subjects processed: {len(hdf5_results)}\")\n",
    "    \n",
    "    if hdf5_results:\n",
    "        print(f\"‚úÖ Successfully processed subjects:\")\n",
    "        for result in hdf5_results:\n",
    "            subject_id = result['subject_id']\n",
    "            tensor_shape = result['tensor_shape']\n",
    "            print(f\"   ‚Ä¢ {subject_id}: {tensor_shape}\")\n",
    "    \n",
    "    # List generated HDF5 files\n",
    "    hdf5_files = glob.glob(os.path.join(SCRATCH_OUTPUT_PATH, \"**/*.h5\"), recursive=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Generated HDF5 files: {len(hdf5_files)}\")\n",
    "    total_size_mb = 0\n",
    "    \n",
    "    for hdf5_file in hdf5_files[:10]:  # Show first 10\n",
    "        rel_path = os.path.relpath(hdf5_file, SCRATCH_OUTPUT_PATH)\n",
    "        size_mb = os.path.getsize(hdf5_file) / (1024*1024)\n",
    "        total_size_mb += size_mb\n",
    "        print(f\"   üìÑ {rel_path} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    if len(hdf5_files) > 10:\n",
    "        print(f\"   ... and {len(hdf5_files) - 10} more files\")\n",
    "    \n",
    "    print(f\"\\nüíΩ Total data size: {total_size_mb:.1f} MB\")\n",
    "    \n",
    "    print(\"\\nüéØ Next steps for tokenization:\")\n",
    "    print(\"   1. HDF5 files are ready for tokenizable timeseries processing\")\n",
    "    print(\"   2. Each file contains 4D tensors: (epochs, height, width, timepoints)\")\n",
    "    print(\"   3. Use these files as input for transformer-based EEG analysis\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate visualizations and summary\n",
    "if hdf5_results:\n",
    "    visualize_hdf5_outputs(hdf5_results)\n",
    "\n",
    "generate_summary_report(hdf5_results)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# TUEG Data Processing (Future Extension)\n",
    "\n",
    "The following section provides a template for processing TUEG data when available:\n",
    "\n",
    "```python\n",
    "# TUEG Processing Template\n",
    "# =======================\n",
    "\n",
    "def process_tueg_data():\n",
    "    \"\"\"Template for processing TUEG EEG data\"\"\"\n",
    "    \n",
    "    print(\"üîç Processing TUEG data...\")\n",
    "    \n",
    "    # Check if TUEG data path exists\n",
    "    if not os.path.exists(TUEG_DATA_PATH):\n",
    "        print(f\"‚ö†Ô∏è  TUEG data path not found: {TUEG_DATA_PATH}\")\n",
    "        print(\"   Please update TUEG_DATA_PATH variable with correct path\")\n",
    "        return []\n",
    "    \n",
    "    # Find TUEG EEG files\n",
    "    tueg_files = glob.glob(os.path.join(TUEG_DATA_PATH, \"**/*.edf\"), recursive=True)\n",
    "    \n",
    "    if not tueg_files:\n",
    "        print(\"   No TUEG .edf files found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"   Found {len(tueg_files)} TUEG files\")\n",
    "    \n",
    "    # Create TUEG cohort config\n",
    "    tueg_config = {\n",
    "        'regexp': f'{TUEG_DATA_PATH}**/*.edf',\n",
    "        'create_epochs': True,\n",
    "        'rename_channels': False,\n",
    "        'set_montage': 'standard_1020'\n",
    "    }\n",
    "    \n",
    "    # Process with OmnEEG\n",
    "    # ... (similar to HBN processing)\n",
    "    \n",
    "    return tueg_files\n",
    "\n",
    "# Uncomment to process TUEG data when available:\n",
    "# tueg_results = process_tueg_data()\n",
    "```\n",
    "\n",
    "**Note**: Update the `TUEG_DATA_PATH` variable when TUEG data becomes available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
