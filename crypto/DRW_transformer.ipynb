{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "a532a883232a4de6bb3a93c9542d6ac4",
            "6e6679524c3c498e9f8138993d72384e",
            "e363c6fe27ce4aeb8bacea9613786f73",
            "b34d0672c2ec43ec9c8b4592b0ed93ec",
            "e34d30b7cf4542e2b8e332cf94b18c63",
            "c52cc0e516344b1694e7fdfe87d5c98d",
            "dec1fd44a6184e1faa39b606b5bfb073",
            "41a82705d75f4f9aa7822d814285dbb0",
            "d1b2a12b402e412f8ddf62db5c111119",
            "7a8b316e2d7b4871908a75ef41c50657",
            "0f100293ac544c3e8615e189a4bb8e00",
            "68872a6012264841a99b87bc27ef3cfa",
            "c6f59ff4de904947893cdcf6630fe500",
            "3fe743c840564e3db58ca640457c6d4f",
            "0e23729589a149b8bbac5abbaf20bd73",
            "bb274a4efe3b461381af9c86ac03d4dd",
            "3828e5a9457d4b808609d568b6aad74a",
            "38feef5be5604414a6c53a7955187953",
            "9f90efa390f947df87fa3e976569a9e6",
            "71c9c27a40da4c6ab8ca86f460e84e5d",
            "8570b29c6c6a48f8872978e6c87833f0",
            "138bfa59102f46deb0686bd14fc7e5ed",
            "86aa95f469d9461bb79fdd7a5caa7105",
            "e6e366575d7f466c8f37731859fc34f2",
            "2d35bccaaaa24a71909163dd2017593d",
            "ed7260bc8da9460f86f278d4928dd679",
            "8b0776cbba2e473380bfb98c15a48c0d",
            "89e5aff0f112449992b3cf3470a4e8f8",
            "6b8b5bab9e4943ebb1c1dda0209ed11b"
          ]
        },
        "id": "gINwh2vyjMMT",
        "outputId": "5dc6320b-a8da-434f-8df1-239015414767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle credentials set.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "# kagglehub.login()\n",
        "# //read from kaggle.json\n",
        "from kagglehub.config import set_kaggle_credentials\n",
        "\n",
        "import json\n",
        "with open('kaggle.json', 'r') as f:\n",
        "    kaggle_creds = json.load(f)\n",
        "    \n",
        "# kagglehub.login(username=kaggle_creds['username'], key=kaggle_creds['key'])\n",
        "set_kaggle_credentials(username=kaggle_creds['username'], api_key=kaggle_creds['key'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from types import SimpleNamespace\n",
        "import warnings\n",
        "import time\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "N_oEzmcxjMMU",
        "outputId": "8f81242b-5a58-415b-c478-c8731d20a24a"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "# drw_crypto_market_prediction_path = kagglehub.competition_download('drw-crypto-market-prediction')\n",
        "\n",
        "# print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97En93vRjMMU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Competition: DRW Crypto Market Prediction | Date: Week 3 | Purpose: Add Back X174\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def reduce_mem_usage(dataframe, dataset):\n",
        "   print('Reducing memory usage for:', dataset)\n",
        "   initial_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
        "\n",
        "   for col in dataframe.columns:\n",
        "       col_type = dataframe[col].dtype\n",
        "       c_min = dataframe[col].min()\n",
        "       c_max = dataframe[col].max()\n",
        "\n",
        "       if str(col_type)[:3] == 'int':\n",
        "           if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "               dataframe[col] = dataframe[col].astype(np.int8)\n",
        "           elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "               dataframe[col] = dataframe[col].astype(np.int16)\n",
        "           elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "               dataframe[col] = dataframe[col].astype(np.int32)\n",
        "           elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "               dataframe[col] = dataframe[col].astype(np.int64)\n",
        "       else:\n",
        "           if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "               dataframe[col] = dataframe[col].astype(np.float16)\n",
        "           elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "               dataframe[col] = dataframe[col].astype(np.float32)\n",
        "           else:\n",
        "               dataframe[col] = dataframe[col].astype(np.float64)\n",
        "\n",
        "   final_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
        "   print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n",
        "   print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))\n",
        "   print('--- Decreased memory usage by {:.1f}%\\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n",
        "\n",
        "   return dataframe\n",
        "\n",
        "def create_time_weights(n_samples, decay_factor=0.95):\n",
        "   \"\"\"Create exponentially decaying weights based on sample position.\"\"\"\n",
        "   positions = np.arange(n_samples)\n",
        "   normalized_positions = positions / (n_samples - 1)\n",
        "   weights = decay_factor ** (1 - normalized_positions)\n",
        "   weights = weights * n_samples / weights.sum()\n",
        "   return weights\n",
        "\n",
        "# Load data\n",
        "train = pd.read_parquet(drw_crypto_market_prediction_path + '/train.parquet')\n",
        "test = pd.read_parquet(drw_crypto_market_prediction_path + '/test.parquet')\n",
        "sample_submission = pd.read_csv(drw_crypto_market_prediction_path + '/sample_submission.csv')\n",
        "\n",
        "# Add back X174 (lowest AV importance of removed features)\n",
        "selected_features = [\n",
        "   'X863', 'X856', 'X344', 'X598', 'X862', 'X385', 'X852', 'X603', 'X860',\n",
        "   'X415', 'X345', 'X137', 'X855', 'X178', 'X532', 'X168', 'X174',\n",
        "   'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'\n",
        "]\n",
        "\n",
        "train = train[selected_features + [\"label\"]]\n",
        "test = test[selected_features]\n",
        "\n",
        "train = reduce_mem_usage(train, \"train\")\n",
        "test = reduce_mem_usage(test, \"test\")\n",
        "\n",
        "print(\"Train=\", train.shape)\n",
        "print(\"Test=\", test.shape)\n",
        "\n",
        "FEATURES = [c for c in train.columns if c not in [\"label\"]]\n",
        "print(f\"There are {len(FEATURES)} FEATURES (added back X174)\")\n",
        "\n",
        "# Cross-validation\n",
        "FOLDS = 5\n",
        "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "# Create bins for stratification\n",
        "train['label_float32'] = train['label'].astype(np.float32)\n",
        "train['label_bins'] = pd.qcut(train['label_float32'], q=10, labels=False, duplicates='drop')\n",
        "train = train.drop('label_float32', axis=1)\n",
        "\n",
        "# XGBoost parameters\n",
        "xgb_params = {\n",
        "   \"tree_method\": \"hist\",\n",
        "   \"device\": \"cuda\",\n",
        "   \"colsample_bylevel\": 0.4778015829774066,\n",
        "   \"colsample_bynode\": 0.362764358742407,\n",
        "   \"colsample_bytree\": 0.7107423488010493,\n",
        "   \"gamma\": 1.7094857725240398,\n",
        "   \"learning_rate\": 0.02213323588455387,\n",
        "   \"max_depth\": 20,\n",
        "   \"max_leaves\": 12,\n",
        "   \"min_child_weight\": 16,\n",
        "   \"n_estimators\": 1667,\n",
        "   \"n_jobs\": -1,\n",
        "   \"random_state\": 42,\n",
        "   \"reg_alpha\": 39.352415706891264,\n",
        "   \"reg_lambda\": 75.44843704068275,\n",
        "   \"subsample\": 0.06566669853471274,\n",
        "   \"verbosity\": 0,\n",
        "   \"objective\": \"reg:squarederror\"\n",
        "}\n",
        "\n",
        "# Define model configurations\n",
        "model_configs = [\n",
        "   {\"name\": \"Model 1 (100% Full Data)\", \"percent\": 1.00},\n",
        "   {\"name\": \"Model 2 (90% Recent)\", \"percent\": 0.90},\n",
        "   {\"name\": \"Model 3 (80% Recent)\", \"percent\": 0.80},\n",
        "   {\"name\": \"Model 4 (70% Recent)\", \"percent\": 0.70},\n",
        "   {\"name\": \"Model 5 (60% Recent)\", \"percent\": 0.60},\n",
        "   {\"name\": \"Model 6 (50% Recent)\", \"percent\": 0.50},\n",
        "   {\"name\": \"Model 7 (40% Recent)\", \"percent\": 0.40}\n",
        "]\n",
        "\n",
        "# Initialize predictions\n",
        "n_models = len(model_configs)\n",
        "oof_preds_all = [np.zeros(len(train)) for _ in range(n_models)]\n",
        "test_preds_all = [np.zeros(len(test)) for _ in range(n_models)]\n",
        "\n",
        "# Generate sample weights for full data\n",
        "sample_weights_full = create_time_weights(len(train), decay_factor=0.95)\n",
        "print(f\"\\nFull data sample weights range: [{sample_weights_full.min():.4f}, {sample_weights_full.max():.4f}]\")\n",
        "\n",
        "# Calculate cutoffs\n",
        "cutoffs = []\n",
        "for config in model_configs:\n",
        "   if config[\"percent\"] == 1.00:\n",
        "       cutoffs.append(0)\n",
        "   else:\n",
        "       cutoff_idx = int(len(train) * (1 - config[\"percent\"]))\n",
        "       cutoffs.append(cutoff_idx)\n",
        "       print(f\"{config['name']} - Using most recent {len(train) - cutoff_idx} samples\")\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold_num, (train_idx, valid_idx) in enumerate(kf.split(train, train['label_bins'])):\n",
        "   print(\"\\n\" + \"#\" * 50)\n",
        "   print(f\"### Fold {fold_num + 1}\")\n",
        "   print(\"#\" * 50)\n",
        "\n",
        "   X_valid = train.iloc[valid_idx][FEATURES]\n",
        "   y_valid = train.iloc[valid_idx][\"label\"]\n",
        "   X_test = test[FEATURES]\n",
        "\n",
        "   # Train each model\n",
        "   for model_idx, (config, cutoff) in enumerate(zip(model_configs, cutoffs)):\n",
        "       print(f\"\\n--- {config['name']} ---\")\n",
        "\n",
        "       if config[\"percent\"] == 1.00:\n",
        "           X_train = train.iloc[train_idx][FEATURES]\n",
        "           y_train = train.iloc[train_idx][\"label\"]\n",
        "           train_weights = sample_weights_full[train_idx]\n",
        "       else:\n",
        "           train_idx_recent = train_idx[train_idx >= cutoff]\n",
        "           train_idx_recent_adjusted = train_idx_recent - cutoff\n",
        "           train_recent = train.iloc[cutoff:].reset_index(drop=True)\n",
        "\n",
        "           X_train = train_recent.iloc[train_idx_recent_adjusted][FEATURES]\n",
        "           y_train = train_recent.iloc[train_idx_recent_adjusted][\"label\"]\n",
        "\n",
        "           sample_weights_recent = create_time_weights(len(train_recent), decay_factor=0.95)\n",
        "           train_weights = sample_weights_recent[train_idx_recent_adjusted]\n",
        "\n",
        "       # Train model\n",
        "       model = xgb.XGBRegressor(**xgb_params, early_stopping_rounds=25)\n",
        "       model.fit(\n",
        "           X_train, y_train,\n",
        "           sample_weight=train_weights,\n",
        "           eval_set=[(X_valid, y_valid)],\n",
        "           verbose=200\n",
        "       )\n",
        "\n",
        "       # Make predictions\n",
        "       if config[\"percent\"] == 1.00:\n",
        "           oof_preds_all[model_idx][valid_idx] = model.predict(X_valid)\n",
        "       else:\n",
        "           valid_idx_in_range = valid_idx[valid_idx >= cutoff]\n",
        "           if len(valid_idx_in_range) > 0:\n",
        "               X_valid_subset = train.iloc[valid_idx_in_range][FEATURES]\n",
        "               oof_preds_all[model_idx][valid_idx_in_range] = model.predict(X_valid_subset)\n",
        "\n",
        "           valid_idx_out_range = valid_idx[valid_idx < cutoff]\n",
        "           if len(valid_idx_out_range) > 0:\n",
        "               oof_preds_all[model_idx][valid_idx_out_range] = oof_preds_all[0][valid_idx_out_range]\n",
        "\n",
        "       test_preds_all[model_idx] += model.predict(X_test)\n",
        "\n",
        "# Average test predictions across folds\n",
        "for i in range(n_models):\n",
        "   test_preds_all[i] /= FOLDS\n",
        "\n",
        "# Calculate individual model scores\n",
        "pearson_scores = []\n",
        "for i, config in enumerate(model_configs):\n",
        "   score = pearsonr(train[\"label\"], oof_preds_all[i])[0]\n",
        "   pearson_scores.append(score)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"INDIVIDUAL MODEL PERFORMANCE (+X174)\")\n",
        "print(\"=\" * 50)\n",
        "for config, score in zip(model_configs, pearson_scores):\n",
        "   print(f\"{config['name']} Pearson Correlation: {score:.4f}\")\n",
        "\n",
        "# Create ensemble predictions\n",
        "ensemble_oof_preds = np.mean(oof_preds_all, axis=0)\n",
        "ensemble_test_preds = np.mean(test_preds_all, axis=0)\n",
        "\n",
        "ensemble_pearson_score = pearsonr(train[\"label\"], ensemble_oof_preds)[0]\n",
        "print(f\"\\nEnsemble (Equal Weight) Pearson Correlation: {ensemble_pearson_score:.4f}\")\n",
        "\n",
        "# Weighted ensemble\n",
        "total_score = sum(pearson_scores)\n",
        "weights = [score / total_score for score in pearson_scores]\n",
        "\n",
        "weighted_ensemble_oof = np.zeros(len(train))\n",
        "weighted_ensemble_test = np.zeros(len(test))\n",
        "\n",
        "for i in range(n_models):\n",
        "   weighted_ensemble_oof += weights[i] * oof_preds_all[i]\n",
        "   weighted_ensemble_test += weights[i] * test_preds_all[i]\n",
        "\n",
        "weighted_ensemble_score = pearsonr(train[\"label\"], weighted_ensemble_oof)[0]\n",
        "print(f\"Weighted Ensemble Pearson Correlation: {weighted_ensemble_score:.4f}\")\n",
        "\n",
        "# Use the better ensemble\n",
        "if weighted_ensemble_score > ensemble_pearson_score:\n",
        "   final_test_preds = weighted_ensemble_test\n",
        "   print(\"\\nUsing weighted ensemble for final predictions\")\n",
        "else:\n",
        "   final_test_preds = ensemble_test_preds\n",
        "   print(\"\\nUsing simple average ensemble for final predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Simple transformer benchmark ready!\n"
          ]
        }
      ],
      "source": [
        "# Alternative approach: Simple transformer benchmark using PyTorch directly\n",
        "# This doesn't require the full Time-Series-Library framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class SimpleTransformerModel(nn.Module):\n",
        "    \"\"\"A simple transformer model for time series forecasting\"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=2, seq_len=96, pred_len=24):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            nhead=nhead, \n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 1)\n",
        "        \n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Linear(seq_len, pred_len)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Project input to model dimension\n",
        "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding.unsqueeze(0)  # Broadcasting\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Project to single output dimension\n",
        "        x = self.output_projection(x)  # (batch_size, seq_len, 1)\n",
        "        x = x.squeeze(-1)  # (batch_size, seq_len)\n",
        "        \n",
        "        # Project to prediction length\n",
        "        predictions = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "        \n",
        "        return predictions.unsqueeze(-1)  # (batch_size, pred_len, 1)\n",
        "\n",
        "def run_simple_transformer_benchmark(train_data, val_data, test_data, data_info):\n",
        "    \"\"\"Run a simple transformer benchmark\"\"\"\n",
        "    print(\"üöÄ Running Simple Transformer Benchmark...\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    seq_len, pred_len = 96, 24\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = CryptoDataset(train_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    val_dataset = CryptoDataset(val_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    test_dataset = CryptoDataset(test_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SimpleTransformerModel(\n",
        "        input_dim=data_info['n_features'],\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        seq_len=seq_len,\n",
        "        pred_len=pred_len\n",
        "    ).to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Training the model...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(5):  # Quick training for benchmark\n",
        "        total_loss = 0\n",
        "        for batch_idx, (batch_x, batch_y, _, _) in enumerate(train_loader):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float().to(device)  # Last pred_len steps, target feature\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Testing\n",
        "    print(\"Testing the model...\")\n",
        "    test_start = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, _, _ in test_loader:\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float()  # Target values\n",
        "            \n",
        "            outputs = model(batch_x).cpu()\n",
        "            \n",
        "            predictions.append(outputs.numpy())\n",
        "            targets.append(batch_y.numpy())\n",
        "    \n",
        "    # Concatenate predictions\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    \n",
        "    testing_time = time.time() - test_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions_flat = predictions.flatten()\n",
        "    targets_flat = targets.flatten()\n",
        "    \n",
        "    mse = np.mean((predictions_flat - targets_flat) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions_flat - targets_flat))\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(predictions_flat, targets_flat)[0, 1]\n",
        "    \n",
        "    # Model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    result = {\n",
        "        'model': 'Simple Transformer',\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,                                \n",
        "        'mape': np.mean(np.abs((targets_flat - predictions_flat) / (targets_flat + 1e-8))) * 100,\n",
        "        'correlation': correlation,\n",
        "        'training_time': training_time,\n",
        "        'testing_time': testing_time,\n",
        "        'total_params': total_params,\n",
        "        'status': 'Success'\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Simple Transformer completed!\")\n",
        "    print(f\"Correlation: {correlation:.6f}, RMSE: {rmse:.6f}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"üõ†Ô∏è Simple transformer benchmark ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save predictions\n",
        "sample_sub = pd.read_csv('/Users/mahta/Projects/Time-Series-Library/data/drw-crypto-market-prediction/sample_submission.csv')\n",
        "submission = pd.DataFrame({\n",
        "   sample_sub.columns[0]: sample_sub.iloc[:, 0],\n",
        "   'prediction': final_test_preds\n",
        "})\n",
        "submission.to_csv(\"submission_with_X174.csv\", index=False)\n",
        "print(\"\\nSubmission saved to submission_with_X174.csv!\")\n",
        "\n",
        "# Gap-first validation\n",
        "cv_scores = [np.sqrt(np.mean((train[\"label\"] - oof_preds_all[i])**2)) for i in range(n_models)]\n",
        "cv_mean = np.mean(cv_scores)\n",
        "cv_std = np.std(cv_scores)\n",
        "\n",
        "print(f\"\\n=== GAP-FIRST VALIDATION ===\")\n",
        "print(f\"Ensemble Pearson: {weighted_ensemble_score:.4f}\")\n",
        "print(f\"Model RMSE Std: {cv_std:.6f}\")\n",
        "print(f\"Features: {len(FEATURES)} (added back X174)\")\n",
        "\n",
        "if cv_std < 0.01 and weighted_ensemble_score > 0.4:\n",
        "   print(\"‚úÖ SUBMIT: High correlation and stable models\")\n",
        "else:\n",
        "   print(\"‚ö†Ô∏è  CAUTION: Review before submission\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Timeseries Time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Import error: No module named 'exp'\n",
            "Some functionality may be limited, but we'll continue with available components.\n",
            "‚úÖ Time-Series Library setup complete!\n",
            "Available models for benchmarking...\n",
            "Models to benchmark: ['Transformer', 'Informer', 'Autoformer', 'FEDformer', 'TimesNet', 'PatchTST', 'iTransformer', 'Mamba', 'TimeMixer', 'TSMixer', 'Crossformer', 'ETSformer']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Time-Series Transformer Benchmark for Crypto Prediction\n",
        "Let's prepare the data and benchmark multiple transformer models from the library\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Import the Time-Series-Library modules\n",
        "sys.path.append('/Users/mahta/Projects/Time-Series-Library')\n",
        "\n",
        "# Try to import the required modules with error handling\n",
        "try:\n",
        "    from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
        "    from utils.metrics import metric\n",
        "    from utils.tools import EarlyStopping, adjust_learning_rate\n",
        "    print(\"‚úÖ Time-Series-Library modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
        "    print(\"Some functionality may be limited, but we'll continue with available components.\")\n",
        "\n",
        "# Create a custom dataset class for our crypto data\n",
        "class CryptoDataset(Dataset):\n",
        "    def __init__(self, data, seq_len=96, label_len=48, pred_len=24):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.label_len = label_len  \n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Prepare features (everything except label)\n",
        "        self.features = [col for col in data.columns if col != 'label']\n",
        "        self.data_x = data[self.features].values\n",
        "        self.data_y = data['label'].values.reshape(-1, 1)\n",
        "        \n",
        "        # Normalize the data\n",
        "        self.data_x = (self.data_x - self.data_x.mean(axis=0)) / (self.data_x.std(axis=0) + 1e-8)\n",
        "        self.data_y = (self.data_y - self.data_y.mean()) / (self.data_y.std() + 1e-8)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "        \n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        \n",
        "        # Create dummy time marks (zeros for now)\n",
        "        seq_x_mark = np.zeros((self.seq_len, 1))\n",
        "        seq_y_mark = np.zeros((self.label_len + self.pred_len, 1))\n",
        "        \n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "# Function to create args for different models\n",
        "def create_model_args(model_name, data_info):\n",
        "    args = SimpleNamespace()\n",
        "    \n",
        "    # Basic configuration\n",
        "    args.task_name = 'long_term_forecast'\n",
        "    args.is_training = 1\n",
        "    args.model_id = f'crypto_{model_name}'\n",
        "    args.model = model_name\n",
        "    \n",
        "    # Data configuration\n",
        "    args.data = 'custom'\n",
        "    args.features = 'M'  # Multivariate\n",
        "    args.target = 'label'\n",
        "    args.freq = 't'  # minutely\n",
        "    args.checkpoints = './checkpoints/'\n",
        "    \n",
        "    # Model dimensions\n",
        "    args.seq_len = 96\n",
        "    args.label_len = 48\n",
        "    args.pred_len = 24\n",
        "    args.enc_in = data_info['n_features']\n",
        "    args.dec_in = data_info['n_features'] \n",
        "    args.c_out = 1  # predicting single target\n",
        "    \n",
        "    # Model architecture\n",
        "    args.d_model = 128\n",
        "    args.n_heads = 8\n",
        "    args.e_layers = 2\n",
        "    args.d_layers = 1\n",
        "    args.d_ff = 256\n",
        "    args.dropout = 0.1\n",
        "    args.activation = 'gelu'\n",
        "    args.embed = 'timeF'\n",
        "    args.distil = True\n",
        "    args.factor = 1\n",
        "    args.moving_avg = 25\n",
        "    \n",
        "    # Training configuration\n",
        "    args.batch_size = 64\n",
        "    args.learning_rate = 0.0001\n",
        "    args.train_epochs = 10\n",
        "    args.patience = 3\n",
        "    args.des = f'{model_name}_benchmark'\n",
        "    args.itr = 1\n",
        "    args.loss = 'MSE'\n",
        "    args.lradj = 'type1'\n",
        "    args.use_amp = False\n",
        "    \n",
        "    # Device configuration\n",
        "    args.use_gpu = torch.cuda.is_available()\n",
        "    args.gpu = 0\n",
        "    args.use_multi_gpu = False\n",
        "    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Model-specific parameters\n",
        "    if model_name == 'TimesNet':\n",
        "        args.top_k = 5\n",
        "        args.num_kernels = 6\n",
        "    elif model_name == 'PatchTST':\n",
        "        args.patch_len = 16\n",
        "        args.stride = 8\n",
        "    elif model_name == 'iTransformer':\n",
        "        args.use_norm = 1\n",
        "    elif model_name == 'Mamba':\n",
        "        args.expand = 2\n",
        "        args.d_conv = 4\n",
        "    elif model_name == 'TimeMixer':\n",
        "        args.down_sampling_layers = 3\n",
        "        args.down_sampling_method = 'avg'\n",
        "        args.down_sampling_window = 2\n",
        "    \n",
        "    return args\n",
        "\n",
        "print(\"‚úÖ Time-Series Library setup complete!\")\n",
        "print(\"Available models for benchmarking...\")\n",
        "\n",
        "# List of transformer models to benchmark\n",
        "transformer_models = [\n",
        "    'Transformer', 'Informer', 'Autoformer', 'FEDformer', \n",
        "    'TimesNet', 'PatchTST', 'iTransformer', 'Mamba',\n",
        "    'TimeMixer', 'TSMixer', 'Crossformer', 'ETSformer'\n",
        "]\n",
        "\n",
        "print(f\"Models to benchmark: {transformer_models}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Preparing crypto data for time series modeling...\n"
          ]
        }
      ],
      "source": [
        "# Prepare the data for time series forecasting\n",
        "print(\"üìä Preparing crypto data for time series modeling...\")\n",
        "\n",
        "# Load the data\n",
        "drw_crypto_market_prediction_path = 'data/'\n",
        "train_ts = pd.read_parquet(drw_crypto_market_prediction_path + '/train.parquet')\n",
        "test_ts = pd.read_parquet(drw_crypto_market_prediction_path + '/test.parquet')\n",
        "\n",
        "print(f\"Train data shape: {train_ts.shape}\")\n",
        "print(f\"Test data shape: {test_ts.shape}\")\n",
        "\n",
        "# Use the same features as our XGBoost model\n",
        "selected_features = [\n",
        "    'X863', 'X856', 'X344', 'X598', 'X862', 'X385', 'X852', 'X603', 'X860',\n",
        "    'X415', 'X345', 'X137', 'X855', 'X178', 'X532', 'X168', 'X174',\n",
        "    'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'\n",
        "]\n",
        "\n",
        "# Prepare training data\n",
        "train_ts_features = train_ts[selected_features + ['label']].copy()\n",
        "\n",
        "# Create time index (assuming sequential data)\n",
        "train_ts_features = train_ts_features.reset_index(drop=True)\n",
        "\n",
        "# Split into train/val/test for time series (70/15/15 split)\n",
        "n_samples = len(train_ts_features)\n",
        "train_size = int(0.7 * n_samples)\n",
        "val_size = int(0.15 * n_samples)\n",
        "\n",
        "ts_train = train_ts_features[:train_size].copy()\n",
        "ts_val = train_ts_features[train_size:train_size+val_size].copy()\n",
        "ts_test = train_ts_features[train_size+val_size:].copy()\n",
        "\n",
        "print(f\"Time series splits - Train: {len(ts_train)}, Val: {len(ts_val)}, Test: {len(ts_test)}\")\n",
        "\n",
        "# Data info for models\n",
        "data_info = {\n",
        "    'n_features': len(selected_features),\n",
        "    'n_samples_train': len(ts_train),\n",
        "    'n_samples_val': len(ts_val),\n",
        "    'n_samples_test': len(ts_test)\n",
        "}\n",
        "\n",
        "print(f\"Data info: {data_info}\")\n",
        "\n",
        "# Check for any missing values\n",
        "print(f\"Missing values in training data: {ts_train.isnull().sum().sum()}\")\n",
        "if ts_train.isnull().sum().sum() > 0:\n",
        "    print(\"Filling missing values with forward fill...\")\n",
        "    ts_train = ts_train.fillna(method='ffill').fillna(method='bfill')\n",
        "    ts_val = ts_val.fillna(method='ffill').fillna(method='bfill')\n",
        "    ts_test = ts_test.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# save the ready data to pickle\n",
        "with open('data/ts_train.pkl', 'wb') as f:\n",
        "    pickle.dump(ts_train, f)\n",
        "with open('data/ts_val.pkl', 'wb') as f:\n",
        "    pickle.dump(ts_val, f)\n",
        "with open('data/ts_test.pkl', 'wb') as f:\n",
        "    pickle.dump(ts_test, f)\n",
        "\n",
        "\n",
        "\n",
        "print(\"‚úÖ Data preparation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Simple transformer benchmark ready!\n"
          ]
        }
      ],
      "source": [
        "# Alternative approach: Simple transformer benchmark using PyTorch directly\n",
        "# This doesn't require the full Time-Series-Library framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class SimpleTransformerModel(nn.Module):\n",
        "    \"\"\"A simple transformer model for time series forecasting\"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=2, seq_len=96, pred_len=24):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            nhead=nhead, \n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 1)\n",
        "        \n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Linear(seq_len, pred_len)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Project input to model dimension\n",
        "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding.unsqueeze(0)  # Broadcasting\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Project to single output dimension\n",
        "        x = self.output_projection(x)  # (batch_size, seq_len, 1)\n",
        "        x = x.squeeze(-1)  # (batch_size, seq_len)\n",
        "        \n",
        "        # Project to prediction length\n",
        "        predictions = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "        \n",
        "        return predictions.unsqueeze(-1)  # (batch_size, pred_len, 1)\n",
        "\n",
        "def run_simple_transformer_benchmark(train_data, val_data, test_data, data_info):\n",
        "    \"\"\"Run a simple transformer benchmark\"\"\"\n",
        "    print(\"üöÄ Running Simple Transformer Benchmark...\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    seq_len, pred_len = 96, 24\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = CryptoDataset(train_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    val_dataset = CryptoDataset(val_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    test_dataset = CryptoDataset(test_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SimpleTransformerModel(\n",
        "        input_dim=data_info['n_features'],\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        seq_len=seq_len,\n",
        "        pred_len=pred_len\n",
        "    ).to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Training the model...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(5):  # Quick training for benchmark\n",
        "        total_loss = 0\n",
        "        for batch_idx, (batch_x, batch_y, _, _) in enumerate(train_loader):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float().to(device)  # Last pred_len steps, target feature\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Testing\n",
        "    print(\"Testing the model...\")\n",
        "    test_start = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, _, _ in test_loader:\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float()  # Target values\n",
        "            \n",
        "            outputs = model(batch_x).cpu()\n",
        "            \n",
        "            predictions.append(outputs.numpy())\n",
        "            targets.append(batch_y.numpy())\n",
        "    \n",
        "    # Concatenate predictions\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    \n",
        "    testing_time = time.time() - test_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions_flat = predictions.flatten()\n",
        "    targets_flat = targets.flatten()\n",
        "    \n",
        "    mse = np.mean((predictions_flat - targets_flat) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions_flat - targets_flat))\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(predictions_flat, targets_flat)[0, 1]\n",
        "    \n",
        "    # Model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    result = {\n",
        "        'model': 'Simple Transformer',\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mape': np.mean(np.abs((targets_flat - predictions_flat) / (targets_flat + 1e-8))) * 100,\n",
        "        'correlation': correlation,\n",
        "        'training_time': training_time,\n",
        "        'testing_time': testing_time,\n",
        "        'total_params': total_params,\n",
        "        'status': 'Success'\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Simple Transformer completed!\")\n",
        "    print(f\"Correlation: {correlation:.6f}, RMSE: {rmse:.6f}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"üõ†Ô∏è Simple transformer benchmark ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Simple transformer benchmark ready!\n"
          ]
        }
      ],
      "source": [
        "# Alternative approach: Simple transformer benchmark using PyTorch directly\n",
        "# This doesn't require the full Time-Series-Library framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class SimpleTransformerModel(nn.Module):\n",
        "    \"\"\"A simple transformer model for time series forecasting\"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=2, seq_len=96, pred_len=24):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            nhead=nhead, \n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 1)\n",
        "        \n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Linear(seq_len, pred_len)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Project input to model dimension\n",
        "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding.unsqueeze(0)  # Broadcasting\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Project to single output dimension\n",
        "        x = self.output_projection(x)  # (batch_size, seq_len, 1)\n",
        "        x = x.squeeze(-1)  # (batch_size, seq_len)\n",
        "        \n",
        "        # Project to prediction length\n",
        "        predictions = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "        \n",
        "        return predictions.unsqueeze(-1)  # (batch_size, pred_len, 1)\n",
        "\n",
        "def run_simple_transformer_benchmark(train_data, val_data, test_data, data_info):\n",
        "    \"\"\"Run a simple transformer benchmark\"\"\"\n",
        "    print(\"üöÄ Running Simple Transformer Benchmark...\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    seq_len, pred_len = 96, 24\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = CryptoDataset(train_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    val_dataset = CryptoDataset(val_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    test_dataset = CryptoDataset(test_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SimpleTransformerModel(\n",
        "        input_dim=data_info['n_features'],\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        seq_len=seq_len,\n",
        "        pred_len=pred_len\n",
        "    ).to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Training the model...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(5):  # Quick training for benchmark\n",
        "        total_loss = 0\n",
        "        for batch_idx, (batch_x, batch_y, _, _) in enumerate(train_loader):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float().to(device)  # Last pred_len steps, target feature\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Testing\n",
        "    print(\"Testing the model...\")\n",
        "    test_start = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, _, _ in test_loader:\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float()  # Target values\n",
        "            \n",
        "            outputs = model(batch_x).cpu()\n",
        "            \n",
        "            predictions.append(outputs.numpy())\n",
        "            targets.append(batch_y.numpy())\n",
        "    \n",
        "    # Concatenate predictions\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    \n",
        "    testing_time = time.time() - test_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions_flat = predictions.flatten()\n",
        "    targets_flat = targets.flatten()\n",
        "    \n",
        "    mse = np.mean((predictions_flat - targets_flat) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions_flat - targets_flat))\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(predictions_flat, targets_flat)[0, 1]\n",
        "    \n",
        "    # Model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    result = {\n",
        "        'model': 'Simple Transformer',\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mape': np.mean(np.abs((targets_flat - predictions_flat) / (targets_flat + 1e-8))) * 100,\n",
        "        'correlation': correlation,\n",
        "        'training_time': training_time,\n",
        "        'testing_time': testing_time,\n",
        "        'total_params': total_params,\n",
        "        'status': 'Success'\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Simple Transformer completed!\")\n",
        "    print(f\"Correlation: {correlation:.6f}, RMSE: {rmse:.6f}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"üõ†Ô∏è Simple transformer benchmark ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting bg_data_prep job...\n",
            "Job submitted: 46103189\n"
          ]
        }
      ],
      "source": [
        "from notebook_utils import submit_cell_as_job\n",
        "import time\n",
        "\n",
        "# Your cell code as a string\n",
        "cell_code = '''\n",
        "# Prepare the data for time series forecasting\n",
        "print(\"üìä Preparing crypto data for time series modeling...\")\n",
        "\n",
        "# Load the data\n",
        "drw_crypto_market_prediction_path = 'data/'\n",
        "train_ts = pd.read_parquet(drw_crypto_market_prediction_path + '/train.parquet')\n",
        "test_ts = pd.read_parquet(drw_crypto_market_prediction_path + '/test.parquet')\n",
        "\n",
        "print(f\"Train data shape: {train_ts.shape}\")\n",
        "print(f\"Test data shape: {test_ts.shape}\")\n",
        "\n",
        "# Use the same features as our XGBoost model\n",
        "selected_features = [\n",
        "    'X863', 'X856', 'X344', 'X598', 'X862', 'X385', 'X852', 'X603', 'X860',\n",
        "    'X415', 'X345', 'X137', 'X855', 'X178', 'X532', 'X168', 'X174',\n",
        "    'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'\n",
        "]\n",
        "\n",
        "# Prepare training data\n",
        "train_ts_features = train_ts[selected_features + ['label']].copy()\n",
        "\n",
        "# Create time index (assuming sequential data)\n",
        "train_ts_features = train_ts_features.reset_index(drop=True)\n",
        "\n",
        "# Split into train/val/test for time series (70/15/15 split)\n",
        "n_samples = len(train_ts_features)\n",
        "train_size = int(0.7 * n_samples)\n",
        "val_size = int(0.15 * n_samples)\n",
        "\n",
        "ts_train = train_ts_features[:train_size].copy()\n",
        "ts_val = train_ts_features[train_size:train_size+val_size].copy()\n",
        "ts_test = train_ts_features[train_size+val_size:].copy()\n",
        "\n",
        "print(f\"Time series splits - Train: {len(ts_train)}, Val: {len(ts_val)}, Test: {len(ts_test)}\")\n",
        "\n",
        "# Data info for models\n",
        "data_info = {\n",
        "    'n_features': len(selected_features),\n",
        "    'n_samples_train': len(ts_train),\n",
        "    'n_samples_val': len(ts_val),\n",
        "    'n_samples_test': len(ts_test)\n",
        "}\n",
        "\n",
        "print(f\"Data info: {data_info}\")\n",
        "\n",
        "# Check for any missing values\n",
        "print(f\"Missing values in training data: {ts_train.isnull().sum().sum()}\")\n",
        "if ts_train.isnull().sum().sum() > 0:\n",
        "    print(\"Filling missing values with forward fill...\")\n",
        "    ts_train = ts_train.fillna(method='ffill').fillna(method='bfill')\n",
        "    ts_val = ts_val.fillna(method='ffill').fillna(method='bfill')\n",
        "    ts_test = ts_test.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    \n",
        "print(\"‚úÖ Data preparation complete!\")\n",
        "with open('data/ts_train.pkl', 'wb') as f:\n",
        "    pickle.dump(ts_train, f)\n",
        "with open('data/ts_val.pkl', 'wb') as f:\n",
        "    pickle.dump(ts_val, f)\n",
        "with open('data/ts_test.pkl', 'wb') as f:\n",
        "    pickle.dump(ts_test, f)\n",
        "    \n",
        "'''\n",
        "results = submit_cell_as_job(\n",
        "    cell_code=cell_code,\n",
        "    job_name=\"bg_data_prep\", \n",
        "    output_file='data_prep' + str(time.time()) + '.out',\n",
        "    # account=\"def-gdumas85\"  # or any other account you have access to\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          JOBID     USER      ACCOUNT           NAME  ST  TIME_LEFT NODES CPUS TRES_PER_N MIN_MEM NODELIST (REASON) \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!squeue -u $USER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_ts':                      bid_qty  ask_qty  buy_qty  sell_qty   volume        X1  \\\n",
              " timestamp                                                                     \n",
              " 2023-03-01 00:00:00   15.283    8.425  176.405    44.984  221.389  0.121263   \n",
              " 2023-03-01 00:01:00   38.590    2.336  525.846   321.950  847.796  0.302841   \n",
              " 2023-03-01 00:02:00    0.442   60.250  159.227   136.369  295.596  0.167462   \n",
              " 2023-03-01 00:03:00    4.865   21.016  335.742   124.963  460.705  0.072944   \n",
              " 2023-03-01 00:04:00   27.158    3.451   98.411    44.407  142.818  0.173820   \n",
              " ...                      ...      ...      ...       ...      ...       ...   \n",
              " 2024-02-29 23:55:00    4.163    6.805   39.037    55.351   94.388  0.020155   \n",
              " 2024-02-29 23:56:00    2.290    4.058  110.201    67.171  177.372  0.016262   \n",
              " 2024-02-29 23:57:00    5.237    3.640   70.499    30.753  101.252  0.045407   \n",
              " 2024-02-29 23:58:00    5.731    4.901   22.365    52.195   74.560  0.124783   \n",
              " 2024-02-29 23:59:00    3.925    3.865   86.585   217.137  303.722  0.368659   \n",
              " \n",
              "                            X2        X3        X4        X5  ...      X882  \\\n",
              " timestamp                                                    ...             \n",
              " 2023-03-01 00:00:00 -0.417690  0.005399  0.125948  0.058359  ...  1.925423   \n",
              " 2023-03-01 00:01:00 -0.049576  0.356667  0.481087  0.237954  ...  1.928569   \n",
              " 2023-03-01 00:02:00 -0.291212  0.083138  0.206881  0.101727  ...  1.928047   \n",
              " 2023-03-01 00:03:00 -0.436590 -0.102483  0.017551  0.007149  ...  1.928621   \n",
              " 2023-03-01 00:04:00 -0.213489  0.096067  0.215709  0.107133  ...  1.927084   \n",
              " ...                       ...       ...       ...       ...  ...       ...   \n",
              " 2024-02-29 23:55:00  0.076565  0.228994  0.288856  0.151634  ...  3.219345   \n",
              " 2024-02-29 23:56:00  0.062527  0.214072  0.276463  0.146521  ...  3.216719   \n",
              " 2024-02-29 23:57:00  0.109834  0.263577  0.329266  0.174214  ...  3.213444   \n",
              " 2024-02-29 23:58:00  0.244168  0.408704  0.480016  0.251493  ...  3.209945   \n",
              " 2024-02-29 23:59:00  0.665382  0.867538  0.951903  0.491276  ...  3.208415   \n",
              " \n",
              "                          X883      X884      X885      X886      X887  \\\n",
              " timestamp                                                               \n",
              " 2023-03-01 00:00:00  1.847943  0.005676  0.190791  0.369691  0.377630   \n",
              " 2023-03-01 00:01:00  1.849468  0.005227  0.184660  0.363642  0.374515   \n",
              " 2023-03-01 00:02:00  1.849282  0.004796  0.178719  0.357689  0.371424   \n",
              " 2023-03-01 00:03:00  1.849608  0.004398  0.172967  0.351832  0.368358   \n",
              " 2023-03-01 00:04:00  1.848950  0.004008  0.167391  0.346066  0.365314   \n",
              " ...                       ...       ...       ...       ...       ...   \n",
              " 2024-02-29 23:55:00  3.340686  0.008679  0.224656  0.401595  0.393726   \n",
              " 2024-02-29 23:56:00  3.339353  0.007928  0.217422  0.395019  0.390476   \n",
              " 2024-02-29 23:57:00  3.337740  0.007243  0.210421  0.388549  0.387252   \n",
              " 2024-02-29 23:58:00  3.336030  0.006608  0.203642  0.382184  0.384054   \n",
              " 2024-02-29 23:59:00  3.335166  0.006072  0.197096  0.375931  0.380886   \n",
              " \n",
              "                          X888      X889      X890     label  \n",
              " timestamp                                                    \n",
              " 2023-03-01 00:00:00  0.210153  0.159183  0.530636  0.562539  \n",
              " 2023-03-01 00:01:00  0.209573  0.158963  0.530269  0.533686  \n",
              " 2023-03-01 00:02:00  0.208993  0.158744  0.529901  0.546505  \n",
              " 2023-03-01 00:03:00  0.208416  0.158524  0.529534  0.357703  \n",
              " 2023-03-01 00:04:00  0.207839  0.158304  0.529167  0.362452  \n",
              " ...                       ...       ...       ...       ...  \n",
              " 2024-02-29 23:55:00  0.212651  0.136494  0.243172  0.396289  \n",
              " 2024-02-29 23:56:00  0.212063  0.136305  0.243004  0.328993  \n",
              " 2024-02-29 23:57:00  0.211477  0.136117  0.242836  0.189909  \n",
              " 2024-02-29 23:58:00  0.210892  0.135928  0.242668  0.410831  \n",
              " 2024-02-29 23:59:00  0.210310  0.135741  0.242501  0.731542  \n",
              " \n",
              " [525887 rows x 896 columns],\n",
              " 'ts_train':             X863      X856      X344      X598      X862      X385      X852  \\\n",
              " 0       0.218570 -0.216525 -0.362607  0.075641 -1.027483 -0.589209  0.878094   \n",
              " 1       0.088014 -0.180112 -0.376922  0.067653 -1.024055 -0.588391  0.891413   \n",
              " 2      -0.147363 -0.265966 -0.368205  0.067288 -1.024056 -0.587575  0.859856   \n",
              " 3      -0.094590 -0.322244 -0.356326  0.069881 -1.024058 -0.647419  0.839141   \n",
              " 4       0.162221 -0.369625 -0.347715  0.072288 -1.024060 -0.646521  0.821680   \n",
              " ...          ...       ...       ...       ...       ...       ...       ...   \n",
              " 368115 -0.119978  0.250024  0.285751 -0.111418  0.026020 -0.077169  1.236050   \n",
              " 368116  0.164144  0.250024  0.285901 -0.110962  0.026021 -0.077062  1.236050   \n",
              " 368117 -0.044288  0.250024  0.285505 -0.107886  0.026021 -0.076955  1.236050   \n",
              " 368118 -0.288989  0.250024  0.285108 -0.107806  0.026022 -0.076848  1.236050   \n",
              " 368119 -0.079457  0.250024  0.286847 -0.107838  0.026022 -0.076742  1.236050   \n",
              " \n",
              "             X603      X860      X415  ...      X178      X532      X168  \\\n",
              " 0       0.839766  0.541286 -1.035101  ... -0.033564  0.594644 -0.258087   \n",
              " 1       0.839766  0.450331 -1.029366  ... -0.033518  0.578271 -0.256658   \n",
              " 2       0.833221  0.420681 -1.023663  ... -0.033471  0.577898 -0.255236   \n",
              " 3       0.826309  0.386584 -1.214450  ... -0.033425  0.577524 -0.281572   \n",
              " 4       0.800184  0.389969 -1.207722  ... -0.033378  0.577151 -0.280012   \n",
              " ...          ...       ...       ...  ...       ...       ...       ...   \n",
              " 368115 -1.042854 -0.131165 -0.363868  ... -0.308104 -0.036089 -0.206310   \n",
              " 368116 -1.026016 -0.037838 -0.361852  ... -0.307676 -0.035871 -0.205167   \n",
              " 368117 -1.021411 -0.141992 -0.359847  ... -0.307249 -0.035653 -0.207651   \n",
              " 368118 -0.986653 -0.155198 -0.357854  ... -0.306823 -0.035436 -0.206501   \n",
              " 368119 -0.997004 -0.077505 -0.355871  ... -0.304623 -0.035218 -0.205357   \n",
              " \n",
              "             X174  bid_qty  ask_qty  buy_qty  sell_qty   volume     label  \n",
              " 0      -0.167949   15.283    8.425  176.405    44.984  221.389  0.562539  \n",
              " 1      -0.167483   38.590    2.336  525.846   321.950  847.796  0.533686  \n",
              " 2      -0.167019    0.442   60.250  159.227   136.369  295.596  0.546505  \n",
              " 3      -0.180450    4.865   21.016  335.742   124.963  460.705  0.357703  \n",
              " 4      -0.179949   27.158    3.451   98.411    44.407  142.818  0.362452  \n",
              " ...          ...      ...      ...      ...       ...      ...       ...  \n",
              " 368115 -0.242198   15.493   12.788    6.651    35.962   42.613  0.835549  \n",
              " 368116 -0.241526   13.247    0.497   13.049    12.001   25.050  0.838347  \n",
              " 368117 -0.242669    8.662    3.416   28.396     5.613   34.009  0.750225  \n",
              " 368118 -0.241996    7.386   12.186   13.698    26.998   40.696  0.772381  \n",
              " 368119 -0.241325   20.625    0.101  125.332    21.859  147.191  0.724751  \n",
              " \n",
              " [368120 rows x 23 columns],\n",
              " 'ts_val':             X863      X856      X344      X598      X862      X385     X852  \\\n",
              " 368120 -0.144238  0.250024  0.286449 -0.107088  0.026023 -0.076635  1.23605   \n",
              " 368121 -0.304026  0.250024  0.284236 -0.098562  0.036097 -0.076529  1.23605   \n",
              " 368122 -0.124652  0.250024  0.284432 -0.099541  0.036098 -0.076422  1.23605   \n",
              " 368123 -0.156767  0.250024  0.284602 -0.096138  0.036099 -0.076316  1.23605   \n",
              " 368124 -0.448326  0.250024  0.284376 -0.100130  0.036100 -0.076210  1.23605   \n",
              " ...          ...       ...       ...       ...       ...       ...      ...   \n",
              " 446998  0.239563  0.121200  1.092600  0.132574 -0.325862 -0.036101  1.64659   \n",
              " 446999  0.156666  0.121200  1.093104  0.131918 -0.317943 -0.036051  1.64659   \n",
              " 447000  0.026276  0.121200  1.092902  0.130996 -0.317951 -0.036001  1.64659   \n",
              " 447001  0.137536  0.121200  1.095365  0.120790 -0.317958 -0.028396  1.64659   \n",
              " 447002  0.185911  0.121200  1.097338  0.120607 -0.317965 -0.028356  1.64659   \n",
              " \n",
              "             X603      X860      X415  ...      X178      X532      X168  \\\n",
              " 368120 -0.988131 -0.074145 -0.353900  ... -0.301602 -0.035000 -0.263404   \n",
              " 368121 -0.979306 -0.444361 -0.706510  ... -0.301183 -0.073111 -0.261944   \n",
              " 368122 -0.990498 -0.331550 -0.702595  ... -0.300765 -0.072822 -0.260493   \n",
              " 368123 -0.974073 -0.315495 -0.698703  ... -0.300348 -0.072533 -0.259050   \n",
              " 368124 -0.946727 -0.321266 -0.694832  ... -0.299931 -0.072244 -0.257615   \n",
              " ...          ...       ...       ...  ...       ...       ...       ...   \n",
              " 446998 -0.299508  0.590443  0.726204  ...  0.135672 -0.722511  0.312447   \n",
              " 446999 -0.310088  0.574772  0.722180  ...  0.135484 -0.750956  0.310716   \n",
              " 447000 -0.347807  0.713578  0.725699  ...  0.135296 -0.750600  0.308995   \n",
              " 447001 -0.312343  0.729086  0.721678  ...  0.082590 -0.750244  0.307283   \n",
              " 447002 -0.307221  0.692144  0.717680  ...  0.082475 -0.749889  0.107192   \n",
              " \n",
              "             X174  bid_qty  ask_qty  buy_qty  sell_qty   volume     label  \n",
              " 368120 -0.270289    6.444    2.201   66.811    34.375  101.186  0.664206  \n",
              " 368121 -0.269539    5.959   21.174  304.254    97.663  401.917  0.397444  \n",
              " 368122 -0.268791   13.233   26.320   18.414    47.223   65.637  0.480276  \n",
              " 368123 -0.268046    6.813    6.409   39.013    24.013   63.026  0.470193  \n",
              " 368124 -0.267302    5.888   22.793    6.794    50.068   56.862  0.528308  \n",
              " ...          ...      ...      ...      ...       ...      ...       ...  \n",
              " 446998  0.617948    4.713    1.517   13.286    32.652   45.938 -0.225838  \n",
              " 446999  0.616233    6.429    3.052   20.698    19.369   40.067 -0.245997  \n",
              " 447000  0.614524    4.685    6.364   33.656   154.405  188.061 -0.133412  \n",
              " 447001  0.612819    1.242    0.449  134.019   345.716  479.735  0.111464  \n",
              " 447002  0.511787    7.255    3.368   94.691    48.792  143.483  0.019021  \n",
              " \n",
              " [78883 rows x 23 columns],\n",
              " 'ts_test':             X863      X856      X344      X598      X862      X385      X852  \\\n",
              " 447003  0.140448  0.121200  1.095815  0.120336 -0.317972 -0.028317  1.646590   \n",
              " 447004  0.180894  0.121200  1.096738  0.119515 -0.323265 -0.028278  1.646590   \n",
              " 447005  0.330265  0.121200  1.097845  0.119283 -0.323272 -0.026740  1.646590   \n",
              " 447006  0.518487  0.121200  1.099394  0.114814 -0.323279 -0.013111  1.646590   \n",
              " 447007  0.633032  0.121200  1.101907  0.115496 -0.323287 -0.013093  1.646590   \n",
              " ...          ...       ...       ...       ...       ...       ...       ...   \n",
              " 525882  0.636555  0.035728  1.891481 -0.241033 -0.218730 -1.422923  1.527801   \n",
              " 525883  0.649406  0.052891  1.888856 -0.238704 -0.206114 -1.421335  1.539154   \n",
              " 525884  0.535593  0.024071  1.886235 -0.236690 -0.206118 -1.419362  1.520011   \n",
              " 525885  0.647059  0.035102  1.883617 -0.236390 -0.206122 -1.417392  1.527291   \n",
              " 525886  0.331538 -0.052765  1.881002 -0.238326 -0.206125 -1.399878  1.469018   \n",
              " \n",
              "             X603      X860      X415  ...      X178      X532      X168  \\\n",
              " 447003 -0.312034  0.652159  0.713704  ...  0.082361 -0.749533  0.106598   \n",
              " 447004 -0.328145  0.659128  0.709750  ...  0.082247 -0.729916  0.136139   \n",
              " 447005 -0.367696  0.755369  0.712401  ...  0.073776 -0.729498  0.135384   \n",
              " 447006 -0.428050  0.732302  0.708454  ...  0.035640 -0.729080  0.140304   \n",
              " 447007 -0.422342  0.738773  0.701441  ...  0.035591 -0.728662  0.139527   \n",
              " ...          ...       ...       ...  ...       ...       ...       ...   \n",
              " 525882 -0.477566 -0.040562 -1.035280  ... -0.230051  0.381244  0.232956   \n",
              " 525883 -0.479550 -0.027268 -1.029544  ... -0.229732  0.313218  0.231665   \n",
              " 525884 -0.475724 -0.065439 -1.034079  ... -0.229413  0.312391  0.230382   \n",
              " 525885 -0.447471 -0.035077 -1.027346  ... -0.229095  0.311564  0.231605   \n",
              " 525886 -0.406783  0.202887 -1.021654  ... -0.228777  0.310738  0.230322   \n",
              " \n",
              "             X174  bid_qty  ask_qty  buy_qty  sell_qty   volume     label  \n",
              " 447003  0.510368    6.038    2.942   37.044    25.672   62.716 -0.036555  \n",
              " 447004  0.524038    3.992    3.754   24.374    62.838   87.212  0.045812  \n",
              " 447005  0.522585    4.681   10.342   58.828    98.697  157.525  0.153316  \n",
              " 447006  0.523974  104.884    1.632   81.531   421.161  502.692  0.381330  \n",
              " 447007  0.522521    3.611    2.903  103.417    44.324  147.741  0.258816  \n",
              " ...          ...      ...      ...      ...       ...      ...       ...  \n",
              " 525882  0.236631    4.163    6.805   39.037    55.351   94.388  0.396289  \n",
              " 525883  0.235975    2.290    4.058  110.201    67.171  177.372  0.328993  \n",
              " 525884  0.235320    5.237    3.640   70.499    30.753  101.252  0.189909  \n",
              " 525885  0.235919    5.731    4.901   22.365    52.195   74.560  0.410831  \n",
              " 525886  0.235264    3.925    3.865   86.585   217.137  303.722  0.731542  \n",
              " \n",
              " [78884 rows x 23 columns],\n",
              " 'data_info': {'n_features': 22,\n",
              "  'n_samples_train': 368120,\n",
              "  'n_samples_val': 78883,\n",
              "  'n_samples_test': 78884},\n",
              " 'selected_features': ['X863',\n",
              "  'X856',\n",
              "  'X344',\n",
              "  'X598',\n",
              "  'X862',\n",
              "  'X385',\n",
              "  'X852',\n",
              "  'X603',\n",
              "  'X860',\n",
              "  'X415',\n",
              "  'X345',\n",
              "  'X137',\n",
              "  'X855',\n",
              "  'X178',\n",
              "  'X532',\n",
              "  'X168',\n",
              "  'X174',\n",
              "  'bid_qty',\n",
              "  'ask_qty',\n",
              "  'buy_qty',\n",
              "  'sell_qty',\n",
              "  'volume']}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Simple transformer benchmark ready!\n"
          ]
        }
      ],
      "source": [
        "# Alternative approach: Simple transformer benchmark using PyTorch directly\n",
        "# This doesn't require the full Time-Series-Library framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class SimpleTransformerModel(nn.Module):\n",
        "    \"\"\"A simple transformer model for time series forecasting\"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=2, seq_len=96, pred_len=24):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            nhead=nhead, \n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 1)\n",
        "        \n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Linear(seq_len, pred_len)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Project input to model dimension\n",
        "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding.unsqueeze(0)  # Broadcasting\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Project to single output dimension\n",
        "        x = self.output_projection(x)  # (batch_size, seq_len, 1)\n",
        "        x = x.squeeze(-1)  # (batch_size, seq_len)\n",
        "        \n",
        "        # Project to prediction length\n",
        "        predictions = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "        \n",
        "        return predictions.unsqueeze(-1)  # (batch_size, pred_len, 1)\n",
        "\n",
        "def run_simple_transformer_benchmark(train_data, val_data, test_data, data_info):\n",
        "    \"\"\"Run a simple transformer benchmark\"\"\"\n",
        "    print(\"üöÄ Running Simple Transformer Benchmark...\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    seq_len, pred_len = 96, 24\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = CryptoDataset(train_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    val_dataset = CryptoDataset(val_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    test_dataset = CryptoDataset(test_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SimpleTransformerModel(\n",
        "        input_dim=data_info['n_features'],\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        seq_len=seq_len,\n",
        "        pred_len=pred_len\n",
        "    ).to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Training the model...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(5):  # Quick training for benchmark\n",
        "        total_loss = 0\n",
        "        for batch_idx, (batch_x, batch_y, _, _) in enumerate(train_loader):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float().to(device)  # Last pred_len steps, target feature\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Testing\n",
        "    print(\"Testing the model...\")\n",
        "    test_start = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, _, _ in test_loader:\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float()  # Target values\n",
        "            \n",
        "            outputs = model(batch_x).cpu()\n",
        "            \n",
        "            predictions.append(outputs.numpy())\n",
        "            targets.append(batch_y.numpy())\n",
        "    \n",
        "    # Concatenate predictions\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    \n",
        "    testing_time = time.time() - test_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions_flat = predictions.flatten()\n",
        "    targets_flat = targets.flatten()\n",
        "    \n",
        "    mse = np.mean((predictions_flat - targets_flat) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions_flat - targets_flat))\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(predictions_flat, targets_flat)[0, 1]\n",
        "    \n",
        "    # Model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    result = {\n",
        "        'model': 'Simple Transformer',\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mape': np.mean(np.abs((targets_flat - predictions_flat) / (targets_flat + 1e-8))) * 100,\n",
        "        'correlation': correlation,\n",
        "        'training_time': training_time,\n",
        "        'testing_time': testing_time,\n",
        "        'total_params': total_params,\n",
        "        'status': 'Success'\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Simple Transformer completed!\")\n",
        "    print(f\"Correlation: {correlation:.6f}, RMSE: {rmse:.6f}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"üõ†Ô∏è Simple transformer benchmark ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'test_ts'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m variables = pd.read_pickle(\u001b[33m'\u001b[39m\u001b[33mresults/saved_variables.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m train_ts = variables[\u001b[33m'\u001b[39m\u001b[33mtrain_ts\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m test_ts = \u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest_ts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print the first 5 rows of the train_ts\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_ts.head())\n",
            "\u001b[31mKeyError\u001b[39m: 'test_ts'"
          ]
        }
      ],
      "source": [
        "# open pickle file \n",
        "import pandas as pd\n",
        "variables = pd.read_pickle('results/saved_variables.pkl')\n",
        "train_ts = variables['train_ts']\n",
        "test_ts = variables['test_ts']\n",
        "\n",
        "\n",
        "# print the first 5 rows of the train_ts\n",
        "print(train_ts.head())\n",
        "\n",
        "# print the shape of the train_ts\n",
        "print(train_ts.shape)\n",
        "\n",
        "print(test_ts.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mahta/Time-Series-Library/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot bid_qty  ask_qty  buy_qty  sell_qty   volume    in train_ts\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# train_ts = pd.read_pickle('results/saved_vari√∑ables.pkl')\n",
        "\n",
        "# plot bid_qty  ask_qty  buy_qty  sell_qty   volume    in train_ts in one legended plot\n",
        "plt.plot(train_ts['bid_qty'], label='bid_qty')\n",
        "plt.plot(train_ts['ask_qty'], label='ask_qty')\n",
        "plt.plot(train_ts['buy_qty'], label='buy_qty')\n",
        "plt.plot(train_ts['sell_qty'], label='sell_qty')\n",
        "# plt.plot(train_ts['volume'], label='volume')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Simple transformer benchmark ready!\n"
          ]
        }
      ],
      "source": [
        "# Alternative approach: Simple transformer benchmark using PyTorch directly\n",
        "# This doesn't require the full Time-Series-Library framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class SimpleTransformerModel(nn.Module):\n",
        "    \"\"\"A simple transformer model for time series forecasting\"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=2, seq_len=96, pred_len=24):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            nhead=nhead, \n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 1)\n",
        "        \n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Linear(seq_len, pred_len)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Project input to model dimension\n",
        "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding.unsqueeze(0)  # Broadcasting\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        # Project to single output dimension\n",
        "        x = self.output_projection(x)  # (batch_size, seq_len, 1)\n",
        "        x = x.squeeze(-1)  # (batch_size, seq_len)\n",
        "        \n",
        "        # Project to prediction length\n",
        "        predictions = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "        \n",
        "        return predictions.unsqueeze(-1)  # (batch_size, pred_len, 1)\n",
        "\n",
        "def run_simple_transformer_benchmark(train_data, val_data, test_data, data_info):\n",
        "    \"\"\"Run a simple transformer benchmark\"\"\"\n",
        "    print(\"üöÄ Running Simple Transformer Benchmark...\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    seq_len, pred_len = 96, 24\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = CryptoDataset(train_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    val_dataset = CryptoDataset(val_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    test_dataset = CryptoDataset(test_data, seq_len=seq_len, pred_len=pred_len)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SimpleTransformerModel(\n",
        "        input_dim=data_info['n_features'],\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        seq_len=seq_len,\n",
        "        pred_len=pred_len\n",
        "    ).to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Training the model...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(5):  # Quick training for benchmark\n",
        "        total_loss = 0\n",
        "        for batch_idx, (batch_x, batch_y, _, _) in enumerate(train_loader):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float().to(device)  # Last pred_len steps, target feature\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Testing\n",
        "    print(\"Testing the model...\")\n",
        "    test_start = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, _, _ in test_loader:\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[:, -pred_len:, -1:].float()  # Target values\n",
        "            \n",
        "            outputs = model(batch_x).cpu()\n",
        "            \n",
        "            predictions.append(outputs.numpy())\n",
        "            targets.append(batch_y.numpy())\n",
        "    \n",
        "    # Concatenate predictions\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    \n",
        "    testing_time = time.time() - test_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    predictions_flat = predictions.flatten()\n",
        "    targets_flat = targets.flatten()\n",
        "    \n",
        "    mse = np.mean((predictions_flat - targets_flat) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions_flat - targets_flat))\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(predictions_flat, targets_flat)[0, 1]\n",
        "    \n",
        "    # Model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    result = {\n",
        "        'model': 'Simple Transformer',\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mape': np.mean(np.abs((targets_flat - predictions_flat) / (targets_flat + 1e-8))) * 100,\n",
        "        'correlation': correlation,\n",
        "        'training_time': training_time,\n",
        "        'testing_time': testing_time,\n",
        "        'total_params': total_params,\n",
        "        'status': 'Success'\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Simple Transformer completed!\")\n",
        "    print(f\"Correlation: {correlation:.6f}, RMSE: {rmse:.6f}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"üõ†Ô∏è Simple transformer benchmark ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting bg_data_prep job... None\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting bg_data_prep job...\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          JOBID     USER      ACCOUNT           NAME  ST  TIME_LEFT NODES CPUS TRES_PER_N MIN_MEM NODELIST (REASON) \n",
            "       46032474    mahta def-gdumas85  crypto_python  PD    8:00:00     1    8 gres:gpu:1     64G  (ReqNodeNotAvail, UnavailableNodes:ng[10101-10104,10201-10204,10301-10304,10401-10404,10501-10504,10601-10610,10701-10712,10801-10808,10901-10906,11001-11006,11101-11106,20101-20104,20201-20204,20301-20303,20403,30601-30605,30701-30712,31001-31006,31101-31104,31201-31205,31301-31305,31401-31402]) \n"
          ]
        }
      ],
      "source": [
        "!squeue -u $USER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è CustomExp class not available - skipping transformer benchmark\n",
            "üõ†Ô∏è  Custom experiment framework ready!\n",
            "Starting transformer benchmark...\n"
          ]
        }
      ],
      "source": [
        "# Custom experiment class to work with our data\n",
        "# Only define if we successfully imported the base class\n",
        "if 'Exp_Long_Term_Forecast' in globals():\n",
        "    class CustomExp(Exp_Long_Term_Forecast):\n",
        "        def __init__(self, args, train_data, val_data, test_data):\n",
        "            super().__init__(args)\n",
        "            self.train_data = train_data\n",
        "            self.val_data = val_data\n",
        "            self.test_data = test_data\n",
        "            \n",
        "        def _get_data(self, flag):\n",
        "            if flag == 'train':\n",
        "                dataset = CryptoDataset(self.train_data, self.args.seq_len, self.args.label_len, self.args.pred_len)\n",
        "            elif flag == 'val':\n",
        "                dataset = CryptoDataset(self.val_data, self.args.seq_len, self.args.label_len, self.args.pred_len)\n",
        "            else:  # test\n",
        "                dataset = CryptoDataset(self.test_data, self.args.seq_len, self.args.label_len, self.args.pred_len)\n",
        "                \n",
        "            data_loader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=self.args.batch_size,\n",
        "                shuffle=(flag == 'train'),\n",
        "                num_workers=0,\n",
        "                drop_last=True\n",
        "            )\n",
        "            \n",
        "            return dataset, data_loader\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CustomExp class not available - skipping transformer benchmark\")\n",
        "    CustomExp = None\n",
        "\n",
        "def run_single_model_benchmark(model_name, train_data, val_data, test_data, data_info):\n",
        "    \"\"\"Run benchmark for a single model\"\"\"\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\"üöÄ Benchmarking {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Check if required classes are available\n",
        "    if CustomExp is None:\n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'mae': np.nan,\n",
        "            'mse': np.nan,\n",
        "            'rmse': np.nan,\n",
        "            'mape': np.nan,\n",
        "            'correlation': np.nan,\n",
        "            'training_time': np.nan,\n",
        "            'testing_time': np.nan,\n",
        "            'total_params': np.nan,\n",
        "            'status': 'Failed: Time-Series-Library dependencies not available'\n",
        "        }\n",
        "    \n",
        "    try:\n",
        "        # Create model arguments\n",
        "        args = create_model_args(model_name, data_info)\n",
        "        \n",
        "        # Create experiment\n",
        "        exp = CustomExp(args, train_data, val_data, test_data)\n",
        "        \n",
        "        # Create setting name\n",
        "        setting = f'{args.task_name}_{args.model_id}_{args.model}_{args.data}'\n",
        "        \n",
        "        print(f\"‚öôÔ∏è  Training {model_name}...\")\n",
        "        \n",
        "        # Train the model\n",
        "        start_time = time.time()\n",
        "        model = exp.train(setting)\n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        print(f\"‚è±Ô∏è  Training completed in {training_time:.2f} seconds\")\n",
        "        \n",
        "        # Test the model\n",
        "        print(f\"üß™ Testing {model_name}...\")\n",
        "        test_start = time.time()\n",
        "        \n",
        "        # Get test predictions\n",
        "        test_data_loader = exp._get_data('test')[1]\n",
        "        preds = []\n",
        "        trues = []\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_data_loader):\n",
        "                batch_x = batch_x.float().to(exp.device)\n",
        "                batch_y = batch_y.float().to(exp.device)\n",
        "                batch_x_mark = batch_x_mark.float().to(exp.device)\n",
        "                batch_y_mark = batch_y_mark.float().to(exp.device)\n",
        "\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
        "                dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(exp.device)\n",
        "                \n",
        "                # prediction\n",
        "                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "                \n",
        "                # Extract predictions for the target sequence\n",
        "                pred = outputs[:, -args.pred_len:, -1:].detach().cpu().numpy()  # Last feature is target\n",
        "                true = batch_y[:, -args.pred_len:, -1:].detach().cpu().numpy()\n",
        "                \n",
        "                preds.append(pred)\n",
        "                trues.append(true)\n",
        "        \n",
        "        # Concatenate all predictions\n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        trues = np.concatenate(trues, axis=0)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
        "        \n",
        "        # Calculate correlation (similar to Pearson correlation for our task)\n",
        "        preds_flat = preds.flatten()\n",
        "        trues_flat = trues.flatten()\n",
        "        correlation = np.corrcoef(preds_flat, trues_flat)[0, 1]\n",
        "        \n",
        "        testing_time = time.time() - test_start\n",
        "        \n",
        "        print(f\"‚úÖ {model_name} completed!\")\n",
        "        \n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'mae': mae,\n",
        "            'mse': mse, \n",
        "            'rmse': rmse,\n",
        "            'mape': mape,\n",
        "            'correlation': correlation,\n",
        "            'training_time': training_time,\n",
        "            'testing_time': testing_time,\n",
        "            'total_params': sum(p.numel() for p in model.parameters()),\n",
        "            'status': 'Success'\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error with {model_name}: {str(e)}\")\n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'mae': np.nan,\n",
        "            'mse': np.nan,\n",
        "            'rmse': np.nan, \n",
        "            'mape': np.nan,\n",
        "            'correlation': np.nan,\n",
        "            'training_time': np.nan,\n",
        "            'testing_time': np.nan,\n",
        "            'total_params': np.nan,\n",
        "            'status': f'Failed: {str(e)[:100]}'\n",
        "        }\n",
        "\n",
        "print(\"üõ†Ô∏è  Custom experiment framework ready!\")\n",
        "print(\"Starting transformer benchmark...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'completed', 'job_name': 'bg_data_prep'}\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'head'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# show the first 5 rows of the results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m())\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# show the last 5 rows of the results\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(results.tail())\n",
            "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'head'"
          ]
        }
      ],
      "source": [
        "with open('results.pkl', 'rb') as f:\n",
        "    results = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Starting Transformer Benchmark for Crypto Prediction...\n",
            "============================================================\n",
            "üîß Using Simple PyTorch Transformer approach...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'ts_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîß Using Simple PyTorch Transformer approach...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Run the simple transformer benchmark\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m result = run_simple_transformer_benchmark(\u001b[43mts_train\u001b[49m, ts_val, ts_test, data_info)\n\u001b[32m     52\u001b[39m benchmark_results.append(result)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Also add a simple baseline for comparison\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'ts_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Determine which benchmark approach to use\n",
        "benchmark_results = []\n",
        "\n",
        "print(\"üéØ Starting Transformer Benchmark for Crypto Prediction...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try the Time-Series-Library approach first\n",
        "if CustomExp is not None:\n",
        "    print(\"üìö Using Time-Series-Library framework...\")\n",
        "    \n",
        "    # Run the benchmark on a subset of models first (to avoid long runtime)\n",
        "    priority_models = [\n",
        "        'Transformer',      # Classic transformer\n",
        "        'Autoformer',       # Popular for time series\n",
        "        'Informer',         # Popular for long sequences  \n",
        "        'DLinear',          # Simple but effective\n",
        "        'PatchTST',         # State-of-the-art for many tasks\n",
        "        'iTransformer',     # Recent improvement\n",
        "        'TimesNet'          # Good general purpose model\n",
        "    ]\n",
        "\n",
        "    print(f\"Models to test: {priority_models}\")\n",
        "\n",
        "    # Run benchmark for each model\n",
        "    for i, model_name in enumerate(priority_models):\n",
        "        print(f\"\\\\n[{i+1}/{len(priority_models)}] Processing {model_name}...\")\n",
        "        \n",
        "        # Run the benchmark\n",
        "        result = run_single_model_benchmark(\n",
        "            model_name, \n",
        "            ts_train, \n",
        "            ts_val, \n",
        "            ts_test, \n",
        "            data_info\n",
        "        )\n",
        "        \n",
        "        benchmark_results.append(result)\n",
        "        \n",
        "        # Clean up memory\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "        gc.collect()\n",
        "        \n",
        "        print(f\"Result: {result['status']}\")\n",
        "        if result['status'] == 'Success':\n",
        "            print(f\"Correlation: {result['correlation']:.4f}, RMSE: {result['rmse']:.6f}\")\n",
        "\n",
        "else:\n",
        "    print(\"üîß Using Simple PyTorch Transformer approach...\")\n",
        "    \n",
        "    # Run the simple transformer benchmark\n",
        "    result = run_simple_transformer_benchmark(ts_train, ts_val, ts_test, data_info)\n",
        "    benchmark_results.append(result)\n",
        "    \n",
        "    # Also add a simple baseline for comparison\n",
        "    print(\"\\\\nüîß Adding Linear Regression baseline...\")\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    \n",
        "    # Prepare data for linear regression\n",
        "    def prepare_linear_data(data, seq_len=96, pred_len=24):\n",
        "        features = [col for col in data.columns if col != 'label']\n",
        "        X, y = [], []\n",
        "        \n",
        "        for i in range(len(data) - seq_len - pred_len + 1):\n",
        "            # Use last value of sequence as features\n",
        "            X.append(data[features].iloc[i + seq_len - 1].values)\n",
        "            # Predict next pred_len values average\n",
        "            y.append(data['label'].iloc[i + seq_len:i + seq_len + pred_len].mean())\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    X_train, y_train = prepare_linear_data(ts_train)\n",
        "    X_test, y_test = prepare_linear_data(ts_test)\n",
        "    \n",
        "    # Train linear regression\n",
        "    start_time = time.time()\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Test\n",
        "    test_start = time.time()\n",
        "    y_pred = lr_model.predict(X_test)\n",
        "    testing_time = time.time() - test_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    correlation = np.corrcoef(y_test, y_pred)[0, 1]\n",
        "    \n",
        "    lr_result = {\n",
        "        'model': 'Linear Regression Baseline',\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': np.sqrt(mse),\n",
        "        'mape': np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100,\n",
        "        'correlation': correlation,\n",
        "        'training_time': training_time,\n",
        "        'testing_time': testing_time,\n",
        "        'total_params': X_train.shape[1],  # Number of features\n",
        "        'status': 'Success'\n",
        "    }\n",
        "    \n",
        "    benchmark_results.append(lr_result)\n",
        "    print(f\"Linear Regression - Correlation: {correlation:.6f}, RMSE: {np.sqrt(mse):.6f}\")\n",
        "\n",
        "print(f\"\\\\nüèÅ Benchmark completed! Processed {len(benchmark_results)} models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Creating Transformer Benchmark Results Table...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'benchmark_results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä Creating Transformer Benchmark Results Table...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convert results to DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results_df = pd.DataFrame(\u001b[43mbenchmark_results\u001b[49m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Sort by correlation (descending) for successful models\u001b[39;00m\n\u001b[32m      8\u001b[39m successful_results = results_df[results_df[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mSuccess\u001b[39m\u001b[33m'\u001b[39m].copy()\n",
            "\u001b[31mNameError\u001b[39m: name 'benchmark_results' is not defined"
          ]
        }
      ],
      "source": [
        "# Create comprehensive benchmark results table\n",
        "print(\"üìä Creating Transformer Benchmark Results Table...\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(benchmark_results)\n",
        "\n",
        "# Sort by correlation (descending) for successful models\n",
        "successful_results = results_df[results_df['status'] == 'Success'].copy()\n",
        "if len(successful_results) > 0:\n",
        "    successful_results = successful_results.sort_values('correlation', ascending=False)\n",
        "\n",
        "# Format the results table\n",
        "def format_results_table(df):\n",
        "    \"\"\"Format the results table for better display\"\"\"\n",
        "    formatted_df = df.copy()\n",
        "    \n",
        "    # Round numerical columns\n",
        "    numeric_cols = ['mae', 'mse', 'rmse', 'mape', 'correlation', 'training_time', 'testing_time']\n",
        "    for col in numeric_cols:\n",
        "        if col in formatted_df.columns:\n",
        "            formatted_df[col] = formatted_df[col].round(6)\n",
        "    \n",
        "    # Format parameter count\n",
        "    if 'total_params' in formatted_df.columns:\n",
        "        formatted_df['total_params'] = formatted_df['total_params'].apply(\n",
        "            lambda x: f\"{x:,}\" if not pd.isna(x) else \"N/A\"\n",
        "        )\n",
        "    \n",
        "    return formatted_df\n",
        "\n",
        "# Display results\n",
        "print(\"\\\\n\" + \"=\"*100)\n",
        "print(\"üèÜ TRANSFORMER MODEL BENCHMARK RESULTS - CRYPTO MARKET PREDICTION\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "if len(successful_results) > 0:\n",
        "    print(\"\\\\nüìà SUCCESSFUL MODELS (Ranked by Correlation):\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    formatted_successful = format_results_table(successful_results)\n",
        "    \n",
        "    # Select key columns for display\n",
        "    display_cols = ['model', 'correlation', 'rmse', 'mae', 'training_time', 'total_params']\n",
        "    available_cols = [col for col in display_cols if col in formatted_successful.columns]\n",
        "    \n",
        "    print(formatted_successful[available_cols].to_string(index=False))\n",
        "    \n",
        "    # Highlight best model\n",
        "    best_model = formatted_successful.iloc[0]\n",
        "    print(f\"\\\\nü•á BEST MODEL: {best_model['model']}\")\n",
        "    print(f\"   Correlation: {best_model['correlation']:.6f}\")\n",
        "    print(f\"   RMSE: {best_model['rmse']:.6f}\")\n",
        "    print(f\"   Training Time: {best_model['training_time']:.2f}s\")\n",
        "\n",
        "# Show failed models if any\n",
        "failed_results = results_df[results_df['status'] != 'Success']\n",
        "if len(failed_results) > 0:\n",
        "    print(f\"\\\\n‚ùå FAILED MODELS ({len(failed_results)}):\")\n",
        "    print(\"-\" * 40)\n",
        "    for _, row in failed_results.iterrows():\n",
        "        print(f\"   {row['model']}: {row['status']}\")\n",
        "\n",
        "# Compare with XGBoost baseline\n",
        "print(f\"\\\\nüìä COMPARISON WITH XGBOOST BASELINE:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"XGBoost Ensemble Correlation: {weighted_ensemble_score:.6f}\")\n",
        "\n",
        "if len(successful_results) > 0:\n",
        "    best_transformer_corr = successful_results.iloc[0]['correlation']\n",
        "    print(f\"Best Transformer Correlation: {best_transformer_corr:.6f}\")\n",
        "    \n",
        "    improvement = best_transformer_corr - weighted_ensemble_score\n",
        "    print(f\"Improvement: {improvement:+.6f} ({improvement/weighted_ensemble_score*100:+.2f}%)\")\n",
        "    \n",
        "    if improvement > 0:\n",
        "        print(\"‚úÖ Transformers outperformed XGBoost!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  XGBoost still leads, but transformers are competitive\")\n",
        "        \n",
        "# Model complexity analysis\n",
        "print(f\"\\\\nüîß MODEL COMPLEXITY ANALYSIS:\")\n",
        "print(\"-\" * 40)\n",
        "if len(successful_results) > 0:\n",
        "    for _, row in successful_results.iterrows():\n",
        "        efficiency_score = row['correlation'] / (row['training_time'] / 60)  # correlation per minute\n",
        "        print(f\"{row['model']:15s} | {row['total_params']:>10s} params | {efficiency_score:.4f} corr/min\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save benchmark results and create visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_df.to_csv('transformer_benchmark_results.csv', index=False)\n",
        "print(\"üíæ Benchmark results saved to 'transformer_benchmark_results.csv'\")\n",
        "\n",
        "# Create summary visualization if we have successful results\n",
        "if len(successful_results) > 0:\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # 1. Correlation comparison\n",
        "    plt.subplot(2, 3, 1)\n",
        "    models = successful_results['model'].tolist()\n",
        "    correlations = successful_results['correlation'].tolist()\n",
        "    \n",
        "    bars = plt.bar(range(len(models)), correlations, alpha=0.7, color='skyblue')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Correlation')\n",
        "    plt.title('Model Correlation Comparison')\n",
        "    plt.xticks(range(len(models)), models, rotation=45)\n",
        "    \n",
        "    # Add XGBoost baseline line\n",
        "    plt.axhline(y=weighted_ensemble_score, color='red', linestyle='--', \n",
        "                label=f'XGBoost Baseline ({weighted_ensemble_score:.4f})')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Highlight best model\n",
        "    best_idx = 0\n",
        "    bars[best_idx].set_color('gold')\n",
        "    bars[best_idx].set_alpha(1.0)\n",
        "    \n",
        "    # 2. Training time comparison  \n",
        "    plt.subplot(2, 3, 2)\n",
        "    training_times = successful_results['training_time'].tolist()\n",
        "    plt.bar(range(len(models)), training_times, alpha=0.7, color='lightcoral')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Training Time (seconds)')\n",
        "    plt.title('Training Time Comparison')\n",
        "    plt.xticks(range(len(models)), models, rotation=45)\n",
        "    \n",
        "    # 3. RMSE comparison\n",
        "    plt.subplot(2, 3, 3)\n",
        "    rmse_values = successful_results['rmse'].tolist()\n",
        "    plt.bar(range(len(models)), rmse_values, alpha=0.7, color='lightgreen')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('RMSE Comparison (Lower is Better)')\n",
        "    plt.xticks(range(len(models)), models, rotation=45)\n",
        "    \n",
        "    # 4. Model parameters vs Performance\n",
        "    plt.subplot(2, 3, 4)\n",
        "    params = [int(str(p).replace(',', '')) if isinstance(p, str) else p for p in successful_results['total_params']]\n",
        "    plt.scatter(params, correlations, alpha=0.7, s=100, color='purple')\n",
        "    plt.xlabel('Total Parameters')\n",
        "    plt.ylabel('Correlation')\n",
        "    plt.title('Model Size vs Performance')\n",
        "    \n",
        "    # Add model labels\n",
        "    for i, model in enumerate(models):\n",
        "        plt.annotate(model, (params[i], correlations[i]), \n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "    \n",
        "    # 5. Efficiency Analysis (Correlation per minute)\n",
        "    plt.subplot(2, 3, 5)\n",
        "    efficiency_scores = [corr / (time/60) for corr, time in zip(correlations, training_times)]\n",
        "    bars = plt.bar(range(len(models)), efficiency_scores, alpha=0.7, color='orange')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Correlation per Minute')\n",
        "    plt.title('Training Efficiency')\n",
        "    plt.xticks(range(len(models)), models, rotation=45)\n",
        "    \n",
        "    # Highlight most efficient\n",
        "    most_efficient_idx = efficiency_scores.index(max(efficiency_scores))\n",
        "    bars[most_efficient_idx].set_color('darkorange')\n",
        "    bars[most_efficient_idx].set_alpha(1.0)\n",
        "    \n",
        "    # 6. Summary radar chart for top 3 models\n",
        "    plt.subplot(2, 3, 6)\n",
        "    top_3_models = successful_results.head(3)\n",
        "    \n",
        "    if len(top_3_models) >= 2:\n",
        "        # Normalize metrics for radar chart\n",
        "        metrics = ['correlation', 'rmse']\n",
        "        normalized_data = []\n",
        "        \n",
        "        for _, model_row in top_3_models.iterrows():\n",
        "            # Normalize correlation (higher is better)\n",
        "            norm_corr = (model_row['correlation'] - successful_results['correlation'].min()) / (successful_results['correlation'].max() - successful_results['correlation'].min())\n",
        "            # Normalize RMSE (lower is better, so invert)\n",
        "            norm_rmse = 1 - (model_row['rmse'] - successful_results['rmse'].min()) / (successful_results['rmse'].max() - successful_results['rmse'].min())\n",
        "            normalized_data.append([norm_corr, norm_rmse])\n",
        "        \n",
        "        x = range(len(metrics))\n",
        "        for i, (_, model_row) in enumerate(top_3_models.iterrows()):\n",
        "            plt.plot(x, normalized_data[i], 'o-', label=model_row['model'], alpha=0.7)\n",
        "        \n",
        "        plt.xlabel('Metrics')\n",
        "        plt.ylabel('Normalized Score')\n",
        "        plt.title('Top 3 Models Comparison')\n",
        "        plt.xticks(x, ['Correlation‚Üë', 'RMSE‚Üì'])\n",
        "        plt.legend()\n",
        "        plt.ylim(0, 1)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Need at least 2 successful models\\\\nfor comparison', \n",
        "                ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Model Comparison')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transformer_benchmark_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìà Visualization saved as 'transformer_benchmark_visualization.png'\")\n",
        "\n",
        "# Create final summary report\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"üìã FINAL BENCHMARK SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "successful_count = len(successful_results)\n",
        "total_count = len(results_df)\n",
        "\n",
        "print(f\"üéØ Models tested: {total_count}\")\n",
        "print(f\"‚úÖ Successful: {successful_count}\")\n",
        "print(f\"‚ùå Failed: {total_count - successful_count}\")\n",
        "\n",
        "if successful_count > 0:\n",
        "    print(f\"\\\\nüèÜ Best performing model: {successful_results.iloc[0]['model']}\")\n",
        "    print(f\"üìä Highest correlation: {successful_results.iloc[0]['correlation']:.6f}\")\n",
        "    \n",
        "    # Find most efficient model\n",
        "    if len(successful_results) > 1:\n",
        "        efficiency_scores = successful_results['correlation'] / (successful_results['training_time'] / 60)\n",
        "        most_efficient_idx = efficiency_scores.idxmax()\n",
        "        most_efficient_model = successful_results.loc[most_efficient_idx, 'model']\n",
        "        print(f\"‚ö° Most efficient model: {most_efficient_model}\")\n",
        "        \n",
        "    print(f\"\\\\nüí° Key insights:\")\n",
        "    print(f\"   - Average correlation: {successful_results['correlation'].mean():.4f}\")\n",
        "    print(f\"   - Average training time: {successful_results['training_time'].mean():.1f}s\")\n",
        "    print(f\"   - XGBoost baseline: {weighted_ensemble_score:.4f}\")\n",
        "    \n",
        "    # Recommendation\n",
        "    best_corr = successful_results.iloc[0]['correlation']\n",
        "    if best_corr > weighted_ensemble_score:\n",
        "        print(f\"\\\\nüöÄ RECOMMENDATION: Use {successful_results.iloc[0]['model']} for improved performance!\")\n",
        "    else:\n",
        "        print(f\"\\\\nüí≠ RECOMMENDATION: Consider ensemble of transformers + XGBoost for best results\")\n",
        "\n",
        "print(\"\\\\nüéâ Transformer benchmark completed successfully!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "20250531_DRW",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 11418275,
          "sourceId": 96164,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e23729589a149b8bbac5abbaf20bd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0f100293ac544c3e8615e189a4bb8e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138bfa59102f46deb0686bd14fc7e5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed7260bc8da9460f86f278d4928dd679",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b0776cbba2e473380bfb98c15a48c0d",
            "value": "You don't have permission to access resource at URL: https://www.kaggle.com/api/v1/hello. The server reported the following issues: Unauthenticated"
          }
        },
        "2d35bccaaaa24a71909163dd2017593d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3828e5a9457d4b808609d568b6aad74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38feef5be5604414a6c53a7955187953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f90efa390f947df87fa3e976569a9e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_71c9c27a40da4c6ab8ca86f460e84e5d",
            "value": "Connecting..."
          }
        },
        "3fe743c840564e3db58ca640457c6d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a82705d75f4f9aa7822d814285dbb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68872a6012264841a99b87bc27ef3cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b8b5bab9e4943ebb1c1dda0209ed11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e6679524c3c498e9f8138993d72384e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41a82705d75f4f9aa7822d814285dbb0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d1b2a12b402e412f8ddf62db5c111119",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "71c9c27a40da4c6ab8ca86f460e84e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a8b316e2d7b4871908a75ef41c50657": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8570b29c6c6a48f8872978e6c87833f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e366575d7f466c8f37731859fc34f2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2d35bccaaaa24a71909163dd2017593d",
            "value": "401 Client Error."
          }
        },
        "86aa95f469d9461bb79fdd7a5caa7105": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89e5aff0f112449992b3cf3470a4e8f8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6b8b5bab9e4943ebb1c1dda0209ed11b",
            "value": "Please make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
          }
        },
        "89e5aff0f112449992b3cf3470a4e8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0776cbba2e473380bfb98c15a48c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f90efa390f947df87fa3e976569a9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a532a883232a4de6bb3a93c9542d6ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8570b29c6c6a48f8872978e6c87833f0",
              "IPY_MODEL_138bfa59102f46deb0686bd14fc7e5ed",
              "IPY_MODEL_86aa95f469d9461bb79fdd7a5caa7105"
            ],
            "layout": "IPY_MODEL_dec1fd44a6184e1faa39b606b5bfb073"
          }
        },
        "b34d0672c2ec43ec9c8b4592b0ed93ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_68872a6012264841a99b87bc27ef3cfa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c6f59ff4de904947893cdcf6630fe500",
            "value": ""
          }
        },
        "bb274a4efe3b461381af9c86ac03d4dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52cc0e516344b1694e7fdfe87d5c98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb274a4efe3b461381af9c86ac03d4dd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3828e5a9457d4b808609d568b6aad74a",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "c6f59ff4de904947893cdcf6630fe500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b2a12b402e412f8ddf62db5c111119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dec1fd44a6184e1faa39b606b5bfb073": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e34d30b7cf4542e2b8e332cf94b18c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3fe743c840564e3db58ca640457c6d4f",
            "style": "IPY_MODEL_0e23729589a149b8bbac5abbaf20bd73",
            "tooltip": ""
          }
        },
        "e363c6fe27ce4aeb8bacea9613786f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7a8b316e2d7b4871908a75ef41c50657",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0f100293ac544c3e8615e189a4bb8e00",
            "value": "mahtaao"
          }
        },
        "e6e366575d7f466c8f37731859fc34f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7260bc8da9460f86f278d4928dd679": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
